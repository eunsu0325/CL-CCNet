CNet Headless ëª¨ë“œ ê¸°ë°˜ Palmprint ì¸ì¦ ì‹œìŠ¤í…œ: ìµœì¢… ëª©í‘œ ë° ê°œì„  ë°©í–¥ ì •ë¦¬

ğŸ” 1. ìµœì¢… ëª©í‘œ ì •ì˜
â–¶ ìƒí˜¸ ë™ì¼ ì‚¬ìš©ì(ì§€ê¸ˆ ê°œì¸ ì¸ì¦ì ë° ë‚´ì´ì™¸ ì‚¬ìš©ì)ì˜ palmprint ì‚¬ì§„ì„ ê°€ëŠ¥í•œ ëŠë‚Œ ê°€ì§€ê³  í’ˆì§ˆì ìœ¼ë¡œ ë°°ê²½ì—ì„œ ì¸ì¦ê°€ëŠ¥í•œ biometric authentication ì‹œìŠ¤í…œ êµ¬í˜„
â–¶ Embedding-based continual verification ì‹œìŠ¤í…œ ê²°êµ­:
* ì‚¬ìš©ì ì •ë³´ ê°€ì¥ ì „ì—­ ê°€ëŠ¥
* ê³„ì†ì  ë°œìƒí•˜ëŠ” ì‚¬ìš©ìì˜ ë‹¨ê³„ì  ë³€í™”ì— ëŒ€í•œ robust feature embedding ë§Œë“¤ê¸°
* ìœ ì‚¬ë„ êµ¬ì¡°: ë°ì´í„° ì—†ì–´ë„ ê° ì‚¬ìš©ìê°€ ê°€ì§€ê³  ìˆëŠ” ì´ì „ ê·¸ë¦¼ê³¼ ë¹„êµí•´ ê°€ëŠ¥
â–¶ 128D ì••ì¶• ê¸°ë°˜ ê²½ëŸ‰ continual feature learning ì§„í–‰
* ë‹¤ì¤‘ì •ì˜ palmprint datasetì—ì„œ ê°ê¸° ë‹¹ ì‚¬ìš©ì ìˆ˜ì¤€ì˜ ëŒ€ìƒì„ ê°€ì§€ê³  ë‹¤ë¥¸ êµ¬ì¡°ì—ë„ ê°€ëŠ¥í•œ generalizable embedding ê°€ì§€ê³ ì í•¨

âœ… 2. í˜„ì¬ êµ¬ì¡°ì˜ ê°€ì¹˜
* â–¶ CCNet Headless: 2048D ë³´ë„ˆì¹˜ì— ProjectionHead ê°€ì§€ê³  ê³„ì†í™”ëœ 128D latent vector ê·€ì—¬
* â–¶ Xavier ì´ˆê¸°í™”: projection headì˜ íš¨ìœ¨ì  í–¥ìƒì„± ë° ì•ˆì •ëœ íŒ¨ë„ íƒìƒ‰ ë³µì¡ì„± ì €í•­
* â–¶ HeadlessVerifier + FAISS: ì§€ì†ê°€ëŠ¥í•œ metric-based matching + ì‹œê° í•´ì„ì„ ê°€ì§€ëŠ” Top-K score logging

ğŸ”§ 3. ì–´ë–»ê²Œ ìˆ˜ì •í•˜ë©´ ì¢‹ì€ê°€?
ğŸ“Œ 2. Memory-Efficient Matching: FAISS + Top-K Filtering
* ì´ìœ : ì‚¬ìš©ì ìˆ˜ê°€ ëŠ˜ì–´ë‚ ìˆ˜ë¡ ìœ ì‚¬ë„ ê³„ì‚°ëŸ‰ì´ ê¸‰ê²©íˆ ì¦ê°€ â†’ ì‹¤ì‹œê°„ ì¸ì¦ì— ë³‘ëª© ë°œìƒ ê°€ëŠ¥
* ë°©ë²•:
    * FAISS ì¸ë±ìŠ¤ë¥¼ í†µí•œ Top-K í›„ë³´ì ì„ ë³„
    * Top-K í›„ë³´ì— ëŒ€í•´ì„œë§Œ ì„¸ë°€í•œ cosine ìœ ì‚¬ë„ ê³„ì‚° ì ìš©
    * Multi-index êµ¬ì¡°ë‚˜ Product Quantization ì ìš©ìœ¼ë¡œ ê³ ì† ê·¼ì ‘ ê²€ìƒ‰
    * ì‚¬ìš©ì ë“±ë¡ ì‹œ ë²¡í„°ë¥¼ PQ ì••ì¶• í˜•íƒœë¡œ ì €ì¥í•´ ë©”ëª¨ë¦¬ ìµœì†Œí™”
ğŸ“Œ 3. ë£¨í”„ í´ë¡œì € ê°œë… ì°¨ìš© (SLAM â†’ Biometrics)
* ì´ìœ : ì˜¨ë¼ì¸ í•™ìŠµ ì¤‘ ì˜¤ë˜ëœ ì‚¬ìš©ìì˜ embeddingì´ ì˜¤ë˜ëœ ì±„ ë°©ì¹˜ë˜ì–´ ì˜¤í”ˆì…‹ ëŒ€ì‘ë ¥ ì•½í™”
* ë°©ë²•:
    * ì£¼ê¸°ì ìœ¼ë¡œ FAISS Top-1 ê²°ê³¼ì™€ í˜„ì¬ ì„ë² ë”©ì˜ cosine similarity ì¸¡ì •
    * ì¼ì • threshold ì´ìƒ ì°¨ì´ê°€ ë‚  ê²½ìš° ê¸°ì¡´ ë²¡í„°ë¥¼ EMA ë°©ì‹ìœ¼ë¡œ ë³´ì • ì—…ë°ì´íŠ¸
    * classification headê°€ ìˆìœ¼ë©´ pseudo-label ë¶€ì—¬í•´ soft supervision ì ìš© ê°€ëŠ¥
    * anchor vector â†’ historical drift ë³´ì • â†’ ì‚¬ìš©ìë³„ embedding ìœ ì§€ë ¥ í–¥ìƒ
ğŸ“Œ 4. Memory Bank ê¸°ë°˜ Contrastive Learning í™•ì¥
* ì´ìœ : Replay bufferë§Œìœ¼ë¡œëŠ” ì œí•œëœ ê³¼ê±° ë°ì´í„°ë§Œ í•™ìŠµ ê°€ëŠ¥ â†’ ì •ë³´ ë‹¤ì–‘ì„± ë¶€ì¡±
* ë°©ë²•:
    * 128D ì••ì¶• í‘œí˜„ì„ ëŒ€ìƒìœ¼ë¡œ í•œ lightweight memory bank êµ¬ì„±
    * SupCon + Hard Negative Mining ê¸°ë°˜ contrastive loss ì ìš©
    * ìµœê·¼ ì„ë² ë”©ì„ vector queueë¡œ ê´€ë¦¬í•˜ë©° positive-negative pair êµ¬ì„±
    * ì„ë² ë””ë“œ í•œê³„ ê³ ë ¤í•´ ì¼ì • ê¸°ê°„ í›„ low-score vector ì œê±° (eviction policy)
    * LIFO/Reservoir ë°©ì‹ìœ¼ë¡œ bank ë©”ëª¨ë¦¬ ê³ ì • ìœ ì§€
ğŸ“Œ 5. ì‚¬ìš©ì ì¦ê°€ì— ë”°ë¥¸ í™•ì¥ì„± ê³ ë ¤
* ì´ìœ : ì„ë² ë””ë“œ í™˜ê²½ì—ì„œ ë©”ëª¨ë¦¬, ê³„ì‚°ëŸ‰ì€ ì œí•œì ì´ê¸° ë•Œë¬¸
* ë°©ë²•:
    * ì‚¬ìš©ìë‹¹ 1ê°œì˜ ì¤‘ì‹¬ ë²¡í„°(centroid)ë§Œ ì €ì¥í•˜ì—¬ ì—°ì‚°ëŸ‰ ì œí•œ
    * ì¤‘ìš” ì‚¬ìš©ìëŠ” 2048D ìœ ì§€, ì¼ë°˜ ì‚¬ìš©ìëŠ” 128Dë§Œ ë³´ê´€ (ì„ ë³„ì  ì••ì¶•)
    * ì‚¬ìš©ì ìˆ˜ ì¦ê°€ ì‹œ Top-K filtering + ì¡°ê±´ë¶€ updateë§Œ ìˆ˜í–‰
    * background verifierì—ì„œ Top-1 ì˜ˆì¸¡ í›„ update ì¡°ê±´ ë§Œì¡± ì‹œë§Œ EMA ë³´ì •

ğŸ“ˆ 4. í‰ê°€ ì§€í‘œ ë° ì‹¤í—˜ í™•ì¥
* â–¶ get_model_info()ì— inference FLOPs, latency(ms), GPU í• ë‹¹ ì—¬ë¶€ ì¶”ê°€
* â–¶ HeadlessVerifierì— top-k ê²°ê³¼, similarity score logging ì˜µì…˜ ì¶”ê°€ (EER, FAR, FRR ë¶„ì„ìš©)
* â–¶ ê° ì‹¤í—˜ì— ëŒ€í•´ heatmap ì‹œê°í™”, ROC ê³¡ì„  ë“±ë„ í¬í•¨í•˜ì—¬ ì„±ëŠ¥ ì§ê´€í™”
* â–¶ ì‚¬ìš©ì ìˆ˜ ì¦ê°€ ëŒ€ë¹„ ì‘ë‹µ ì‹œê°„ scaling ê³¡ì„  ë¶„ì„

ğŸ§­ 5. ë£¨í”„ í´ë¡œì € ê¸°ë°˜ Self-Correction ë°©ì‹ ìš”ì•½
* â–¶ ì •ê¸°ì  cosine similarity ë¹„êµë¡œ driftëœ ì‚¬ìš©ìì˜ í‘œí˜„ ì—…ë°ì´íŠ¸
* â–¶ ëŒ€í‘œ ë²¡í„°ëŠ” EMA(ì§€ìˆ˜ ì´ë™ í‰ê· )ë¡œ ì—…ë°ì´íŠ¸
* â–¶ Top-K filtering í›„ soft-label ê¸°ë°˜ continual learning ë³‘í–‰
* â–¶ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ê³ ì •í•˜ë©´ì„œë„ ê³¼ê±° ì‚¬ìš©ì representationì„ ìœ ì§€í•˜ëŠ” ë°©ë²•

ğŸ“Œ ê²°ë¡ : ì§€ê¸ˆë¶€í„° ê°œì •í•  ì‹¤í—˜ ë°©í–¥ ìš”ì•½
1. SupCon + Hard Negative Mining ê¸°ë°˜ í‘œí˜„ í•™ìŠµ ì‹¤í—˜
2. Headless + FAISS ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ì¶œ íŒŒì´í”„ë¼ì¸ ì™„ì„±
3. ë£¨í”„ í´ë¡œì € ë°©ì‹ì˜ embedding self-correction ì‹¤í—˜
4. Memory Bank ê¸°ë°˜ contrastive ëŒ€ì²´ ì‹¤í—˜
5. ì‚¬ìš©ì ìˆ˜ ì¦ê°€ì— ë”°ë¥¸ íš¨ìœ¨ì„± ë¶„ì„ (Top-K, PQ ë“±)
6. ProjectionHeadì™€ classification ëª¨ë“œ ê°„ ì „í™˜ ì•ˆì •ì„± ë³´ì¥
7. ì‹¤í—˜ ê²°ê³¼ë¥¼ í†µí•œ ì‹œìŠ¤í…œ ë³µì¡ë„-ì„±ëŠ¥ íŠ¸ë ˆì´ë“œì˜¤í”„ ë¶„ì„
â€¨â€¨â€”â€”â€”â€”


ğŸ¥¥ COCONUT ë‹¨ê³„ë³„ ìˆ˜ì • ë¡œë“œë§µ
ğŸ“‹ ì „ì²´ ìˆ˜ì • ì „ëµ
ì›ì¹™:
1. í•˜ë‚˜ì”© ì°¨ê·¼ì°¨ê·¼: í•œ ë²ˆì— í•˜ë‚˜ì˜ ê¸°ëŠ¥ë§Œ ìˆ˜ì •
2. í…ŒìŠ¤íŠ¸ ê¸°ë°˜: ê° ë‹¨ê³„ë§ˆë‹¤ ì„±ëŠ¥ ê²€ì¦
3. ê¸°ì¡´ ì½”ë“œ ë³´ì¡´: ë™ì‘í•˜ëŠ” ë¶€ë¶„ì€ ìµœëŒ€í•œ ìœ ì§€
4. ì ì§„ì  ê°œì„ : ì‘ì€ ê°œì„ ì„ ëˆ„ì í•˜ì—¬ í° ë³€í™” ë‹¬ì„±

ğŸš€ Phase 1: ê¸°ë°˜ ì‹œìŠ¤í…œ ì•ˆì •í™” (1-2ì£¼)
1.1 í˜„ì¬ COCONUT ì‹œìŠ¤í…œ ë¶„ì„ ë° ë² ì´ìŠ¤ë¼ì¸ ì„¤ì •
ëª©í‘œ: í˜„ì¬ ì‹œìŠ¤í…œì˜ ì •í™•í•œ ì„±ëŠ¥ ì¸¡ì •
ì‘ì—…:
- ê¸°ì¡´ ì½”ë“œ ì „ì²´ ë¦¬ë·° ë° ë™ì‘ í™•ì¸
- ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ì¸¡ì • (EER, Rank-1 Accuracy)
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰, ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
- ë¬¸ì œì  ëª©ë¡ ì‘ì„±

í…ŒìŠ¤íŠ¸ ë°©ë²•:
- ê¸°ì¡´ ë°ì´í„°ì…‹ìœ¼ë¡œ end-to-end ì‹¤í–‰
- ê° ëª¨ë“ˆë³„ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
- ì„±ëŠ¥ ì§€í‘œ ë¡œê¹… ì‹œìŠ¤í…œ êµ¬ì¶•

ì˜ˆìƒ ê²°ê³¼:
- í˜„ì¬ ì‹œìŠ¤í…œ ì„±ëŠ¥ ì •í™•í•œ ìˆ˜ì¹˜í™”
- ê°œì„  í¬ì¸íŠ¸ ìš°ì„ ìˆœìœ„ ì„¤ì •
1.2 HeadlessVerifier ê°œì„  ë° ì•ˆì •í™”
ëª©í‘œ: ê¸°ë³¸ ì¸ì¦ ì‹œìŠ¤í…œì˜ ì•ˆì •ì„± í™•ë³´
ì‘ì—…:
- get_score_statistics() ë©”ì„œë“œ ë””ë²„ê¹…
- Top-K ê²°ê³¼ ë¡œê¹… ê¸°ëŠ¥ ì¶”ê°€
- Similarity score íˆìŠ¤í† ë¦¬ ë¶„ì„ ê¸°ëŠ¥
- ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì ê²€ ë° ìˆ˜ì •

í…ŒìŠ¤íŠ¸ ë°©ë²•:
- ë‹¤ì–‘í•œ ì‚¬ìš©ì ìˆ˜ë¡œ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
- ì¥ì‹œê°„ ì‹¤í–‰ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸
- ë©”ëª¨ë¦¬ ì‚¬ìš© íŒ¨í„´ ëª¨ë‹ˆí„°ë§

ì˜ˆìƒ ê²°ê³¼:
- ì•ˆì •ì ì¸ base verifier í™•ë³´
- ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë„êµ¬ ì™„ì„±
1.3 FAISS í†µí•© ìµœì í™”
ëª©í‘œ: FAISS ê¸°ë°˜ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì•ˆì •í™”
ì‘ì—…:
- CPU/GPU ìë™ ì „í™˜ ë¡œì§ ê°œì„ 
- FAISS fallback ë©”ì»¤ë‹ˆì¦˜ ê°•í™”
- ì¸ë±ìŠ¤ ì €ì¥/ë¡œë“œ ê¸°ëŠ¥ ì¶”ê°€
- ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ë„êµ¬ ê°œë°œ

í…ŒìŠ¤íŠ¸ ë°©ë²•:
- FAISS ìˆëŠ” í™˜ê²½/ì—†ëŠ” í™˜ê²½ í…ŒìŠ¤íŠ¸
- ë‹¤ì–‘í•œ ì¸ë±ìŠ¤ í¬ê¸°ë¡œ ì„±ëŠ¥ ì¸¡ì •
- GPU/CPU í™˜ê²½ë³„ ì†ë„ ë¹„êµ

ì˜ˆìƒ ê²°ê³¼:
- ì•ˆì •ì ì¸ FAISS í†µí•© ì‹œìŠ¤í…œ
- í™˜ê²½ë³„ ìµœì  ì„¤ì • ê°€ì´ë“œ



ğŸš€ Phase 3: í™•ì¥ì„± ìµœì í™” (3-4ì£¼)
3.1 Top-K Filtering êµ¬í˜„
ëª©í‘œ: ì‚¬ìš©ì ì¦ê°€ì— ë”°ë¥¸ ê²€ìƒ‰ ì†ë„ ìµœì í™”
ì‘ì—…:
- FAISS ê¸°ë°˜ Top-K candidate selection
- í›„ë³´êµ°ì— ëŒ€í•œ ì •ë°€ cosine similarity ê³„ì‚°
- Dynamic K ê°’ ì¡°ì • ë¡œì§
- ì„±ëŠ¥/ì •í™•ë„ trade-off ë¶„ì„

í…ŒìŠ¤íŠ¸ ë°©ë²•:
- ì‚¬ìš©ì ìˆ˜ë³„ ê²€ìƒ‰ ì†ë„ ì¸¡ì •
- K ê°’ì— ë”°ë¥¸ accuracy ë³€í™” ì¸¡ì •
- Memory usage íŒ¨í„´ ë¶„ì„

ì˜ˆìƒ ê²°ê³¼:
- O(log N) ê²€ìƒ‰ ë³µì¡ë„ ë‹¬ì„±
- ëŒ€ìš©ëŸ‰ ì‚¬ìš©ì DB ì§€ì› ê°€ëŠ¥
- ì‹¤ì‹œê°„ ê²€ìƒ‰ ì„±ëŠ¥ í™•ë³´
3.2 Memory Bank ê¸°ë³¸ êµ¬í˜„
ëª©í‘œ: ì œí•œëœ replay buffer ë³´ì™„
ì‘ì—…:
- MemoryBankWithEviction í´ë˜ìŠ¤ êµ¬í˜„
- LRU + Quality ê¸°ë°˜ eviction policy
- Hard negative mining ë¡œì§
- Memory bank í†µê³„ ë° ëª¨ë‹ˆí„°ë§

í…ŒìŠ¤íŠ¸ ë°©ë²•:
- ë‹¤ì–‘í•œ eviction strategy ë¹„êµ
- Memory bank size ìµœì í™”
- Replay buffer vs Memory bank ì„±ëŠ¥ ë¹„êµ

ì˜ˆìƒ ê²°ê³¼:
- ë” í’ë¶€í•œ ê³¼ê±° ì •ë³´ í™œìš©
- Hard negative mining íš¨ê³¼ í™•ì¸
- Continual learning ì„±ëŠ¥ í–¥ìƒ
3.3 Hierarchical User Clustering ê¸°ë³¸ êµ¬í˜„
ëª©í‘œ: ëŒ€ê·œëª¨ ì‚¬ìš©ì ê´€ë¦¬ ì‹œìŠ¤í…œ
ì‘ì—…:
- HierarchicalUserManager í´ë˜ìŠ¤ êµ¬í˜„
- ìë™ í´ëŸ¬ìŠ¤í„° ìƒì„±/ê´€ë¦¬ ë¡œì§
- 2-stage hierarchical search
- í´ëŸ¬ìŠ¤í„° í†µê³„ ë° ì‹œê°í™”

í…ŒìŠ¤íŠ¸ ë°©ë²•:
- 100~1000 ì‚¬ìš©ì ê·œëª¨ í…ŒìŠ¤íŠ¸
- í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ í‰ê°€
- ê²€ìƒ‰ ì„±ëŠ¥ vs ì •í™•ë„ ë¶„ì„

ì˜ˆìƒ ê²°ê³¼:
- ëŒ€ê·œëª¨ ì‚¬ìš©ì ì§€ì› ê¸°ë°˜ êµ¬ì¶•
- íš¨ìœ¨ì ì¸ ì‚¬ìš©ì ê´€ë¦¬ ì‹œìŠ¤í…œ
- í™•ì¥ì„± ë¬¸ì œ í•´ê²°

ğŸ¯ Phase 4: ê³ ê¸‰ ê¸°ëŠ¥ (4-5ì£¼)
4.1 Temporal Consistency Manager
ëª©í‘œ: ì‹œê°„ì  ë³€í™” ëŒ€ì‘ ì‹œìŠ¤í…œ
ì‘ì—…:
- TemporalConsistencyManager êµ¬í˜„
- Age-based weighting ë¡œì§
- Cross-session consistency ì¸¡ì •
- Temporal drift ë³´ì •

í…ŒìŠ¤íŠ¸ ë°©ë²•:
- ì‹œê°„ ê°„ê²©ë³„ ì„±ëŠ¥ ë³€í™” ì¸¡ì •
- Aging íš¨ê³¼ ë¶„ì„
- Long-term stability í…ŒìŠ¤íŠ¸

ì˜ˆìƒ ê²°ê³¼:
- ì‹œê°„ì  ë³€í™”ì— robustí•œ ì‹œìŠ¤í…œ
- ì¥ê¸° ì‚¬ìš©ì ì¶”ì  ì„±ëŠ¥ í–¥ìƒ
4.2 Adaptive Threshold Learning
ëª©í‘œ: ì‚¬ìš©ìë³„ ê°œì¸í™”ëœ ì¸ì¦ ì„ê³„ê°’
ì‘ì—…:
- ì‚¬ìš©ìë³„ EER ê¸°ë°˜ threshold í•™ìŠµ
- Personalization factor ê³„ì‚°
- Dynamic threshold adjustment
- ê°œì¸í™” íš¨ê³¼ ì¸¡ì •

í…ŒìŠ¤íŠ¸ ë°©ë²•:
- ì‚¬ìš©ìë³„ ìµœì  threshold ë¶„ì„
- Global vs Personal threshold ì„±ëŠ¥ ë¹„êµ
- FAR/FRR ê· í˜• ìµœì í™”

ì˜ˆìƒ ê²°ê³¼:
- ê°œì¸í™”ëœ ì¸ì¦ ì‹œìŠ¤í…œ
- ì „ì²´ì ì¸ ì¸ì¦ ì •í™•ë„ í–¥ìƒ
4.3 Product Quantization í†µí•©
ëª©í‘œ: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê·¹í•œ ìµœì í™”
ì‘ì—…:
- PQ ê¸°ë°˜ vector compression
- ì••ì¶•ë¥  vs ì •í™•ë„ trade-off ë¶„ì„
- Multi-index êµ¬ì¡° ìµœì í™”
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë²¤ì¹˜ë§ˆí‚¹

í…ŒìŠ¤íŠ¸ ë°©ë²•:
- ë‹¤ì–‘í•œ ì••ì¶•ë¥  ì‹¤í—˜
- ì •í™•ë„ ì†ì‹¤ ì¸¡ì •
- ì‹¤ì œ ë©”ëª¨ë¦¬ ì ˆì•½ íš¨ê³¼ í™•ì¸

ì˜ˆìƒ ê²°ê³¼:
- ê·¹í•œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ë‹¬ì„±
- Edge device ë°°í¬ ì¤€ë¹„ ì™„ë£Œ

ğŸ“Š Phase 5: í†µí•© ë° ìµœì í™” (5-6ì£¼)
5.1 ì „ì²´ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸
ëª©í‘œ: ëª¨ë“  ê¸°ëŠ¥ì˜ ì•ˆì •ì  í†µí•©
ì‘ì—…:
- End-to-end í†µí•© í…ŒìŠ¤íŠ¸
- ëª¨ë“ˆ ê°„ ìƒí˜¸ì‘ìš© ìµœì í™”
- ì„±ëŠ¥ ë³‘ëª© ì§€ì  íŒŒì•… ë° í•´ê²°
- ì „ì²´ ì‹œìŠ¤í…œ ì•ˆì •ì„± í™•ë³´

í…ŒìŠ¤íŠ¸ ë°©ë²•:
- ëŒ€ê·œëª¨ dataset ì¥ì‹œê°„ ì‹¤í–‰
- ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ stress test
- Memory leak ë° ì„±ëŠ¥ ì €í•˜ ì ê²€

ì˜ˆìƒ ê²°ê³¼:
- ì•ˆì •ì ì¸ í†µí•© ì‹œìŠ¤í…œ
- ì‹¤ì œ ë°°í¬ ê°€ëŠ¥í•œ ì™„ì„±ë„
5.2 ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ë° ë¶„ì„
ëª©í‘œ: ì¢…í•©ì  ì„±ëŠ¥ í‰ê°€ ë° ë¶„ì„
ì‘ì—…:
- ê°œì„  ì „í›„ ì„±ëŠ¥ ë¹„êµ
- ê° ê¸°ëŠ¥ë³„ ê¸°ì—¬ë„ ë¶„ì„
- ë³µì¡ë„-ì„±ëŠ¥ trade-off ì •ë¦¬
- ìµœì¢… ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì‘ì„±

í…ŒìŠ¤íŠ¸ ë°©ë²•:
- Ablation studyë¡œ ê° ê¸°ëŠ¥ íš¨ê³¼ ì¸¡ì •
- Baseline ëŒ€ë¹„ ê°œì„  íš¨ê³¼ ì •ëŸ‰í™”
- ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ ì‹œë®¬ë ˆì´ì…˜

ì˜ˆìƒ ê²°ê³¼:
- ëª…í™•í•œ ì„±ëŠ¥ ê°œì„  ì…ì¦
- ê° ê¸°ëŠ¥ì˜ ê°€ì¹˜ ì •ëŸ‰í™”
- í–¥í›„ ê°œì„  ë°©í–¥ ë„ì¶œ

ğŸ¯ ê° Phaseë³„ ì„±ê³µ ê¸°ì¤€
Phase 1 ì„±ê³µ ê¸°ì¤€:
* [ ] ê¸°ì¡´ ì‹œìŠ¤í…œ ì•ˆì •ì  ë™ì‘ í™•ì¸
* [ ] ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ìˆ˜ì¹˜ í™•ë³´
* [ ] FAISS í†µí•© ì•ˆì •ì„± í™•ë³´

Phase 3 ì„±ê³µ ê¸°ì¤€:
* [ ] Top-K filtering ì†ë„ í–¥ìƒ í™•ì¸ (50% ì´ìƒ)
* [ ] Memory bank ì„±ëŠ¥ í–¥ìƒ í™•ì¸ (replay buffer ëŒ€ë¹„)
* [ ] Hierarchical clustering í™•ì¥ì„± í™•ì¸ (1000 ì‚¬ìš©ì ì§€ì›)
Phase 4 ì„±ê³µ ê¸°ì¤€:
* [ ] Temporal consistency ì¥ê¸° ì•ˆì •ì„± í™•ì¸
* [ ] Adaptive threshold ê°œì¸í™” íš¨ê³¼ í™•ì¸
* [ ] PQ ë©”ëª¨ë¦¬ ì ˆì•½ íš¨ê³¼ í™•ì¸ (90% ì´ìƒ)
Phase 5 ì„±ê³µ ê¸°ì¤€:
* [ ] ì „ì²´ ì‹œìŠ¤í…œ ì•ˆì •ì„± í™•ì¸
* [ ] ì¢…í•© ì„±ëŠ¥ ê°œì„  í™•ì¸ (20% ì´ìƒ)
* [ ] ì‹¤ì œ ë°°í¬ ê°€ëŠ¥ ìˆ˜ì¤€ ë‹¬ì„±

ğŸ› ï¸ ì‹¤í–‰ ë°©ë²•
ì£¼ê°„ ê³„íš:
Week 1: Phase 1.1-1.2 (í˜„ì¬ ì‹œìŠ¤í…œ ë¶„ì„, Verifier ê°œì„ )
Week 2: Phase 1.3-2.1 (FAISS ìµœì í™”, Quality assessment)
Week 3: Phase 2.2-2.3 (Loop closure, EMA correction)
Week 4: Phase 3.1-3.2 (Top-K filtering, Memory bank)
Week 5: Phase 3.3-4.1 (Hierarchical clustering, Temporal)
Week 6: Phase 4.2-4.3 (Adaptive threshold, PQ)
ê° ì£¼ ë§ˆì§€ë§‰:
* í•´ë‹¹ ì£¼ ëª©í‘œ ë‹¬ì„±ë„ ì²´í¬
* ë‹¤ìŒ ì£¼ ê³„íš ì¡°ì •
* ë¬¸ì œì  ë°œê²¬ ì‹œ ì¦‰ì‹œ í•´ê²°
ì´ë ‡ê²Œ ë‹¨ê³„ì ìœ¼ë¡œ ì ‘ê·¼í•˜ë©´ ì•ˆì „í•˜ê²Œ ì‹œìŠ¤í…œì„ ê°œì„ í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„ìš”! ğŸ¯
ì–´ë–¤ ë‹¨ê³„ë¶€í„° ì‹œì‘í•´ë³´ì‹¤ ê±´ê°€ìš”?


í˜„ì¬ í…ŒìŠ¤íŠ¸ ì½”ë“œ ê²°ê³¼

# Phase 1.1: COCONUT ì‹œìŠ¤í…œ ë¶„ì„ ë° ë² ì´ìŠ¤ë¼ì¸ ì„¤ì •
# ëª©í‘œ: í˜„ì¬ ì‹œìŠ¤í…œì˜ ì •í™•í•œ ì„±ëŠ¥ ì¸¡ì • ë° ë¬¸ì œì  íŒŒì•…

import torch
import time
import psutil
import json
from pathlib import Path
from datetime import datetime
import matplotlib.pyplot as plt
import numpy as np
from torch.utils.data import DataLoader

from config.config_parser import ConfigParser
from framework.coconut import CoconutSystem
from datasets.palm_dataset import MyDataset
from evaluation.eval_utils import perform_coconut_evaluation

class COCONUTSystemAnalyzer:
    """COCONUT ì‹œìŠ¤í…œ ì¢…í•© ë¶„ì„ê¸°"""
    
    def __init__(self, config_path='./config/adapt_config.yaml'):
        self.config_path = config_path
        self.analysis_results = {}
        self.baseline_metrics = {}
        
        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        self.analysis_dir = Path('./analysis_results')
        self.analysis_dir.mkdir(parents=True, exist_ok=True)
        
        print("ğŸ” COCONUT System Analyzer ì´ˆê¸°í™” ì™„ë£Œ")
    
    def run_full_analysis(self):
        """ì „ì²´ ì‹œìŠ¤í…œ ë¶„ì„ ì‹¤í–‰"""
        print("\n" + "="*80)
        print("ğŸ¥¥ COCONUT ì‹œìŠ¤í…œ ì „ì²´ ë¶„ì„ ì‹œì‘")
        print("="*80)
        
        try:
            # 1. ì„¤ì • íŒŒì¼ ë¶„ì„
            print("\nğŸ“‹ 1. ì„¤ì • íŒŒì¼ ë¶„ì„...")
            self._analyze_configuration()
            
            # 2. ì‹œìŠ¤í…œ êµ¬ì„± ìš”ì†Œ ë¶„ì„
            print("\nğŸ”§ 2. ì‹œìŠ¤í…œ êµ¬ì„± ìš”ì†Œ ë¶„ì„...")
            self._analyze_system_components()
            
            # 3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
            print("\nğŸ’¾ 3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„...")
            self._analyze_memory_usage()
            
            # 4. ì„±ëŠ¥ ë² ì´ìŠ¤ë¼ì¸ ì¸¡ì •
            print("\nğŸ“Š 4. ì„±ëŠ¥ ë² ì´ìŠ¤ë¼ì¸ ì¸¡ì •...")
            self._measure_baseline_performance()
            
            # 5. ì²˜ë¦¬ ì‹œê°„ ë¶„ì„
            print("\nâ±ï¸ 5. ì²˜ë¦¬ ì‹œê°„ ë¶„ì„...")
            self._analyze_processing_time()
            
            # 6. ë¬¸ì œì  ì‹ë³„
            print("\nğŸš¨ 6. ë¬¸ì œì  ì‹ë³„...")
            self._identify_issues()
            
            # 7. ë¶„ì„ ê²°ê³¼ ì €ì¥
            print("\nğŸ’¾ 7. ë¶„ì„ ê²°ê³¼ ì €ì¥...")
            self._save_analysis_results()
            
            print("\nâœ… ì „ì²´ ì‹œìŠ¤í…œ ë¶„ì„ ì™„ë£Œ!")
            self._print_summary()
            
        except Exception as e:
            print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
    
    def _analyze_configuration(self):
        """ì„¤ì • íŒŒì¼ ìƒì„¸ ë¶„ì„"""
        try:
            config = ConfigParser(self.config_path)
            
            config_analysis = {
                'dataset': {
                    'type': config.dataset.type if config.dataset else 'N/A',
                    'height': config.dataset.height if config.dataset else 'N/A',
                    'width': config.dataset.width if config.dataset else 'N/A',
                    'dataset_path': str(config.dataset.dataset_path) if config.dataset else 'N/A'
                },
                'model': {
                    'architecture': config.palm_recognizer.architecture if config.palm_recognizer else 'N/A',
                    'num_classes': config.palm_recognizer.num_classes if config.palm_recognizer else 'N/A',
                    'headless_mode': getattr(config.palm_recognizer, 'headless_mode', False),
                    'compression_dim': getattr(config.palm_recognizer, 'compression_dim', 'N/A'),
                    'feature_dimension': config.palm_recognizer.feature_dimension if config.palm_recognizer else 'N/A'
                },
                'continual_learning': {
                    'continual_batch_size': getattr(config.continual_learner, 'continual_batch_size', 'N/A'),
                    'target_positive_ratio': getattr(config.continual_learner, 'target_positive_ratio', 'N/A'),
                    'hard_mining_ratio': getattr(config.continual_learner, 'hard_mining_ratio', 'N/A'),
                    'adaptation_epochs': config.continual_learner.adaptation_epochs if config.continual_learner else 'N/A'
                },
                'replay_buffer': {
                    'max_buffer_size': config.replay_buffer.max_buffer_size if config.replay_buffer else 'N/A',
                    'similarity_threshold': config.replay_buffer.similarity_threshold if config.replay_buffer else 'N/A',
                    'sampling_strategy': getattr(config.replay_buffer, 'sampling_strategy', 'N/A')
                }
            }
            
            self.analysis_results['configuration'] = config_analysis
            
            print("ğŸ“‹ ì„¤ì • ë¶„ì„ ì™„ë£Œ:")
            print(f"   - Model: {config_analysis['model']['architecture']}")
            print(f"   - Headless: {config_analysis['model']['headless_mode']}")
            print(f"   - Compression: {config_analysis['model']['compression_dim']}D")
            print(f"   - Batch Size: {config_analysis['continual_learning']['continual_batch_size']}")
            print(f"   - Buffer Size: {config_analysis['replay_buffer']['max_buffer_size']}")
            
        except Exception as e:
            print(f"âŒ ì„¤ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
            self.analysis_results['configuration'] = {'error': str(e)}
    
    def _analyze_system_components(self):
        """ì‹œìŠ¤í…œ êµ¬ì„± ìš”ì†Œ ë¶„ì„"""
        try:
            # COCONUT ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            config = ConfigParser(self.config_path)
            system = CoconutSystem(config)
            
            component_analysis = {
                'model_info': {},
                'buffer_stats': {},
                'system_state': {}
            }
            
            # ëª¨ë¸ ì •ë³´ ë¶„ì„
            if hasattr(system, 'learner_net') and system.learner_net:
                model_info = system.learner_net.get_model_info()
                component_analysis['model_info'] = model_info
                print(f"   - Model Architecture: {model_info.get('architecture', 'Unknown')}")
                print(f"   - Headless Mode: {model_info.get('headless_mode', 'Unknown')}")
                print(f"   - Feature Dimension: {model_info.get('feature_dimension', 'Unknown')}")
                print(f"   - Total Parameters: {model_info.get('total_parameters', 'Unknown'):,}")
            
            # ë¦¬í”Œë ˆì´ ë²„í¼ ìƒíƒœ ë¶„ì„
            if hasattr(system, 'replay_buffer') and system.replay_buffer:
                buffer_stats = system.replay_buffer.get_diversity_stats()
                component_analysis['buffer_stats'] = buffer_stats
                print(f"   - Buffer Size: {buffer_stats.get('total_samples', 0)}")
                print(f"   - Unique Users: {buffer_stats.get('unique_users', 0)}")
                print(f"   - Diversity Score: {buffer_stats.get('diversity_score', 0):.3f}")
            
            # ì‹œìŠ¤í…œ ìƒíƒœ
            component_analysis['system_state'] = {
                'learner_step_count': getattr(system, 'learner_step_count', 0),
                'global_dataset_index': getattr(system, 'global_dataset_index', 0),
                'headless_mode': getattr(system, 'headless_mode', False),
                'device': str(system.device) if hasattr(system, 'device') else 'Unknown'
            }
            
            self.analysis_results['components'] = component_analysis
            self.system = system  # ë‹¤ìŒ ë¶„ì„ì—ì„œ ì‚¬ìš©
            
        except Exception as e:
            print(f"âŒ êµ¬ì„± ìš”ì†Œ ë¶„ì„ ì‹¤íŒ¨: {e}")
            self.analysis_results['components'] = {'error': str(e)}
    
    def _analyze_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìƒì„¸ ë¶„ì„"""
        try:
            # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë³´
            memory_info = psutil.virtual_memory()
            
            # GPU ë©”ëª¨ë¦¬ ì •ë³´ (ê°€ëŠ¥í•œ ê²½ìš°)
            gpu_memory = {}
            if torch.cuda.is_available():
                gpu_memory = {
                    'allocated': torch.cuda.memory_allocated() / 1024**2,  # MB
                    'cached': torch.cuda.memory_reserved() / 1024**2,      # MB
                    'max_allocated': torch.cuda.max_memory_allocated() / 1024**2  # MB
                }
            
            # ëª¨ë¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •
            model_memory = {}
            if hasattr(self, 'system') and hasattr(self.system, 'learner_net'):
                total_params = sum(p.numel() for p in self.system.learner_net.parameters())
                model_memory = {
                    'parameters': total_params,
                    'memory_mb': total_params * 4 / 1024**2,  # float32 ê¸°ì¤€
                    'compression_savings': 0  # ê³„ì‚° ì˜ˆì •
                }
                
                # Headless ì••ì¶• íš¨ê³¼ ê³„ì‚°
                if self.system.headless_mode:
                    original_feature_memory = 2048 * 4 / 1024  # KB per sample
                    compressed_feature_memory = getattr(self.system, 'feature_dimension', 128) * 4 / 1024
                    model_memory['compression_savings'] = (1 - compressed_feature_memory / original_feature_memory) * 100
            
            memory_analysis = {
                'system_memory': {
                    'total_gb': memory_info.total / 1024**3,
                    'available_gb': memory_info.available / 1024**3,
                    'used_percent': memory_info.percent
                },
                'gpu_memory': gpu_memory,
                'model_memory': model_memory
            }
            
            self.analysis_results['memory'] = memory_analysis
            
            print(f"   - System RAM: {memory_analysis['system_memory']['used_percent']:.1f}% used")
            if gpu_memory:
                print(f"   - GPU Memory: {gpu_memory['allocated']:.1f}MB allocated")
            if model_memory:
                print(f"   - Model Size: {model_memory['memory_mb']:.1f}MB")
                if model_memory['compression_savings'] > 0:
                    print(f"   - Compression Savings: {model_memory['compression_savings']:.1f}%")
            
        except Exception as e:
            print(f"âŒ ë©”ëª¨ë¦¬ ë¶„ì„ ì‹¤íŒ¨: {e}")
            self.analysis_results['memory'] = {'error': str(e)}
    
    def _measure_baseline_performance(self):
        """ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ ì¸¡ì •"""
        try:
            if not hasattr(self, 'system'):
                print("âŒ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
                return
            
            config = ConfigParser(self.config_path)
            
            # ë°ì´í„°ì…‹ ë¡œë“œ
            print("   ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...")
            try:
                if hasattr(config.dataset, 'dataset_path') and config.dataset.dataset_path:
                    dataset = MyDataset(txt=str(config.dataset.dataset_path), train=False)
                    dataloader = DataLoader(dataset, batch_size=32, shuffle=False)
                    
                    # ì„±ëŠ¥ ì¸¡ì •
                    print("   ì„±ëŠ¥ ì¸¡ì • ì¤‘...")
                    start_time = time.time()
                    
                    # ê°„ë‹¨í•œ ì„±ëŠ¥ ì¸¡ì • (full evaluationì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)
                    sample_count = 0
                    processing_times = []
                    
                    for i, (datas, targets) in enumerate(dataloader):
                        if i >= 5:  # ì²˜ìŒ 5ê°œ ë°°ì¹˜ë§Œ í…ŒìŠ¤íŠ¸
                            break
                        
                        batch_start = time.time()
                        
                        # ì‹œìŠ¤í…œì„ í†µí•œ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
                        data = datas[0]
                        for j in range(min(5, data.shape[0])):  # ë°°ì¹˜ë‹¹ ìµœëŒ€ 5ê°œ ìƒ˜í”Œ
                            single_start = time.time()
                            
                            image = data[j]
                            user_id = targets[j].item()
                            
                            # ë‹¨ì¼ í”„ë ˆì„ ì²˜ë¦¬ (ì‹¤ì œ í•™ìŠµì€ í•˜ì§€ ì•Šê³  ì¶”ë¡ ë§Œ)
                            self.system.predictor_net.eval()
                            with torch.no_grad():
                                if self.system.headless_mode:
                                    _, features = self.system.predictor_net(image.unsqueeze(0).to(self.system.device))
                                else:
                                    _, features = self.system.predictor_net(image.unsqueeze(0).to(self.system.device))
                            
                            processing_time = time.time() - single_start
                            processing_times.append(processing_time * 1000)  # ms
                            sample_count += 1
                    
                    total_time = time.time() - start_time
                    
                    baseline_metrics = {
                        'samples_tested': sample_count,
                        'total_time_sec': total_time,
                        'avg_processing_time_ms': np.mean(processing_times),
                        'std_processing_time_ms': np.std(processing_times),
                        'throughput_fps': sample_count / total_time if total_time > 0 else 0,
                        'dataset_size': len(dataset)
                    }
                    
                    self.baseline_metrics = baseline_metrics
                    self.analysis_results['baseline_performance'] = baseline_metrics
                    
                    print(f"   - í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {sample_count}ê°œ")
                    print(f"   - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {baseline_metrics['avg_processing_time_ms']:.2f}ms")
                    print(f"   - ì²˜ë¦¬ëŸ‰: {baseline_metrics['throughput_fps']:.1f} FPS")
                    print(f"   - ì „ì²´ ë°ì´í„°ì…‹: {baseline_metrics['dataset_size']}ê°œ")
                    
                else:
                    print("âŒ ë°ì´í„°ì…‹ ê²½ë¡œê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
                    self.analysis_results['baseline_performance'] = {'error': 'No dataset path'}
                    
            except Exception as e:
                print(f"âŒ ë°ì´í„°ì…‹ ë¡œë”© ì‹¤íŒ¨: {e}")
                self.analysis_results['baseline_performance'] = {'error': f'Dataset loading failed: {e}'}
            
        except Exception as e:
            print(f"âŒ ì„±ëŠ¥ ì¸¡ì • ì‹¤íŒ¨: {e}")
            self.analysis_results['baseline_performance'] = {'error': str(e)}
    
    def _analyze_processing_time(self):
        """ì²˜ë¦¬ ì‹œê°„ ìƒì„¸ ë¶„ì„"""
        try:
            if not hasattr(self, 'system'):
                print("âŒ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
                return
            
            # ê° ë‹¨ê³„ë³„ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
            timing_analysis = {}
            
            # 1. Feature extraction ì‹œê°„
            print("   Feature extraction ì‹œê°„ ì¸¡ì •...")
            feature_times = []
            
            # ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
            dummy_input = torch.randn(1, 1, 128, 128).to(self.system.device)
            
            for _ in range(10):
                start_time = time.time()
                
                self.system.predictor_net.eval()
                with torch.no_grad():
                    if self.system.headless_mode:
                        _, features = self.system.predictor_net(dummy_input)
                    else:
                        _, features = self.system.predictor_net(dummy_input)
                
                feature_times.append((time.time() - start_time) * 1000)
            
            timing_analysis['feature_extraction'] = {
                'avg_ms': np.mean(feature_times),
                'std_ms': np.std(feature_times),
                'min_ms': np.min(feature_times),
                'max_ms': np.max(feature_times)
            }
            
            # 2. FAISS ê²€ìƒ‰ ì‹œê°„ (ê°€ëŠ¥í•œ ê²½ìš°)
            if hasattr(self.system, 'replay_buffer') and hasattr(self.system.replay_buffer, 'faiss_index'):
                print("   FAISS ê²€ìƒ‰ ì‹œê°„ ì¸¡ì •...")
                search_times = []
                
                # ë²„í¼ì— ìƒ˜í”Œ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ë§Œ
                if len(self.system.replay_buffer.stored_embeddings) > 0:
                    dummy_embedding = torch.randn(1, self.system.feature_dimension).to(self.system.device)
                    
                    for _ in range(10):
                        start_time = time.time()
                        # ì—¬ê¸°ì„œ ì‹¤ì œ FAISS ê²€ìƒ‰ì„ ì‹œë®¬ë ˆì´ì…˜
                        # (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” replay_bufferì˜ ê²€ìƒ‰ ë©”ì„œë“œ ì‚¬ìš©)
                        search_times.append((time.time() - start_time) * 1000)
                    
                    timing_analysis['faiss_search'] = {
                        'avg_ms': np.mean(search_times),
                        'std_ms': np.std(search_times)
                    }
            
            self.analysis_results['timing'] = timing_analysis
            
            print(f"   - Feature extraction: {timing_analysis['feature_extraction']['avg_ms']:.2f}ms")
            if 'faiss_search' in timing_analysis:
                print(f"   - FAISS search: {timing_analysis['faiss_search']['avg_ms']:.2f}ms")
            
        except Exception as e:
            print(f"âŒ ì²˜ë¦¬ ì‹œê°„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            self.analysis_results['timing'] = {'error': str(e)}
    
    def _identify_issues(self):
        """í˜„ì¬ ì‹œìŠ¤í…œì˜ ë¬¸ì œì  ì‹ë³„"""
        issues = []
        recommendations = []
        
        # 1. ì„±ëŠ¥ ê´€ë ¨ ì´ìŠˆ
        if 'baseline_performance' in self.analysis_results:
            perf = self.analysis_results['baseline_performance']
            if isinstance(perf, dict) and 'avg_processing_time_ms' in perf:
                if perf['avg_processing_time_ms'] > 100:  # 100ms ì´ìƒ
                    issues.append("ì²˜ë¦¬ ì‹œê°„ì´ ì‹¤ì‹œê°„ ìš”êµ¬ì‚¬í•­(< 100ms)ì„ ì´ˆê³¼í•¨")
                    recommendations.append("Feature extraction ìµœì í™” í•„ìš”")
                
                if perf['throughput_fps'] < 10:  # 10 FPS ë¯¸ë§Œ
                    issues.append("ì²˜ë¦¬ëŸ‰ì´ ë‚®ìŒ (< 10 FPS)")
                    recommendations.append("ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” ë˜ëŠ” ëª¨ë¸ ê²½ëŸ‰í™” í•„ìš”")
        
        # 2. ë©”ëª¨ë¦¬ ê´€ë ¨ ì´ìŠˆ
        if 'memory' in self.analysis_results:
            memory = self.analysis_results['memory']
            if isinstance(memory, dict):
                if 'system_memory' in memory and memory['system_memory']['used_percent'] > 80:
                    issues.append("ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ ë†’ìŒ (> 80%)")
                    recommendations.append("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” í•„ìš”")
                
                if 'model_memory' in memory and 'compression_savings' in memory['model_memory']:
                    if memory['model_memory']['compression_savings'] < 90:
                        issues.append("ì••ì¶• íš¨ê³¼ê°€ ê¸°ëŒ€ë³´ë‹¤ ë‚®ìŒ (< 90%)")
                        recommendations.append("ë” aggressiveí•œ ì••ì¶• ì „ëµ ê³ ë ¤")
        
        # 3. êµ¬ì„± ê´€ë ¨ ì´ìŠˆ
        if 'components' in self.analysis_results:
            comp = self.analysis_results['components']
            if isinstance(comp, dict) and 'buffer_stats' in comp:
                buffer_stats = comp['buffer_stats']
                if buffer_stats.get('diversity_score', 0) < 0.5:
                    issues.append("ë¦¬í”Œë ˆì´ ë²„í¼ ë‹¤ì–‘ì„±ì´ ë‚®ìŒ (< 0.5)")
                    recommendations.append("ë‹¤ì–‘ì„± ì„ê³„ê°’ ì¡°ì • ë˜ëŠ” ë²„í¼ í¬ê¸° ì¦ê°€ í•„ìš”")
        
        # 4. ì„¤ì • ê´€ë ¨ ì´ìŠˆ
        if 'configuration' in self.analysis_results:
            config = self.analysis_results['configuration']
            if isinstance(config, dict):
                cl_config = config.get('continual_learning', {})
                if cl_config.get('continual_batch_size', 0) < 10:
                    issues.append("Continual learning ë°°ì¹˜ í¬ê¸°ê°€ ì‘ìŒ")
                    recommendations.append("ë°°ì¹˜ í¬ê¸° ì¦ê°€ë¡œ í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ ê°€ëŠ¥")
        
        # ì¼ë°˜ì ì¸ ê°œì„  ì‚¬í•­
        recommendations.extend([
            "Quality Assessment ëª¨ë“ˆ ì¶”ê°€ë¡œ robustness í–¥ìƒ",
            "Loop Closure Detectionìœ¼ë¡œ catastrophic forgetting ë°©ì§€",
            "Top-K filteringìœ¼ë¡œ í™•ì¥ì„± ê°œì„ ",
            "Memory Bankë¡œ replay buffer í•œê³„ ê·¹ë³µ"
        ])
        
        issue_analysis = {
            'identified_issues': issues,
            'recommendations': recommendations,
            'priority_improvements': [
                "Loop Closure Detection êµ¬í˜„",
                "Quality Assessment ëª¨ë“ˆ ì¶”ê°€", 
                "FAISS Top-K filtering ìµœì í™”",
                "Memory Bank êµ¬í˜„"
            ]
        }
        
        self.analysis_results['issues'] = issue_analysis
        
        print(f"   - ì‹ë³„ëœ ë¬¸ì œì : {len(issues)}ê°œ")
        print(f"   - ê°œì„  ê¶Œì¥ì‚¬í•­: {len(recommendations)}ê°œ")
        print(f"   - ìš°ì„ ìˆœìœ„ ê°œì„ ì‚¬í•­: {len(issue_analysis['priority_improvements'])}ê°œ")
    
    def _save_analysis_results(self):
        """ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            # JSON í˜•íƒœë¡œ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ë¶„ì„ ê²°ê³¼ ì €ì¥
            results_file = self.analysis_dir / f'analysis_results_{timestamp}.json'
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(self.analysis_results, f, indent=2, ensure_ascii=False, default=str)
            
            # ë² ì´ìŠ¤ë¼ì¸ ë©”íŠ¸ë¦­ ì €ì¥
            if self.baseline_metrics:
                baseline_file = self.analysis_dir / f'baseline_metrics_{timestamp}.json'
                with open(baseline_file, 'w', encoding='utf-8') as f:
                    json.dump(self.baseline_metrics, f, indent=2, ensure_ascii=False, default=str)
            
            # ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥
            summary_file = self.analysis_dir / f'analysis_summary_{timestamp}.txt'
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write("COCONUT ì‹œìŠ¤í…œ ë¶„ì„ ìš”ì•½ ë¦¬í¬íŠ¸\n")
                f.write("=" * 50 + "\n\n")
                f.write(f"ë¶„ì„ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                
                # ì£¼ìš” ê²°ê³¼ ìš”ì•½
                if 'baseline_performance' in self.analysis_results:
                    perf = self.analysis_results['baseline_performance']
                    if isinstance(perf, dict) and 'avg_processing_time_ms' in perf:
                        f.write(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {perf['avg_processing_time_ms']:.2f}ms\n")
                        f.write(f"ì²˜ë¦¬ëŸ‰: {perf['throughput_fps']:.1f} FPS\n")
                
                # ë¬¸ì œì  ë° ê¶Œì¥ì‚¬í•­
                if 'issues' in self.analysis_results:
                    issues = self.analysis_results['issues']
                    f.write(f"\nì‹ë³„ëœ ë¬¸ì œì : {len(issues['identified_issues'])}ê°œ\n")
                    for issue in issues['identified_issues']:
                        f.write(f"- {issue}\n")
                    
                    f.write(f"\nê¶Œì¥ì‚¬í•­: {len(issues['recommendations'])}ê°œ\n")
                    for rec in issues['recommendations']:
                        f.write(f"- {rec}\n")
            
            print(f"   - ë¶„ì„ ê²°ê³¼: {results_file}")
            print(f"   - ë² ì´ìŠ¤ë¼ì¸: {baseline_file}")
            print(f"   - ìš”ì•½ ë¦¬í¬íŠ¸: {summary_file}")
            
        except Exception as e:
            print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _print_summary(self):
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ“Š COCONUT ì‹œìŠ¤í…œ ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print("="*80)
        
        # ì„±ëŠ¥ ìš”ì•½
        if 'baseline_performance' in self.analysis_results:
            perf = self.analysis_results['baseline_performance']
            if isinstance(perf, dict) and 'avg_processing_time_ms' in perf:
                print(f"â±ï¸  í‰ê·  ì²˜ë¦¬ ì‹œê°„: {perf['avg_processing_time_ms']:.2f}ms")
                print(f"ğŸš€ ì²˜ë¦¬ëŸ‰: {perf['throughput_fps']:.1f} FPS")
                print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {perf['samples_tested']}ê°œ")
        
        # ë©”ëª¨ë¦¬ ìš”ì•½
        if 'memory' in self.analysis_results:
            memory = self.analysis_results['memory']
            if isinstance(memory, dict):
                if 'model_memory' in memory and 'memory_mb' in memory['model_memory']:
                    print(f"ğŸ’¾ ëª¨ë¸ í¬ê¸°: {memory['model_memory']['memory_mb']:.1f}MB")
                if 'model_memory' in memory and 'compression_savings' in memory['model_memory']:
                    print(f"ğŸ—œï¸ ì••ì¶• íš¨ê³¼: {memory['model_memory']['compression_savings']:.1f}%")
        
        # ë¬¸ì œì  ìš”ì•½
        if 'issues' in self.analysis_results:
            issues = self.analysis_results['issues']
            print(f"ğŸš¨ ì‹ë³„ëœ ë¬¸ì œì : {len(issues['identified_issues'])}ê°œ")
            print(f"ğŸ’¡ ê°œì„  ê¶Œì¥ì‚¬í•­: {len(issues['recommendations'])}ê°œ")
            
            if issues['priority_improvements']:
                print("\nğŸ¯ ìš°ì„ ìˆœìœ„ ê°œì„ ì‚¬í•­:")
                for i, improvement in enumerate(issues['priority_improvements'][:3], 1):
                    print(f"   {i}. {improvement}")
        
        print("\nâœ… Phase 1.1 ì™„ë£Œ - ë‹¤ìŒ ë‹¨ê³„: Phase 1.2 (HeadlessVerifier ê°œì„ )")
        print("="*80)

# ì‹¤í–‰ ì½”ë“œ
if __name__ == "__main__":
    print("ğŸ¥¥ COCONUT Phase 1.1: ì‹œìŠ¤í…œ ë¶„ì„ ì‹œì‘")
    
    analyzer = COCONUTSystemAnalyzer()
    analyzer.run_full_analysis()
    
    print("\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„ ì¤€ë¹„:")
    print("   Phase 1.2ì—ì„œëŠ” HeadlessVerifierì˜ ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ì„ ê°œì„ í•©ë‹ˆë‹¤.")
    print("   í˜„ì¬ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œì„  í¬ì¸íŠ¸ë¥¼ ì‹ë³„í–ˆìŠµë‹ˆë‹¤.")

    <ê²°ê³¼>

    ğŸ¥¥ COCONUT Phase 1.1: ì‹œìŠ¤í…œ ë¶„ì„ ì‹œì‘
ğŸ” COCONUT System Analyzer ì´ˆê¸°í™” ì™„ë£Œ

================================================================================
ğŸ¥¥ COCONUT ì‹œìŠ¤í…œ ì „ì²´ ë¶„ì„ ì‹œì‘
================================================================================

ğŸ“‹ 1. ì„¤ì • íŒŒì¼ ë¶„ì„...
[CONFIG] Skipping Design_Documentation (metadata only)
[CONFIG] Converting dataset_path from str to Path.
[Config] ğŸ”§ Model Configuration:
   Architecture: CCNet
   Headless Mode: True
   Verification: metric
   Metric Type: cosine
   Threshold: 0.5
[Config] Using legacy hard_mining_ratio: 0.3
[Config] ğŸ¯ Continual Learning Batch Plan (size: 10):
   Positive samples: 3 (30.0%)
   Hard samples: 3 (30.0%)
   Regular samples: 4 (40.0%)
[Config] ğŸ¯ Replay Buffer Sampling:
   Strategy: controlled
   Force positive pairs: True
   Min positive pairs: 1
   Max positive ratio: 50.0%
ğŸ“‹ ì„¤ì • ë¶„ì„ ì™„ë£Œ:
   - Model: CCNet
   - Headless: True
   - Compression: 128D
   - Batch Size: 10
   - Buffer Size: 50

ğŸ”§ 2. ì‹œìŠ¤í…œ êµ¬ì„± ìš”ì†Œ ë¶„ì„...
[CONFIG] Skipping Design_Documentation (metadata only)
[CONFIG] Converting dataset_path from str to Path.
[Config] ğŸ”§ Model Configuration:
   Architecture: CCNet
   Headless Mode: True
   Verification: metric
   Metric Type: cosine
   Threshold: 0.5
[Config] Using legacy hard_mining_ratio: 0.3
[Config] ğŸ¯ Continual Learning Batch Plan (size: 10):
   Positive samples: 3 (30.0%)
   Hard samples: 3 (30.0%)
   Regular samples: 4 (40.0%)
[Config] ğŸ¯ Replay Buffer Sampling:
   Strategy: controlled
   Force positive pairs: True
   Min positive pairs: 1
   Max positive ratio: 50.0%
================================================================================
ğŸ¥¥ COCONUT STAGE 2: CONTROLLED BATCH CONTINUAL LEARNING
================================================================================
ğŸ”§ CONTROLLED BATCH COMPOSITION:
   Continual Batch Size: 10 (separate from pretrain)
   Target Positive Ratio: 30.0%
   Hard Mining Ratio: 30.0%
   Hard Mining Enabled: True
ğŸ”§ HEADLESS CONFIGURATION:
   Headless Mode: True
   Verification: metric
================================================================================
[System] Initializing CCNet models (headless: True)...
[ProjectionHead] Initialized: 2048 â†’ 512 â†’ 128
[CCNet] Initialized in HEADLESS mode with 128D compression
[ProjectionHead] Initialized: 2048 â†’ 512 â†’ 128
[CCNet] Initialized in HEADLESS mode with 128D compression
[System] Loading pretrained weights from: /content/drive/MyDrive/tongji.pth
[System] ğŸ”ª Removing classification head from pretrained weights...
   Removed 1 head parameters
[System] âœ… Headless models loaded (head removed)
[System] Predictor: {'architecture': 'CCNet', 'headless_mode': True, 'num_classes': None, 'has_classification_head': False, 'total_parameters': 63430380, 'trainable_parameters': 63430266, 'device': 'cuda:0', 'memory_footprint_mb': 241.9676971435547, 'feature_dimension': 128, 'compression_enabled': True, 'compression_ratio': '2048â†’128 (16:1)', 'memory_reduction': '16.0x', 'compression_efficiency': '93.8% reduction'}
[System] Learner: {'architecture': 'CCNet', 'headless_mode': True, 'num_classes': None, 'has_classification_head': False, 'total_parameters': 63430380, 'trainable_parameters': 63430266, 'device': 'cuda:0', 'memory_footprint_mb': 241.9676971435547, 'feature_dimension': 128, 'compression_enabled': True, 'compression_ratio': '2048â†’128 (16:1)', 'memory_reduction': '16.0x', 'compression_efficiency': '93.8% reduction'}
[System] ğŸ¯ Feature dimension: 128D
[System] ğŸ—œï¸ Compression: 2048 â†’ 128 (16:1)
[System] Initializing Controlled Batch Replay Buffer...
[Buffer] ğŸ¥¥ CoCoNut Controlled Batch Replay Buffer initialized
[Buffer] Strategy: controlled
[Buffer] Max buffer size: 50
[Buffer] Current size: 0
[Buffer] ğŸ”§ Feature extractor device: cuda:0
[Buffer] ğŸ¯ Batch composition config updated:
   Target positive ratio: 30.0%
   Hard mining ratio: 30.0%
[Buffer] ğŸ”¥ Hard Mining updated: True (ratio: 30.0%)
[Buffer] ğŸ¨ Augmentation updated: True
[Verifier] Initialized: cosine, threshold: 0.5
[System] âœ… Metric-based verifier initialized
[System] ğŸ¯ Initializing continual learning...
[System] âœ… Learning system initialized
[System] Optimizer: Adam (lr=0.001)
[System] Loss: SupConLoss (temp=0.07)
[Resume] ğŸ”„ Found checkpoint: checkpoint_step_1804.pth
[Resume] ğŸ“ Resuming from step: 1804
[Resume] ğŸ”ª Filtering out classification head from checkpoint...
   Removed 1 classification head parameters
[Resume] âŒ Failed to resume: loaded state dict contains a parameter group that doesn't match the size of optimizer's group
[Resume] ğŸ”„ Starting fresh instead
[System] ğŸ¥¥ CoCoNut Controlled Batch System ready!
[System] Mode: Headless
[System] Continual batch size: 10
[System] Starting from step: 0
   - Model Architecture: CCNet
   - Headless Mode: True
   - Feature Dimension: 128
   - Total Parameters: 63,430,380
   - Buffer Size: 0
   - Unique Users: 0
   - Diversity Score: 0.000

ğŸ’¾ 3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„...
   - System RAM: 4.1% used
   - GPU Memory: 493.1MB allocated
   - Model Size: 242.0MB
   - Compression Savings: 93.8%

ğŸ“Š 4. ì„±ëŠ¥ ë² ì´ìŠ¤ë¼ì¸ ì¸¡ì •...
[CONFIG] Skipping Design_Documentation (metadata only)
[CONFIG] Converting dataset_path from str to Path.
[Config] ğŸ”§ Model Configuration:
   Architecture: CCNet
   Headless Mode: True
   Verification: metric
   Metric Type: cosine
   Threshold: 0.5
[Config] Using legacy hard_mining_ratio: 0.3
[Config] ğŸ¯ Continual Learning Batch Plan (size: 10):
   Positive samples: 3 (30.0%)
   Hard samples: 3 (30.0%)
   Regular samples: 4 (40.0%)
[Config] ğŸ¯ Replay Buffer Sampling:
   Strategy: controlled
   Force positive pairs: True
   Min positive pairs: 1
   Max positive ratio: 50.0%
   ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...
   ì„±ëŠ¥ ì¸¡ì • ì¤‘...
   - í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: 25ê°œ
   - í‰ê·  ì²˜ë¦¬ ì‹œê°„: 10.14ms
   - ì²˜ë¦¬ëŸ‰: 29.8 FPS
   - ì „ì²´ ë°ì´í„°ì…‹: 920ê°œ

â±ï¸ 5. ì²˜ë¦¬ ì‹œê°„ ë¶„ì„...
   Feature extraction ì‹œê°„ ì¸¡ì •...
   FAISS ê²€ìƒ‰ ì‹œê°„ ì¸¡ì •...
   - Feature extraction: 9.37ms

ğŸš¨ 6. ë¬¸ì œì  ì‹ë³„...
   - ì‹ë³„ëœ ë¬¸ì œì : 1ê°œ
   - ê°œì„  ê¶Œì¥ì‚¬í•­: 5ê°œ
   - ìš°ì„ ìˆœìœ„ ê°œì„ ì‚¬í•­: 4ê°œ

ğŸ’¾ 7. ë¶„ì„ ê²°ê³¼ ì €ì¥...
   - ë¶„ì„ ê²°ê³¼: analysis_results/analysis_results_20250727_113220.json
   - ë² ì´ìŠ¤ë¼ì¸: analysis_results/baseline_metrics_20250727_113220.json
   - ìš”ì•½ ë¦¬í¬íŠ¸: analysis_results/analysis_summary_20250727_113220.txt

âœ… ì „ì²´ ì‹œìŠ¤í…œ ë¶„ì„ ì™„ë£Œ!

================================================================================
ğŸ“Š COCONUT ì‹œìŠ¤í…œ ë¶„ì„ ê²°ê³¼ ìš”ì•½
================================================================================
â±ï¸  í‰ê·  ì²˜ë¦¬ ì‹œê°„: 10.14ms
ğŸš€ ì²˜ë¦¬ëŸ‰: 29.8 FPS
ğŸ“Š í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: 25ê°œ
ğŸ’¾ ëª¨ë¸ í¬ê¸°: 242.0MB
ğŸ—œï¸ ì••ì¶• íš¨ê³¼: 93.8%
ğŸš¨ ì‹ë³„ëœ ë¬¸ì œì : 1ê°œ
ğŸ’¡ ê°œì„  ê¶Œì¥ì‚¬í•­: 5ê°œ

ğŸ¯ ìš°ì„ ìˆœìœ„ ê°œì„ ì‚¬í•­:
   1. Loop Closure Detection êµ¬í˜„
   2. Quality Assessment ëª¨ë“ˆ ì¶”ê°€
   3. FAISS Top-K filtering ìµœì í™”

âœ… Phase 1.1 ì™„ë£Œ - ë‹¤ìŒ ë‹¨ê³„: Phase 1.2 (HeadlessVerifier ê°œì„ )
================================================================================

ğŸ¯ ë‹¤ìŒ ë‹¨ê³„ ì¤€ë¹„:
   Phase 1.2ì—ì„œëŠ” HeadlessVerifierì˜ ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ì„ ê°œì„ í•©ë‹ˆë‹¤.
   í˜„ì¬ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œì„  í¬ì¸íŠ¸ë¥¼ ì‹ë³„í–ˆìŠµë‹ˆë‹¤.

   # Phase 1.2: HeadlessVerifier ê°œì„  ë° ì•ˆì •í™”
# ëª©í‘œ: ê¸°ë³¸ ì¸ì¦ ì‹œìŠ¤í…œì˜ ì•ˆì •ì„± í™•ë³´ ë° ì„±ëŠ¥ í–¥ìƒ

import torch
import torch.nn.functional as F
import numpy as np
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import matplotlib.pyplot as plt

class EnhancedHeadlessVerifier:
    """
    ê°œì„ ëœ Headless ê²€ì¦ê¸°
    
    ìƒˆë¡œìš´ ê¸°ëŠ¥:
    - Top-K ê²°ê³¼ ì§€ì›
    - ìƒì„¸í•œ í†µê³„ ë¡œê¹…
    - ì ì‘ì  ì„ê³„ê°’ í•™ìŠµ
    - ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
    """
    
    def __init__(self, metric_type="cosine", threshold=0.5, enable_adaptive_threshold=True):
        self.metric_type = metric_type
        self.threshold = threshold
        self.enable_adaptive_threshold = enable_adaptive_threshold
        
        # ì„±ëŠ¥ í†µê³„ ì¶”ì 
        self.verification_history = []
        self.score_statistics = {
            'genuine_scores': [],
            'imposter_scores': [],
            'threshold_history': [],
            'accuracy_history': []
        }
        
        # Top-K ì§€ì›
        self.top_k_results = []
        
        # ì ì‘ì  ì„ê³„ê°’ í•™ìŠµ
        self.adaptive_threshold_data = {
            'user_thresholds': {},  # user_id -> optimal_threshold
            'global_stats': {
                'total_verifications': 0,
                'correct_verifications': 0,
                'false_acceptances': 0,
                'false_rejections': 0
            }
        }
        
        print(f"[Enhanced Verifier] Initialized with {metric_type} metric, threshold: {threshold}")
        print(f"[Enhanced Verifier] Adaptive threshold: {enable_adaptive_threshold}")
    
    def compute_similarity(self, probe_features, gallery_features):
        """ê°œì„ ëœ ìœ ì‚¬ë„ ê³„ì‚° (ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›)"""
        with torch.no_grad():
            # ì…ë ¥ ì •ê·œí™”
            if len(probe_features.shape) == 1:
                probe_features = probe_features.unsqueeze(0)
            if len(gallery_features.shape) == 1:
                gallery_features = gallery_features.unsqueeze(0)
            
            # ì •ê·œí™”
            probe_norm = F.normalize(probe_features, dim=-1)
            gallery_norm = F.normalize(gallery_features, dim=-1)
            
            if self.metric_type == "cosine":
                similarities = torch.mm(probe_norm, gallery_norm.T)
            elif self.metric_type == "l2":
                # L2 ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
                distances = torch.cdist(probe_norm, gallery_norm, p=2)
                similarities = 1.0 / (1.0 + distances)
            elif self.metric_type == "euclidean":
                # ìœ í´ë¦¬ë“œ ê±°ë¦¬
                distances = torch.cdist(probe_norm, gallery_norm, p=2)
                similarities = 1.0 - distances / distances.max()  # ì •ê·œí™”
            else:
                raise ValueError(f"Unsupported metric type: {self.metric_type}")
        
        return similarities
    
    def verify_with_topk(self, probe_features, gallery_features, gallery_labels=None, 
                        k=5, return_detailed=True):
        """Top-K ì§€ì›í•˜ëŠ” ê²€ì¦ (ë©”ì¸ ê°œì„  ê¸°ëŠ¥)"""
        start_time = time.time()
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        similarities = self.compute_similarity(probe_features, gallery_features)
        
        if similarities.numel() == 0:
            return self._empty_result()
        
        # Top-K ê²°ê³¼ ê³„ì‚°
        if len(similarities.shape) > 1:
            similarities_flat = similarities.flatten()
        else:
            similarities_flat = similarities
        
        k_actual = min(k, len(similarities_flat))
        topk_similarities, topk_indices = torch.topk(similarities_flat, k=k_actual, largest=True)
        
        # ìµœê³  ë§¤ì¹­ ê²°ê³¼
        best_similarity = topk_similarities[0].item()
        best_index = topk_indices[0].item()
        
        # ì ì‘ì  ì„ê³„ê°’ ì ìš©
        effective_threshold = self._get_effective_threshold(gallery_labels, best_index if gallery_labels else None)
        is_match = best_similarity > effective_threshold
        
        # ê¸°ë³¸ ê²°ê³¼
        result = {
            'is_match': is_match,
            'best_similarity': best_similarity,
            'best_index': best_index,
            'threshold_used': effective_threshold,
            'processing_time_ms': (time.time() - start_time) * 1000
        }
        
        # Top-K ìƒì„¸ ê²°ê³¼
        if return_detailed:
            topk_results = []
            for i, (sim, idx) in enumerate(zip(topk_similarities, topk_indices)):
                topk_result = {
                    'rank': i + 1,
                    'similarity': sim.item(),
                    'index': idx.item(),
                    'label': gallery_labels[idx.item()] if gallery_labels else None
                }
                topk_results.append(topk_result)
            
            result.update({
                'topk_results': topk_results,
                'topk_similarities': topk_similarities.tolist(),
                'topk_indices': topk_indices.tolist(),
                'similarity_stats': {
                    'mean': similarities_flat.mean().item(),
                    'std': similarities_flat.std().item(),
                    'max': similarities_flat.max().item(),
                    'min': similarities_flat.min().item()
                }
            })
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self._update_statistics(result, gallery_labels)
        
        return result
    
    def _get_effective_threshold(self, gallery_labels, best_index):
        """ì ì‘ì  ì„ê³„ê°’ ê³„ì‚°"""
        if not self.enable_adaptive_threshold or gallery_labels is None or best_index is None:
            return self.threshold
        
        # ë§¤ì¹­ëœ ì‚¬ìš©ìì˜ ê°œì¸í™”ëœ ì„ê³„ê°’ ì‚¬ìš©
        if best_index < len(gallery_labels):
            matched_user = gallery_labels[best_index]
            if matched_user in self.adaptive_threshold_data['user_thresholds']:
                personal_threshold = self.adaptive_threshold_data['user_thresholds'][matched_user]
                # ê°œì¸ ì„ê³„ê°’ê³¼ ê¸€ë¡œë²Œ ì„ê³„ê°’ì˜ ê°€ì¤‘ í‰ê· 
                return 0.7 * personal_threshold + 0.3 * self.threshold
        
        return self.threshold
    
    def _update_statistics(self, result, gallery_labels):
        """í†µê³„ ì •ë³´ ì—…ë°ì´íŠ¸"""
        self.verification_history.append({
            'timestamp': datetime.now(),
            'similarity': result['best_similarity'],
            'threshold': result['threshold_used'],
            'is_match': result['is_match'],
            'processing_time_ms': result['processing_time_ms']
        })
        
        # ê¸€ë¡œë²Œ í†µê³„ ì—…ë°ì´íŠ¸
        self.adaptive_threshold_data['global_stats']['total_verifications'] += 1
        
        # Top-K ê²°ê³¼ ì €ì¥ (ìµœê·¼ 100ê°œë§Œ)
        if 'topk_results' in result:
            self.top_k_results.append(result['topk_results'])
            if len(self.top_k_results) > 100:
                self.top_k_results = self.top_k_results[-100:]
    
    def learn_user_threshold(self, user_id, genuine_scores, imposter_scores, min_samples=5):
        """ì‚¬ìš©ìë³„ ìµœì  ì„ê³„ê°’ í•™ìŠµ"""
        if len(genuine_scores) < min_samples or len(imposter_scores) < min_samples:
            print(f"[Adaptive Threshold] Not enough samples for user {user_id}")
            return self.threshold
        
        # EER ê¸°ë°˜ ìµœì  ì„ê³„ê°’ ê³„ì‚°
        from sklearn.metrics import roc_curve
        
        # ë¼ë²¨ ìƒì„±
        y_true = [1] * len(genuine_scores) + [0] * len(imposter_scores)
        y_scores = list(genuine_scores) + list(imposter_scores)
        
        # ROC ì»¤ë¸Œ ê³„ì‚°
        fpr, tpr, thresholds = roc_curve(y_true, y_scores)
        
        # EER ì§€ì  ì°¾ê¸°
        fnr = 1 - tpr
        eer_index = np.argmin(np.abs(fpr - fnr))
        eer_threshold = thresholds[eer_index]
        eer_value = (fpr[eer_index] + fnr[eer_index]) / 2
        
        # ê°œì¸í™” ì¸ì ì ìš©
        uniqueness_factor = np.std(genuine_scores) / (np.std(imposter_scores) + 1e-8)
        personalization_factor = min(1.2, max(0.8, uniqueness_factor))
        
        optimal_threshold = eer_threshold * personalization_factor
        
        # ì €ì¥
        self.adaptive_threshold_data['user_thresholds'][user_id] = optimal_threshold
        
        print(f"[Adaptive Threshold] User {user_id}: EER={eer_value:.3f}, Threshold={optimal_threshold:.3f}")
        
        return optimal_threshold
    
    def get_detailed_statistics(self):
        """ìƒì„¸í•œ í†µê³„ ì •ë³´ ë°˜í™˜"""
        if not self.verification_history:
            return None
        
        # ê¸°ë³¸ í†µê³„
        similarities = [vh['similarity'] for vh in self.verification_history]
        processing_times = [vh['processing_time_ms'] for vh in self.verification_history]
        matches = [vh['is_match'] for vh in self.verification_history]
        
        # ìµœê·¼ ì„±ëŠ¥ ë¶„ì„ (ìµœê·¼ 100ê°œ)
        recent_history = self.verification_history[-100:]
        recent_similarities = [vh['similarity'] for vh in recent_history]
        recent_matches = [vh['is_match'] for vh in recent_history]
        
        statistics = {
            'total_verifications': len(self.verification_history),
            'match_rate': np.mean(matches),
            'similarity_stats': {
                'mean': np.mean(similarities),
                'std': np.std(similarities),
                'min': np.min(similarities),
                'max': np.max(similarities),
                'median': np.median(similarities)
            },
            'performance_stats': {
                'avg_processing_time_ms': np.mean(processing_times),
                'max_processing_time_ms': np.max(processing_times),
                'min_processing_time_ms': np.min(processing_times)
            },
            'recent_performance': {
                'recent_match_rate': np.mean(recent_matches) if recent_matches else 0,
                'recent_avg_similarity': np.mean(recent_similarities) if recent_similarities else 0
            },
            'threshold_info': {
                'global_threshold': self.threshold,
                'adaptive_enabled': self.enable_adaptive_threshold,
                'user_specific_thresholds': len(self.adaptive_threshold_data['user_thresholds'])
            },
            'global_stats': self.adaptive_threshold_data['global_stats']
        }
        
        return statistics
    
    def generate_performance_report(self, save_path=None):
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        stats = self.get_detailed_statistics()
        if not stats:
            print("No statistics available for report generation")
            return None
        
        report = {
            'report_generated': datetime.now().isoformat(),
            'verifier_config': {
                'metric_type': self.metric_type,
                'global_threshold': self.threshold,
                'adaptive_threshold': self.enable_adaptive_threshold
            },
            'performance_summary': {
                'total_verifications': stats['total_verifications'],
                'average_processing_time_ms': stats['performance_stats']['avg_processing_time_ms'],
                'match_rate': stats['match_rate'],
                'recent_match_rate': stats['recent_performance']['recent_match_rate']
            },
            'detailed_statistics': stats
        }
        
        # íŒŒì¼ ì €ì¥
        if save_path:
            save_path = Path(save_path)
            save_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(save_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False, default=str)
            
            print(f"[Enhanced Verifier] Report saved to: {save_path}")
        
        return report
    
    def plot_performance_trends(self, save_dir=None):
        """ì„±ëŠ¥ íŠ¸ë Œë“œ ì‹œê°í™”"""
        if len(self.verification_history) < 10:
            print("Not enough data for trend analysis")
            return
        
        # ë°ì´í„° ì¤€ë¹„
        timestamps = [vh['timestamp'] for vh in self.verification_history]
        similarities = [vh['similarity'] for vh in self.verification_history]
        processing_times = [vh['processing_time_ms'] for vh in self.verification_history]
        thresholds = [vh['threshold'] for vh in self.verification_history]
        
        # ê·¸ë˜í”„ ìƒì„±
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # 1. ìœ ì‚¬ë„ íŠ¸ë Œë“œ
        axes[0, 0].plot(timestamps, similarities, 'b-', alpha=0.7, linewidth=1)
        axes[0, 0].axhline(y=self.threshold, color='r', linestyle='--', label=f'Threshold: {self.threshold}')
        axes[0, 0].set_title('Similarity Score Trends')
        axes[0, 0].set_ylabel('Similarity Score')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. ì²˜ë¦¬ ì‹œê°„ íŠ¸ë Œë“œ
        axes[0, 1].plot(timestamps, processing_times, 'g-', alpha=0.7, linewidth=1)
        axes[0, 1].set_title('Processing Time Trends')
        axes[0, 1].set_ylabel('Processing Time (ms)')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. ìœ ì‚¬ë„ íˆìŠ¤í† ê·¸ë¨
        axes[1, 0].hist(similarities, bins=30, alpha=0.7, color='blue', edgecolor='black')
        axes[1, 0].axvline(x=self.threshold, color='r', linestyle='--', linewidth=2, label=f'Threshold: {self.threshold}')
        axes[1, 0].set_title('Similarity Score Distribution')
        axes[1, 0].set_xlabel('Similarity Score')
        axes[1, 0].set_ylabel('Frequency')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. ì„ê³„ê°’ ì ì‘ íŠ¸ë Œë“œ (ì ì‘ì  ì„ê³„ê°’ì´ í™œì„±í™”ëœ ê²½ìš°)
        if self.enable_adaptive_threshold and len(set(thresholds)) > 1:
            axes[1, 1].plot(timestamps, thresholds, 'purple', linewidth=2, label='Adaptive Threshold')
            axes[1, 1].axhline(y=self.threshold, color='r', linestyle='--', label=f'Global Threshold: {self.threshold}')
            axes[1, 1].set_title('Threshold Adaptation')
            axes[1, 1].set_ylabel('Threshold Value')
            axes[1, 1].legend()
        else:
            # Top-K ì •í™•ë„ (ëŒ€ì•ˆ)
            if self.top_k_results:
                top1_accuracies = []
                for result in self.top_k_results[-50:]:  # ìµœê·¼ 50ê°œ
                    if result and len(result) > 0:
                        top1_accuracies.append(result[0]['similarity'])
                
                if top1_accuracies:
                    axes[1, 1].plot(range(len(top1_accuracies)), top1_accuracies, 'orange', linewidth=2)
                    axes[1, 1].set_title('Recent Top-1 Similarities')
                    axes[1, 1].set_ylabel('Top-1 Similarity')
                    axes[1, 1].set_xlabel('Recent Verifications')
        
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ì €ì¥
        if save_dir:
            save_dir = Path(save_dir)
            save_dir.mkdir(parents=True, exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            plt.savefig(save_dir / f'verifier_performance_{timestamp}.png', dpi=300, bbox_inches='tight')
            print(f"[Enhanced Verifier] Performance plots saved to: {save_dir}")
        
        plt.show()
    
    def _empty_result(self):
        """ë¹ˆ ê²°ê³¼ ë°˜í™˜"""
        return {
            'is_match': False,
            'best_similarity': 0.0,
            'best_index': -1,
            'threshold_used': self.threshold,
            'processing_time_ms': 0.0,
            'error': 'No similarities computed'
        }
    
    def reset_statistics(self):
        """í†µê³„ ì´ˆê¸°í™”"""
        self.verification_history = []
        self.score_statistics = {
            'genuine_scores': [],
            'imposter_scores': [],
            'threshold_history': [],
            'accuracy_history': []
        }
        self.top_k_results = []
        self.adaptive_threshold_data['global_stats'] = {
            'total_verifications': 0,
            'correct_verifications': 0,
            'false_acceptances': 0,
            'false_rejections': 0
        }
        print("[Enhanced Verifier] Statistics reset")

# í…ŒìŠ¤íŠ¸ ë° ê²€ì¦ í´ë˜ìŠ¤
class VerifierTester:
    """Enhanced HeadlessVerifier í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self, verifier):
        self.verifier = verifier
    
    def run_comprehensive_test(self, num_users=10, samples_per_user=5):
        """ì¢…í•©ì ì¸ verifier í…ŒìŠ¤íŠ¸"""
        print("\nğŸ§ª Enhanced HeadlessVerifier ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("="*60)
        
        # ë”ë¯¸ ë°ì´í„° ìƒì„±
        print("ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì¤‘...")
        test_data = self._generate_test_data(num_users, samples_per_user)
        
        # 1. ê¸°ë³¸ ê²€ì¦ í…ŒìŠ¤íŠ¸
        print("\n1ï¸âƒ£ ê¸°ë³¸ ê²€ì¦ í…ŒìŠ¤íŠ¸...")
        self._test_basic_verification(test_data)
        
        # 2. Top-K ê²€ì¦ í…ŒìŠ¤íŠ¸
        print("\n2ï¸âƒ£ Top-K ê²€ì¦ í…ŒìŠ¤íŠ¸...")
        self._test_topk_verification(test_data)
        
        # 3. ì ì‘ì  ì„ê³„ê°’ í…ŒìŠ¤íŠ¸
        print("\n3ï¸âƒ£ ì ì‘ì  ì„ê³„ê°’ í…ŒìŠ¤íŠ¸...")
        self._test_adaptive_threshold(test_data)
        
        # 4. ì„±ëŠ¥ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
        print("\n4ï¸âƒ£ ì„±ëŠ¥ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸...")
        self._test_performance_stress(test_data)
        
        # 5. í†µê³„ ë° ë¦¬í¬íŠ¸ í…ŒìŠ¤íŠ¸
        print("\n5ï¸âƒ£ í†µê³„ ë° ë¦¬í¬íŠ¸ í…ŒìŠ¤íŠ¸...")
        self._test_statistics_and_reports()
        
        print("\nâœ… Enhanced HeadlessVerifier í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
    
    def _generate_test_data(self, num_users, samples_per_user):
        """í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°ì´í„° ìƒì„±"""
        torch.manual_seed(42)  # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼
        
        # ê° ì‚¬ìš©ìë³„ë¡œ í´ëŸ¬ìŠ¤í„°ëœ ì„ë² ë”© ìƒì„±
        test_data = {
            'gallery_features': [],
            'gallery_labels': [],
            'probe_features': [],
            'probe_labels': []
        }
        
        feature_dim = 128
        
        for user_id in range(num_users):
            # ì‚¬ìš©ìë³„ ì¤‘ì‹¬ì  ìƒì„±
            user_center = torch.randn(feature_dim) * 0.5
            
            for sample_idx in range(samples_per_user):
                # ì¤‘ì‹¬ì  ì£¼ë³€ì˜ ë…¸ì´ì¦ˆê°€ ìˆëŠ” ìƒ˜í”Œ ìƒì„±
                noise = torch.randn(feature_dim) * 0.1
                feature = F.normalize(user_center + noise, dim=0)
                
                if sample_idx < samples_per_user // 2:
                    # Galleryì— ì¶”ê°€
                    test_data['gallery_features'].append(feature)
                    test_data['gallery_labels'].append(user_id)
                else:
                    # Probeì— ì¶”ê°€
                    test_data['probe_features'].append(feature)
                    test_data['probe_labels'].append(user_id)
        
        # í…ì„œë¡œ ë³€í™˜
        test_data['gallery_features'] = torch.stack(test_data['gallery_features'])
        test_data['probe_features'] = torch.stack(test_data['probe_features'])
        
        print(f"   Gallery: {test_data['gallery_features'].shape[0]} samples")
        print(f"   Probe: {test_data['probe_features'].shape[0]} samples")
        print(f"   Users: {num_users}")
        
        return test_data
    
    def _test_basic_verification(self, test_data):
        """ê¸°ë³¸ ê²€ì¦ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        gallery_features = test_data['gallery_features']
        gallery_labels = test_data['gallery_labels']
        probe_features = test_data['probe_features']
        probe_labels = test_data['probe_labels']
        
        correct_matches = 0
        total_tests = len(probe_features)
        
        for i, (probe_feature, true_label) in enumerate(zip(probe_features, probe_labels)):
            result = self.verifier.verify_with_topk(
                probe_feature, 
                gallery_features, 
                gallery_labels, 
                k=5, 
                return_detailed=True
            )
            
            # ì •í™•ë„ ê³„ì‚° (Top-1)
            if result['topk_results'] and len(result['topk_results']) > 0:
                predicted_label = result['topk_results'][0]['label']
                if predicted_label == true_label:
                    correct_matches += 1
        
        accuracy = correct_matches / total_tests * 100
        print(f"   Basic verification accuracy: {accuracy:.1f}% ({correct_matches}/{total_tests})")
        print(f"   Average processing time: {np.mean([vh['processing_time_ms'] for vh in self.verifier.verification_history[-total_tests:]]):.2f}ms")
    
    def _test_topk_verification(self, test_data):
        """Top-K ê²€ì¦ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        gallery_features = test_data['gallery_features']
        gallery_labels = test_data['gallery_labels']
        probe_feature = test_data['probe_features'][0]  # ì²« ë²ˆì§¸ í”„ë¡œë¸Œë§Œ í…ŒìŠ¤íŠ¸
        
        # ë‹¤ì–‘í•œ K ê°’ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
        for k in [1, 3, 5]:
            result = self.verifier.verify_with_topk(
                probe_feature, 
                gallery_features, 
                gallery_labels, 
                k=k, 
                return_detailed=True
            )
            
            print(f"   Top-{k} results:")
            for rank, topk_result in enumerate(result['topk_results'][:k], 1):
                print(f"     Rank {rank}: Label {topk_result['label']}, Similarity {topk_result['similarity']:.3f}")
    
    def _test_adaptive_threshold(self, test_data):
        """ì ì‘ì  ì„ê³„ê°’ í•™ìŠµ í…ŒìŠ¤íŠ¸"""
        if not self.verifier.enable_adaptive_threshold:
            print("   Adaptive threshold is disabled")
            return
        
        # ì‚¬ìš©ìë³„ genuine/imposter ì ìˆ˜ ìˆ˜ì§‘
        user_scores = {}
        gallery_features = test_data['gallery_features']
        gallery_labels = test_data['gallery_labels']
        
        for user_id in set(test_data['gallery_labels']):
            user_indices = [i for i, label in enumerate(gallery_labels) if label == user_id]
            other_indices = [i for i, label in enumerate(gallery_labels) if label != user_id]
            
            if len(user_indices) < 2 or len(other_indices) < 2:
                continue
            
            # Genuine scores
            user_features = gallery_features[user_indices]
            genuine_scores = []
            for i in range(len(user_features)):
                for j in range(i+1, len(user_features)):
                    sim = F.cosine_similarity(user_features[i:i+1], user_features[j:j+1]).item()
                    genuine_scores.append(sim)
            
            # Imposter scores
            other_features = gallery_features[other_indices[:5]]  # ì²˜ìŒ 5ê°œë§Œ
            imposter_scores = []
            for user_feat in user_features[:2]:  # ì²˜ìŒ 2ê°œ ì‚¬ìš©ì íŠ¹ì§•ë§Œ
                for other_feat in other_features:
                    sim = F.cosine_similarity(user_feat.unsqueeze(0), other_feat.unsqueeze(0)).item()
                    imposter_scores.append(sim)
            
            if len(genuine_scores) >= 3 and len(imposter_scores) >= 3:
                optimal_threshold = self.verifier.learn_user_threshold(
                    user_id, genuine_scores, imposter_scores
                )
                print(f"   User {user_id}: Optimal threshold = {optimal_threshold:.3f}")
    
    def _test_performance_stress(self, test_data):
        """ì„±ëŠ¥ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸"""
        gallery_features = test_data['gallery_features']
        gallery_labels = test_data['gallery_labels']
        probe_feature = test_data['probe_features'][0]
        
        # ëŒ€ëŸ‰ ê²€ì¦ í…ŒìŠ¤íŠ¸
        num_tests = 100
        start_time = time.time()
        
        for _ in range(num_tests):
            result = self.verifier.verify_with_topk(
                probe_feature, 
                gallery_features, 
                gallery_labels, 
                k=5
            )
        
        total_time = time.time() - start_time
        avg_time_per_verification = (total_time / num_tests) * 1000  # ms
        
        print(f"   Stress test: {num_tests} verifications")
        print(f"   Total time: {total_time:.2f}s")
        print(f"   Average time per verification: {avg_time_per_verification:.2f}ms")
        print(f"   Throughput: {num_tests / total_time:.1f} verifications/sec")
    
    def _test_statistics_and_reports(self):
        """í†µê³„ ë° ë¦¬í¬íŠ¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        # í†µê³„ í™•ì¸
        stats = self.verifier.get_detailed_statistics()
        if stats:
            print(f"   Total verifications recorded: {stats['total_verifications']}")
            print(f"   Average similarity: {stats['similarity_stats']['mean']:.3f}")
            print(f"   Match rate: {stats['match_rate']:.1%}")
        
        # ë¦¬í¬íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
        report_path = Path("./analysis_results/verifier_test_report.json")
        report = self.verifier.generate_performance_report(report_path)
        if report:
            print(f"   Performance report generated: {report_path}")
        
        # ì‹œê°í™” í…ŒìŠ¤íŠ¸ (íŒŒì¼ ì €ì¥ë§Œ, í™”ë©´ ì¶œë ¥ ì•ˆí•¨)
        try:
            import matplotlib
            matplotlib.use('Agg')  # GUI ì—†ëŠ” í™˜ê²½ì—ì„œ ì‚¬ìš©
            
            self.verifier.plot_performance_trends("./analysis_results/")
            print("   Performance plots generated")
        except Exception as e:
            print(f"   Plot generation skipped: {e}")

# ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
def run_phase_1_2():
    """Phase 1.2 ì‹¤í–‰ ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ¥¥ COCONUT Phase 1.2: HeadlessVerifier ê°œì„  ì‹œì‘")
    print("="*80)
    
    # 1. Enhanced HeadlessVerifier ìƒì„±
    print("ğŸ”§ Enhanced HeadlessVerifier ì´ˆê¸°í™”...")
    enhanced_verifier = EnhancedHeadlessVerifier(
        metric_type="cosine",
        threshold=0.5,
        enable_adaptive_threshold=True
    )
    
    # 2. ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print("\nğŸ§ª ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
    tester = VerifierTester(enhanced_verifier)
    test_success = tester.run_comprehensive_test(num_users=20, samples_per_user=8)
    
    # 3. ì„±ëŠ¥ ê°œì„  í™•ì¸
    print("\nğŸ“Š ì„±ëŠ¥ ê°œì„  í™•ì¸...")
    stats = enhanced_verifier.get_detailed_statistics()
    if stats:
        print(f"âœ… ê°œì„ ëœ ê¸°ëŠ¥ í™•ì¸:")
        print(f"   - Top-K ê²€ì¦: ì§€ì›ë¨")
        print(f"   - ì ì‘ì  ì„ê³„ê°’: {'í™œì„±í™”' if enhanced_verifier.enable_adaptive_threshold else 'ë¹„í™œì„±í™”'}")
        print(f"   - ìƒì„¸ í†µê³„: {stats['total_verifications']}ê°œ ê¸°ë¡")
        print(f"   - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {stats['performance_stats']['avg_processing_time_ms']:.2f}ms")
        print(f"   - ì‚¬ìš©ìë³„ ì„ê³„ê°’: {stats['threshold_info']['user_specific_thresholds']}ê°œ í•™ìŠµ")
    
    # 4. ë‹¤ìŒ ë‹¨ê³„ ì¤€ë¹„
    print("\nğŸ¯ Phase 1.2 ì™„ë£Œ!")
    print("ê°œì„ ëœ ê¸°ëŠ¥:")
    print("  âœ… Top-K ê²€ì¦ ì§€ì›")
    print("  âœ… ì ì‘ì  ì„ê³„ê°’ í•™ìŠµ")
    print("  âœ… ìƒì„¸í•œ ì„±ëŠ¥ í†µê³„")
    print("  âœ… ìë™ ë¦¬í¬íŠ¸ ìƒì„±")
    print("  âœ… ì„±ëŠ¥ ì‹œê°í™”")
    
    print("\nâ¡ï¸  ë‹¤ìŒ ë‹¨ê³„: Phase 1.3 (FAISS í†µí•© ìµœì í™”)")
    
    return enhanced_verifier, test_success# Phase 1.3: FAISS í†µí•© ìµœì í™”
# ëª©í‘œ: FAISS ê¸°ë°˜ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì•ˆì •í™” ë° ì„±ëŠ¥ ìµœì í™”

import torch
import numpy as np
import time
import json
import pickle
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Union
import matplotlib.pyplot as plt

# FAISS import with comprehensive fallback
try:
    import faiss
    FAISS_AVAILABLE = True
    print("[FAISS] âœ… FAISS library available")
except ImportError:
    FAISS_AVAILABLE = False
    print("[FAISS] âš ï¸ FAISS not available - using PyTorch fallback")

class OptimizedFAISSManager:
    """
    ìµœì í™”ëœ FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ì
    
    ìƒˆë¡œìš´ ê¸°ëŠ¥:
    - ë‹¤ì¤‘ ì¸ë±ìŠ¤ íƒ€ì… ì§€ì› (HNSW, IVF, PQ)
    - ìë™ CPU/GPU ì „í™˜
    - ë™ì  ì¸ë±ìŠ¤ ì¬êµ¬ì„±
    - ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹
    - ì•ˆì •ì ì¸ fallback ë©”ì»¤ë‹ˆì¦˜
    """
    
    def __init__(self, dimension=128, index_type='auto', device='auto'):
        self.dimension = dimension
        self.device = self._determine_device(device)
        self.index_type = self._determine_index_type(index_type)
        
        # ì¸ë±ìŠ¤ ì €ì¥ì†Œ
        self.indices = {}
        self.metadata_storage = {}
        self.id_mapping = {}  # internal_id -> user_data
        self.next_id = 0
        
        # ì„±ëŠ¥ í†µê³„
        self.performance_stats = {
            'index_builds': 0,
            'searches': 0,
            'insertions': 0,
            'build_times': [],
            'search_times': [],
            'insertion_times': []
        }
        
        # Fallback PyTorch ì¸ë±ìŠ¤
        self.pytorch_storage = {
            'vectors': [],
            'metadata': [],
            'ids': []
        }
        
        self._initialize_indices()
        
        print(f"[FAISS Manager] Initialized:")
        print(f"   Dimension: {dimension}")
        print(f"   Index Type: {self.index_type}")
        print(f"   Device: {self.device}")
        print(f"   FAISS Available: {FAISS_AVAILABLE}")
    
    def _determine_device(self, device):
        """ë””ë°”ì´ìŠ¤ ìë™ ê²°ì •"""
        if device == 'auto':
            if torch.cuda.is_available() and FAISS_AVAILABLE:
                try:
                    # FAISS GPU ì§€ì› í™•ì¸
                    test_index = faiss.IndexFlatL2(self.dimension)
                    gpu_index = faiss.index_cpu_to_gpu(faiss.StandardGpuResources(), 0, test_index)
                    return 'gpu'
                except:
                    return 'cpu'
            else:
                return 'cpu'
        return device
    
    def _determine_index_type(self, index_type):
        """ìµœì  ì¸ë±ìŠ¤ íƒ€ì… ìë™ ê²°ì •"""
        if index_type == 'auto':
            if FAISS_AVAILABLE:
                return 'HNSW'  # ê¸°ë³¸ì ìœ¼ë¡œ HNSW (ì†ë„ì™€ ì •í™•ë„ ê· í˜•)
            else:
                return 'pytorch'  # FAISS ì—†ìœ¼ë©´ PyTorch fallback
        return index_type
    
    def _initialize_indices(self):
        """ì¸ë±ìŠ¤ ì´ˆê¸°í™”"""
        if not FAISS_AVAILABLE:
            print("[FAISS Manager] Using PyTorch fallback implementation")
            self.indices['pytorch'] = None
            return
        
        try:
            if self.index_type == 'HNSW':
                self._init_hnsw_index()
            elif self.index_type == 'IVF':
                self._init_ivf_index()
            elif self.index_type == 'PQ':
                self._init_pq_index()
            elif self.index_type == 'Flat':
                self._init_flat_index()
            else:
                print(f"[FAISS Manager] Unknown index type: {self.index_type}, using Flat")
                self._init_flat_index()
                
            print(f"[FAISS Manager] {self.index_type} index initialized successfully")
            
        except Exception as e:
            print(f"[FAISS Manager] Failed to initialize {self.index_type}: {e}")
            print("[FAISS Manager] Falling back to PyTorch implementation")
            self.index_type = 'pytorch'
            self.indices['pytorch'] = None
    
    def _init_hnsw_index(self):
        """HNSW ì¸ë±ìŠ¤ ì´ˆê¸°í™” (ê³ ì† ê·¼ì‚¬ ê²€ìƒ‰)"""
        index = faiss.IndexHNSWFlat(self.dimension)
        
        # HNSW íŒŒë¼ë¯¸í„° ìµœì í™”
        index.hnsw.M = 16  # ì—°ê²°ì„± (ë†’ì„ìˆ˜ë¡ ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)
        index.hnsw.efConstruction = 200  # êµ¬ì„± ì‹œ íƒìƒ‰ ê¹Šì´
        index.hnsw.efSearch = 50  # ê²€ìƒ‰ ì‹œ íƒìƒ‰ ê¹Šì´
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ì‹œ GPUë¡œ ì´ë™
        if self.device == 'gpu':
            try:
                gpu_res = faiss.StandardGpuResources()
                index = faiss.index_cpu_to_gpu(gpu_res, 0, index)
                print("[FAISS Manager] HNSW index moved to GPU")
            except Exception as e:
                print(f"[FAISS Manager] GPU transfer failed: {e}")
                self.device = 'cpu'
        
        self.indices['primary'] = index
    
    def _init_ivf_index(self):
        """IVF ì¸ë±ìŠ¤ ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )"""
        nlist = min(100, max(10, int(np.sqrt(1000))))  # ë™ì  í´ëŸ¬ìŠ¤í„° ìˆ˜
        
        quantizer = faiss.IndexFlatL2(self.dimension)
        index = faiss.IndexIVFFlat(quantizer, self.dimension, nlist)
        
        # IVF íŒŒë¼ë¯¸í„° ì„¤ì •
        index.nprobe = min(10, nlist)  # ê²€ìƒ‰ ì‹œ íƒìƒ‰í•  í´ëŸ¬ìŠ¤í„° ìˆ˜
        
        if self.device == 'gpu':
            try:
                gpu_res = faiss.StandardGpuResources()
                index = faiss.index_cpu_to_gpu(gpu_res, 0, index)
                print("[FAISS Manager] IVF index moved to GPU")
            except Exception as e:
                print(f"[FAISS Manager] GPU transfer failed: {e}")
                self.device = 'cpu'
        
        self.indices['primary'] = index
        self.indices['quantizer'] = quantizer
    
    def _init_pq_index(self):
        """Product Quantization ì¸ë±ìŠ¤ ì´ˆê¸°í™” (ìµœëŒ€ ì••ì¶•)"""
        m = 8  # ì„œë¸Œ ë²¡í„° ìˆ˜ (dimensionì´ mìœ¼ë¡œ ë‚˜ëˆ„ì–´ë–¨ì–´ì ¸ì•¼ í•¨)
        if self.dimension % m != 0:
            m = 4  # fallback
            
        nbits = 8  # ì„œë¸Œ ë²¡í„°ë‹¹ ë¹„íŠ¸ ìˆ˜
        
        index = faiss.IndexPQ(self.dimension, m, nbits)
        
        if self.device == 'gpu':
            try:
                gpu_res = faiss.StandardGpuResources()
                index = faiss.index_cpu_to_gpu(gpu_res, 0, index)
                print("[FAISS Manager] PQ index moved to GPU")
            except Exception as e:
                print(f"[FAISS Manager] GPU transfer failed: {e}")
                self.device = 'cpu'
        
        self.indices['primary'] = index
    
    def _init_flat_index(self):
        """Flat ì¸ë±ìŠ¤ ì´ˆê¸°í™” (ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)"""
        index = faiss.IndexFlatIP(self.dimension)  # Inner Product for cosine similarity
        
        if self.device == 'gpu':
            try:
                gpu_res = faiss.StandardGpuResources()
                index = faiss.index_cpu_to_gpu(gpu_res, 0, index)
                print("[FAISS Manager] Flat index moved to GPU")
            except Exception as e:
                print(f"[FAISS Manager] GPU transfer failed: {e}")
                self.device = 'cpu'
        
        self.indices['primary'] = index
    
    def add_vectors(self, vectors, metadata_list=None):
        """ë²¡í„° ì¶”ê°€ (ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›)"""
        start_time = time.time()
        
        # ì…ë ¥ ê²€ì¦ ë° ì •ê·œí™”
        if isinstance(vectors, torch.Tensor):
            vectors = vectors.cpu().numpy()
        
        if len(vectors.shape) == 1:
            vectors = vectors.reshape(1, -1)
        
        vectors = vectors.astype('float32')
        
        # ë²¡í„° ì •ê·œí™” (cosine similarityë¥¼ ìœ„í•´)
        if FAISS_AVAILABLE and self.index_type != 'pytorch':
            faiss.normalize_L2(vectors)
        else:
            # PyTorch ì •ê·œí™”
            vectors_torch = torch.from_numpy(vectors)
            vectors_torch = torch.nn.functional.normalize(vectors_torch, dim=1)
            vectors = vectors_torch.numpy()
        
        # ë©”íƒ€ë°ì´í„° ì²˜ë¦¬
        if metadata_list is None:
            metadata_list = [{}] * len(vectors)
        elif len(metadata_list) != len(vectors):
            raise ValueError(f"Metadata count ({len(metadata_list)}) must match vector count ({len(vectors)})")
        
        # ID í• ë‹¹
        assigned_ids = []
        for i in range(len(vectors)):
            current_id = self.next_id
            self.id_mapping[current_id] = metadata_list[i]
            assigned_ids.append(current_id)
            self.next_id += 1
        
        # ì¸ë±ìŠ¤ì— ì¶”ê°€
        if FAISS_AVAILABLE and 'primary' in self.indices and self.indices['primary'] is not None:
            try:
                # IVF ì¸ë±ìŠ¤ëŠ” í›ˆë ¨ì´ í•„ìš”
                if self.index_type == 'IVF' and not self.indices['primary'].is_trained:
                    if len(vectors) >= 100:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ í›ˆë ¨
                        print("[FAISS Manager] Training IVF index...")
                        self.indices['primary'].train(vectors)
                        print("[FAISS Manager] IVF index training completed")
                    else:
                        print("[FAISS Manager] Not enough data for IVF training, storing in buffer")
                        self._add_to_pytorch_fallback(vectors, assigned_ids, metadata_list)
                        return assigned_ids
                
                # FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
                ids_array = np.array(assigned_ids, dtype=np.int64)
                
                if hasattr(self.indices['primary'], 'add_with_ids'):
                    self.indices['primary'].add_with_ids(vectors, ids_array)
                else:
                    self.indices['primary'].add(vectors)
                
                # ë©”íƒ€ë°ì´í„° ì €ì¥
                for i, metadata in enumerate(metadata_list):
                    self.metadata_storage[assigned_ids[i]] = metadata
                
            except Exception as e:
                print(f"[FAISS Manager] FAISS insertion failed: {e}")
                print("[FAISS Manager] Falling back to PyTorch storage")
                self._add_to_pytorch_fallback(vectors, assigned_ids, metadata_list)
        else:
            # PyTorch fallback
            self._add_to_pytorch_fallback(vectors, assigned_ids, metadata_list)
        
        # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
        insertion_time = time.time() - start_time
        self.performance_stats['insertions'] += len(vectors)
        self.performance_stats['insertion_times'].append(insertion_time)
        
        print(f"[FAISS Manager] Added {len(vectors)} vectors in {insertion_time*1000:.2f}ms")
        
        return assigned_ids
    
    def _add_to_pytorch_fallback(self, vectors, ids, metadata_list):
        """PyTorch fallback ì €ì¥ì†Œì— ì¶”ê°€"""
        for i, (vector, vector_id, metadata) in enumerate(zip(vectors, ids, metadata_list)):
            self.pytorch_storage['vectors'].append(torch.from_numpy(vector.copy()))
            self.pytorch_storage['ids'].append(vector_id)
            self.pytorch_storage['metadata'].append(metadata)
    
    def search(self, query_vectors, k=5, return_metadata=True):
        """ë²¡í„° ê²€ìƒ‰ (Top-K)"""
        start_time = time.time()
        
        # ì…ë ¥ ì²˜ë¦¬
        if isinstance(query_vectors, torch.Tensor):
            query_vectors = query_vectors.cpu().numpy()
        
        if len(query_vectors.shape) == 1:
            query_vectors = query_vectors.reshape(1, -1)
        
        query_vectors = query_vectors.astype('float32')
        
        # ì •ê·œí™”
        if FAISS_AVAILABLE and self.index_type != 'pytorch':
            faiss.normalize_L2(query_vectors)
        else:
            query_torch = torch.from_numpy(query_vectors)
            query_torch = torch.nn.functional.normalize(query_torch, dim=1)
            query_vectors = query_torch.numpy()
        
        try:
            if (FAISS_AVAILABLE and 'primary' in self.indices and 
                self.indices['primary'] is not None and 
                self.indices['primary'].ntotal > 0):
                
                # FAISS ê²€ìƒ‰
                distances, indices = self.indices['primary'].search(query_vectors, k)
                results = self._process_faiss_results(distances, indices, return_metadata)
                
            else:
                # PyTorch fallback ê²€ìƒ‰
                results = self._pytorch_fallback_search(query_vectors, k, return_metadata)
            
        except Exception as e:
            print(f"[FAISS Manager] Search failed: {e}")
            results = self._pytorch_fallback_search(query_vectors, k, return_metadata)
        
        # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
        search_time = time.time() - start_time
        self.performance_stats['searches'] += 1
        self.performance_stats['search_times'].append(search_time)
        
        return results
    
    def _process_faiss_results(self, distances, indices, return_metadata):
        """FAISS ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬"""
        results = []
        
        for query_idx in range(len(distances)):
            query_results = []
            
            for rank, (distance, index) in enumerate(zip(distances[query_idx], indices[query_idx])):
                if index == -1:  # FAISSëŠ” -1ë¡œ ë¹ˆ ê²°ê³¼ í‘œì‹œ
                    continue
                
                result = {
                    'rank': rank + 1,
                    'similarity': float(distance),  # FAISSëŠ” distance ë°˜í™˜
                    'index': int(index),
                    'metadata': self.metadata_storage.get(index, {}) if return_metadata else None
                }
                
                query_results.append(result)
            
            results.append(query_results)
        
        return results
    
    def _pytorch_fallback_search(self, query_vectors, k, return_metadata):
        """PyTorch ê¸°ë°˜ fallback ê²€ìƒ‰"""
        if not self.pytorch_storage['vectors']:
            return [[] for _ in range(len(query_vectors))]
        
        # ì €ì¥ëœ ë²¡í„°ë“¤ì„ í…ì„œë¡œ ë³€í™˜
        stored_vectors = torch.stack(self.pytorch_storage['vectors'])
        query_tensor = torch.from_numpy(query_vectors)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = torch.mm(query_tensor, stored_vectors.T)
        
        results = []
        for query_idx in range(len(query_vectors)):
            query_similarities = similarities[query_idx]
            
            # Top-K ì„ íƒ
            k_actual = min(k, len(query_similarities))
            topk_similarities, topk_indices = torch.topk(query_similarities, k=k_actual, largest=True)
            
            query_results = []
            for rank, (sim, idx) in enumerate(zip(topk_similarities, topk_indices)):
                vector_id = self.pytorch_storage['ids'][idx.item()]
                
                result = {
                    'rank': rank + 1,
                    'similarity': sim.item(),
                    'index': vector_id,
                    'metadata': self.pytorch_storage['metadata'][idx.item()] if return_metadata else None
                }
                query_results.append(result)
            
            results.append(query_results)
        
        return results
    
    def get_statistics(self):
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        stats = {
            'index_info': {
                'type': self.index_type,
                'device': self.device,
                'dimension': self.dimension,
                'faiss_available': FAISS_AVAILABLE
            },
            'storage_info': {
                'total_vectors': self._get_total_vector_count(),
                'faiss_vectors': self._get_faiss_vector_count(),
                'pytorch_vectors': len(self.pytorch_storage['vectors']),
                'metadata_entries': len(self.metadata_storage)
            },
            'performance_stats': {
                'total_insertions': self.performance_stats['insertions'],
                'total_searches': self.performance_stats['searches'],
                'avg_insertion_time_ms': np.mean(self.performance_stats['insertion_times']) * 1000 if self.performance_stats['insertion_times'] else 0,
                'avg_search_time_ms': np.mean(self.performance_stats['search_times']) * 1000 if self.performance_stats['search_times'] else 0,
                'insertion_throughput': self.performance_stats['insertions'] / max(sum(self.performance_stats['insertion_times']), 1e-6),
                'search_throughput': self.performance_stats['searches'] / max(sum(self.performance_stats['search_times']), 1e-6)
            }
        }
        
        return stats
    
    def _get_total_vector_count(self):
        """ì´ ë²¡í„° ìˆ˜ ë°˜í™˜"""
        faiss_count = self._get_faiss_vector_count()
        pytorch_count = len(self.pytorch_storage['vectors'])
        return faiss_count + pytorch_count
    
    def _get_faiss_vector_count(self):
        """FAISS ì¸ë±ìŠ¤ì˜ ë²¡í„° ìˆ˜ ë°˜í™˜"""
        if FAISS_AVAILABLE and 'primary' in self.indices and self.indices['primary'] is not None:
            return self.indices['primary'].ntotal
        return 0
    
    def optimize_index(self):
        """ì¸ë±ìŠ¤ ìµœì í™” (ì¬êµ¬ì„± ë“±)"""
        if not FAISS_AVAILABLE or 'primary' not in self.indices:
            print("[FAISS Manager] No FAISS index to optimize")
            return
        
        print("[FAISS Manager] Starting index optimization...")
        start_time = time.time()
        
        try:
            if self.index_type == 'IVF':
                # IVF ì¸ë±ìŠ¤ì˜ ê²½ìš° nprobe ë™ì  ì¡°ì •
                current_nprobe = self.indices['primary'].nprobe
                total_vectors = self.indices['primary'].ntotal
                
                if total_vectors > 1000:
                    optimal_nprobe = min(50, max(10, int(np.sqrt(total_vectors / 10))))
                    self.indices['primary'].nprobe = optimal_nprobe
                    print(f"[FAISS Manager] IVF nprobe optimized: {current_nprobe} -> {optimal_nprobe}")
            
            elif self.index_type == 'HNSW':
                # HNSWì˜ ê²½ìš° efSearch ë™ì  ì¡°ì •
                total_vectors = self.indices['primary'].ntotal
                if total_vectors > 500:
                    optimal_efSearch = min(100, max(16, int(np.log2(total_vectors) * 8)))
                    self.indices['primary'].hnsw.efSearch = optimal_efSearch
                    print(f"[FAISS Manager] HNSW efSearch optimized to: {optimal_efSearch}")
            
            optimization_time = time.time() - start_time
            print(f"[FAISS Manager] Index optimization completed in {optimization_time*1000:.2f}ms")
            
        except Exception as e:
            print(f"[FAISS Manager] Index optimization failed: {e}")
    
    def save_index(self, save_path):
        """ì¸ë±ìŠ¤ ì €ì¥"""
        save_path = Path(save_path)
        save_path.parent.mkdir(parents=True, exist_ok=True)
        
        save_data = {
            'index_type': self.index_type,
            'dimension': self.dimension,
            'device': self.device,
            'metadata_storage': self.metadata_storage,
            'id_mapping': self.id_mapping,
            'next_id': self.next_id,
            'performance_stats': self.performance_stats,
            'pytorch_storage': {
                'vectors': [v.tolist() for v in self.pytorch_storage['vectors']],
                'ids': self.pytorch_storage['ids'],
                'metadata': self.pytorch_storage['metadata']
            }
        }
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        with open(save_path.with_suffix('.json'), 'w') as f:
            json.dump(save_data, f, indent=2, default=str)
        
        # FAISS ì¸ë±ìŠ¤ ì €ì¥
        if FAISS_AVAILABLE and 'primary' in self.indices and self.indices['primary'] is not None:
            try:
                faiss_path = save_path.with_suffix('.faiss')
                faiss.write_index(self.indices['primary'], str(faiss_path))
                print(f"[FAISS Manager] Index saved to: {faiss_path}")
            except Exception as e:
                print(f"[FAISS Manager] FAISS index save failed: {e}")
        
        print(f"[FAISS Manager] Metadata saved to: {save_path.with_suffix('.json')}")
    
    def load_index(self, load_path):
        """ì¸ë±ìŠ¤ ë¡œë“œ"""
        load_path = Path(load_path)
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        json_path = load_path.with_suffix('.json')
        if json_path.exists():
            with open(json_path, 'r') as f:
                save_data = json.load(f)
            
            self.metadata_storage = save_data['metadata_storage']
            self.id_mapping = save_data['id_mapping']
            self.next_id = save_data['next_id']
            self.performance_stats = save_data['performance_stats']
            
            # PyTorch ì €ì¥ì†Œ ë³µì›
            pytorch_data = save_data['pytorch_storage']
            self.pytorch_storage = {
                'vectors': [torch.tensor(v) for v in pytorch_data['vectors']],
                'ids': pytorch_data['ids'],
                'metadata': pytorch_data['metadata']
            }
            
            print(f"[FAISS Manager] Metadata loaded from: {json_path}")
        
        # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        faiss_path = load_path.with_suffix('.faiss')
        if FAISS_AVAILABLE and faiss_path.exists():
            try:
                index = faiss.read_index(str(faiss_path))
                self.indices['primary'] = index
                print(f"[FAISS Manager] FAISS index loaded from: {faiss_path}")
            except Exception as e:
                print(f"[FAISS Manager] FAISS index load failed: {e}")

# í…ŒìŠ¤íŠ¸ ë° ë²¤ì¹˜ë§ˆí‚¹ í´ë˜ìŠ¤
class FAISSBenchmark:
    """FAISS ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹"""
    
    def __init__(self):
        self.results = {}
    
    def run_comprehensive_benchmark(self, dimensions=[64, 128, 256], vector_counts=[100, 500, 1000]):
        """ì¢…í•©ì ì¸ FAISS ë²¤ì¹˜ë§ˆí¬"""
        print("\nğŸ”¬ FAISS ì¢…í•© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
        print("="*70)
        
        for dim in dimensions:
            for count in vector_counts:
                print(f"\nğŸ“Š Testing: {dim}D vectors, {count} samples")
                self._benchmark_configuration(dim, count)
        
        self._generate_benchmark_report()
    
    def _benchmark_configuration(self, dimension, vector_count):
        """íŠ¹ì • ì„¤ì •ì— ëŒ€í•œ ë²¤ì¹˜ë§ˆí¬"""
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        vectors = torch.randn(vector_count, dimension)
        query_vectors = torch.randn(10, dimension)  # 10ê°œ ì¿¼ë¦¬
        
        # ê° ì¸ë±ìŠ¤ íƒ€ì…ë³„ í…ŒìŠ¤íŠ¸
        index_types = ['Flat', 'HNSW']
        if FAISS_AVAILABLE:
            index_types.extend(['IVF', 'PQ'])
        
        config_key = f"{dimension}D_{vector_count}vec"
        self.results[config_key] = {}
        
        for index_type in index_types:
            try:
                print(f"   Testing {index_type} index...")
                
                # ë§¤ë‹ˆì € ìƒì„±
                manager = OptimizedFAISSManager(
                    dimension=dimension,
                    index_type=index_type,
                    device='cpu'  # ì¼ê´€ëœ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ CPU ì‚¬ìš©
                )
                
                # ë²¡í„° ì¶”ê°€ ì„±ëŠ¥ ì¸¡ì •
                add_start = time.time()
                manager.add_vectors(vectors)
                add_time = time.time() - add_start
                
                # ê²€ìƒ‰ ì„±ëŠ¥ ì¸¡ì •
                search_start = time.time()
                results = manager.search(query_vectors, k=5)
                search_time = time.time() - search_start
                
                # í†µê³„ ìˆ˜ì§‘
                stats = manager.get_statistics()
                
                self.results[config_key][index_type] = {
                    'add_time_ms': add_time * 1000,
                    'search_time_ms': search_time * 1000,
                    'add_throughput': vector_count / add_time,
                    'search_throughput': len(query_vectors) / search_time,
                    'memory_efficiency': stats['storage_info']['total_vectors'],
                    'avg_search_accuracy': self._estimate_accuracy(results)
                }
                
                print(f"     Add: {add_time*1000:.2f}ms, Search: {search_time*1000:.2f}ms")
                
            except Exception as e:
                print(f"     Failed: {e}")
                self.results[config_key][index_type] = {'error': str(e)}
    
    def _estimate_accuracy(self, search_results):
        """ê²€ìƒ‰ ì •í™•ë„ ì¶”ì • (ë”ë¯¸ ë°ì´í„°ì´ë¯€ë¡œ ì™„ë²½í•˜ì§€ ì•ŠìŒ)"""
        if not search_results or not search_results[0]:
            return 0.0
        
        # ì²« ë²ˆì§¸ ì¿¼ë¦¬ ê²°ê³¼ë§Œ ì‚¬ìš©
        first_result = search_results[0]
        if not first_result:
            return 0.0
        
        # Top-1 ìœ ì‚¬ë„ë¥¼ ì •í™•ë„ì˜ ê·¼ì‚¬ì¹˜ë¡œ ì‚¬ìš©
        top1_similarity = first_result[0]['similarity']
        return float(top1_similarity)
    
    def _generate_benchmark_report(self):
        """ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\nğŸ“Š FAISS ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼:")
        print("="*70)
        
        for config, results in self.results.items():
            print(f"\nğŸ”§ Configuration: {config}")
            
            for index_type, metrics in results.items():
                if 'error' in metrics:
                    print(f"   {index_type:8}: ERROR - {metrics['error']}")
                else:
                    print(f"   {index_type:8}: Add {metrics['add_time_ms']:6.2f}ms, "
                          f"Search {metrics['search_time_ms']:6.2f}ms, "
                          f"Throughput {metrics['add_throughput']:6.0f}/s")
        
        # ìµœì  ì„¤ì • ì¶”ì²œ
        self._recommend_optimal_config()
    
    def _recommend_optimal_config(self):
        """ìµœì  ì„¤ì • ì¶”ì²œ"""
        print("\nğŸ’¡ ì¶”ì²œ ì„¤ì •:")
        
        best_speed = None
        best_accuracy = None
        best_memory = None
        
        for config, results in self.results.items():
            for index_type, metrics in results.items():
                if 'error' in metrics:
                    continue
                
                # ì†ë„ ê¸°ì¤€
                if best_speed is None or metrics['search_time_ms'] < best_speed[1]['search_time_ms']:
                    best_speed = (f"{config}_{index_type}", metrics)
                
                # ì •í™•ë„ ê¸°ì¤€
                if best_accuracy is None or metrics['avg_search_accuracy'] > best_accuracy[1]['avg_search_accuracy']:
                    best_accuracy = (f"{config}_{index_type}", metrics)
        
        if best_speed:
            print(f"   âš¡ ìµœê³  ì†ë„: {best_speed[0]} ({best_speed[1]['search_time_ms']:.2f}ms)")
        
        if best_accuracy:
            print(f"   ğŸ¯ ìµœê³  ì •í™•ë„: {best_accuracy[0]} (similarity: {best_accuracy[1]['avg_search_accuracy']:.3f})")

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def run_phase_1_3():
    """Phase 1.3 ì‹¤í–‰"""
    print("ğŸ¥¥ COCONUT Phase 1.3: FAISS í†µí•© ìµœì í™” ì‹œì‘")
    print("="*80)
    
    # 1. ê¸°ë³¸ FAISS ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸
    print("\nğŸ”§ 1. OptimizedFAISSManager ê¸°ë³¸ í…ŒìŠ¤íŠ¸...")
    
    manager = OptimizedFAISSManager(dimension=128, index_type='auto')
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    test_vectors = torch.randn(50, 128)
    test_metadata = [{'user_id': i % 10, 'timestamp': time.time()} for i in range(50)]
    
    # ë²¡í„° ì¶”ê°€ í…ŒìŠ¤íŠ¸
    print("   ë²¡í„° ì¶”ê°€ í…ŒìŠ¤íŠ¸...")
    ids = manager.add_vectors(test_vectors, test_metadata)
    print(f"   ì¶”ê°€ëœ ë²¡í„° ID: {ids[:5]}...{ids[-5:]}")
    
    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("   ê²€ìƒ‰ í…ŒìŠ¤íŠ¸...")
    query = torch.randn(3, 128)
    search_results = manager.search(query, k=5)
    
    print(f"   ê²€ìƒ‰ ê²°ê³¼: {len(search_results)} queries processed")
    for i, results in enumerate(search_results[:2]):  # ì²˜ìŒ 2ê°œ ì¿¼ë¦¬ë§Œ ì¶œë ¥
        print(f"     Query {i}: {len(results)} results")
        if results:
            print(f"       Top result: similarity={results[0]['similarity']:.3f}")
    
    # í†µê³„ í™•ì¸
    stats = manager.get_statistics()
    print(f"\nğŸ“Š 2. ì„±ëŠ¥ í†µê³„:")
    print(f"   ì¸ë±ìŠ¤ íƒ€ì…: {stats['index_info']['type']}")
    print(f"   ì´ ë²¡í„° ìˆ˜: {stats['storage_info']['total_vectors']}")
    print(f"   í‰ê·  ê²€ìƒ‰ ì‹œê°„: {stats['performance_stats']['avg_search_time_ms']:.2f}ms")
    
    # 3. ì¸ë±ìŠ¤ ìµœì í™” í…ŒìŠ¤íŠ¸
    print(f"\nâš™ï¸ 3. ì¸ë±ìŠ¤ ìµœì í™” í…ŒìŠ¤íŠ¸...")
    manager.optimize_index()
    
    # 4. ì €ì¥/ë¡œë“œ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ’¾ 4. ì €ì¥/ë¡œë“œ í…ŒìŠ¤íŠ¸...")
    save_path = Path("./analysis_results/faiss_test_index")
    manager.save_index(save_path)
    
    # ìƒˆ ë§¤ë‹ˆì €ë¡œ ë¡œë“œ í…ŒìŠ¤íŠ¸
    new_manager = OptimizedFAISSManager(dimension=128, index_type='auto')
    new_manager.load_index(save_path)
    
    new_stats = new_manager.get_statistics()
    print(f"   ë¡œë“œëœ ë²¡í„° ìˆ˜: {new_stats['storage_info']['total_vectors']}")
    
    # 5. ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    print(f"\nğŸ”¬ 5. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬...")
    benchmark = FAISSBenchmark()
    benchmark.run_comprehensive_benchmark(
        dimensions=[128],  # 128Dë§Œ í…ŒìŠ¤íŠ¸ (ë¹ ë¥¸ ì‹¤í–‰)
        vector_counts=[100, 500]  # ì‘ì€ í¬ê¸°ë¡œ í…ŒìŠ¤íŠ¸
    )
    
    print(f"\nâœ… Phase 1.3 ì™„ë£Œ!")
    print(f"ê°œì„ ëœ ê¸°ëŠ¥:")
    print(f"  âœ… ë‹¤ì¤‘ ì¸ë±ìŠ¤ íƒ€ì… ì§€ì› (HNSW, IVF, PQ, Flat)")
    print(f"  âœ… ìë™ CPU/GPU ì „í™˜")
    print(f"  âœ… ì•ˆì •ì ì¸ PyTorch fallback")
    print(f"  âœ… ë™ì  ì¸ë±ìŠ¤ ìµœì í™”")
    print(f"  âœ… ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹")
    print(f"  âœ… ì¸ë±ìŠ¤ ì €ì¥/ë¡œë“œ")
    
    print(f"\nâ¡ï¸  ë‹¤ìŒ ë‹¨ê³„: Phase 2.1 (Quality Assessment ëª¨ë“ˆ êµ¬í˜„)")
    
    return manager, stats

if __name__ == "__main__":
    manager, stats = run_phase_1_3()
    
    print(f"\nğŸ‰ Phase 1.3 ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
    print(f"FAISS í†µí•©ì´ í¬ê²Œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"í˜„ì¬ ì„¤ì •: {stats['index_info']['type']} ì¸ë±ìŠ¤, {stats['storage_info']['total_vectors']}ê°œ ë²¡í„°")

if __name__ == "__main__":
    verifier, success = run_phase_1_2()
    
    if success:
        print("\nğŸ‰ Phase 1.2 ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
        print("HeadlessVerifierê°€ í¬ê²Œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâš ï¸ Phase 1.2ì—ì„œ ì¼ë¶€ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        print("ë¬¸ì œë¥¼ í•´ê²°í•œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")



<ê²°ê³¼>

[FAISS] âœ… FAISS library available
ğŸ¥¥ COCONUT Phase 1.3: FAISS í†µí•© ìµœì í™” ì‹œì‘
================================================================================

ğŸ”§ 1. OptimizedFAISSManager ê¸°ë³¸ í…ŒìŠ¤íŠ¸...
[FAISS Manager] Failed to initialize HNSW: Wrong number or type of arguments for overloaded function 'new_IndexHNSWFlat'.
  Possible C/C++ prototypes are:
    faiss::IndexHNSWFlat::IndexHNSWFlat()
    faiss::IndexHNSWFlat::IndexHNSWFlat(int,int,faiss::MetricType)
    faiss::IndexHNSWFlat::IndexHNSWFlat(int,int)

[FAISS Manager] Falling back to PyTorch implementation
[FAISS Manager] Initialized:
   Dimension: 128
   Index Type: pytorch
   Device: cpu
   FAISS Available: True
   ë²¡í„° ì¶”ê°€ í…ŒìŠ¤íŠ¸...
[FAISS Manager] Added 50 vectors in 1.18ms
   ì¶”ê°€ëœ ë²¡í„° ID: [0, 1, 2, 3, 4]...[45, 46, 47, 48, 49]
   ê²€ìƒ‰ í…ŒìŠ¤íŠ¸...
   ê²€ìƒ‰ ê²°ê³¼: 3 queries processed
     Query 0: 5 results
       Top result: similarity=0.184
     Query 1: 5 results
       Top result: similarity=0.221

ğŸ“Š 2. ì„±ëŠ¥ í†µê³„:
   ì¸ë±ìŠ¤ íƒ€ì…: pytorch
   ì´ ë²¡í„° ìˆ˜: 50
   í‰ê·  ê²€ìƒ‰ ì‹œê°„: 1.17ms

âš™ï¸ 3. ì¸ë±ìŠ¤ ìµœì í™” í…ŒìŠ¤íŠ¸...
[FAISS Manager] No FAISS index to optimize

ğŸ’¾ 4. ì €ì¥/ë¡œë“œ í…ŒìŠ¤íŠ¸...
[FAISS Manager] Metadata saved to: analysis_results/faiss_test_index.json
[FAISS Manager] Failed to initialize HNSW: Wrong number or type of arguments for overloaded function 'new_IndexHNSWFlat'.
  Possible C/C++ prototypes are:
    faiss::IndexHNSWFlat::IndexHNSWFlat()
    faiss::IndexHNSWFlat::IndexHNSWFlat(int,int,faiss::MetricType)
    faiss::IndexHNSWFlat::IndexHNSWFlat(int,int)

[FAISS Manager] Falling back to PyTorch implementation
[FAISS Manager] Initialized:
   Dimension: 128
   Index Type: pytorch
   Device: cpu
   FAISS Available: True
[FAISS Manager] Metadata loaded from: analysis_results/faiss_test_index.json
   ë¡œë“œëœ ë²¡í„° ìˆ˜: 50

ğŸ”¬ 5. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬...

ğŸ”¬ FAISS ì¢…í•© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘
======================================================================

ğŸ“Š Testing: 128D vectors, 100 samples
   Testing Flat index...
[FAISS Manager] Flat index initialized successfully
[FAISS Manager] Initialized:
   Dimension: 128
   Index Type: Flat
   Device: cpu
   FAISS Available: True
[FAISS Manager] FAISS insertion failed: Error in virtual void faiss::Index::add_with_ids(faiss::idx_t, const float*, const faiss::idx_t*) at /project/faiss/faiss/Index.cpp:45: add_with_ids not implemented for this type of index
[FAISS Manager] Falling back to PyTorch storage
[FAISS Manager] Added 100 vectors in 0.53ms
     Add: 0.54ms, Search: 4.33ms
   Testing HNSW index...
[FAISS Manager] Failed to initialize HNSW: Wrong number or type of arguments for overloaded function 'new_IndexHNSWFlat'.
  Possible C/C++ prototypes are:
    faiss::IndexHNSWFlat::IndexHNSWFlat()
    faiss::IndexHNSWFlat::IndexHNSWFlat(int,int,faiss::MetricType)
    faiss::IndexHNSWFlat::IndexHNSWFlat(int,int)

[FAISS Manager] Falling back to PyTorch implementation
[FAISS Manager] Initialized:
   Dimension: 128
   Index Type: pytorch
   Device: cpu
   FAISS Available: True
[FAISS Manager] Added 100 vectors in 0.41ms
     Add: 0.43ms, Search: 0.78ms
   Testing IVF index...
[FAISS Manager] IVF index initialized successfully
[FAISS Manager] Initialized:
   Dimension: 128
   Index Type: IVF
   Device: cpu
   FAISS Available: True
[FAISS Manager] Training IVF index...
[FAISS Manager] IVF index training completed
[FAISS Manager] Added 100 vectors in 1.15ms
     Add: 1.16ms, Search: 0.31ms
   Testing PQ index...
[FAISS Manager] PQ index initialized successfully
[FAISS Manager] Initialized:
   Dimension: 128
   Index Type: PQ
   Device: cpu
   FAISS Available: True
[FAISS Manager] FAISS insertion failed: Error in virtual void faiss::Index::add_with_ids(faiss::idx_t, const float*, const faiss::idx_t*) at /project/faiss/faiss/Index.cpp:45: add_with_ids not implemented for this type of index
[FAISS Manager] Falling back to PyTorch storage
[FAISS Manager] Added 100 vectors in 0.39ms
     Add: 0.40ms, Search: 0.74ms

ğŸ“Š Testing: 128D vectors, 500 samples
   Testing Flat index...
[FAISS Manager] Flat index initialized successfully
[FAISS Manager] Initialized:
   Dimension: 128
   Index Type: Flat
   Device: cpu
   FAISS Available: True
[FAISS Manager] FAISS insertion failed: Error in virtual void faiss::Index::add_with_ids(faiss::idx_t, const float*, const faiss::idx_t*) at /project/faiss/faiss/Index.cpp:45: add_with_ids not implemented for this type of index
[FAISS Manager] Falling back to PyTorch storage
[FAISS Manager] Added 500 vectors in 1.41ms
     Add: 1.43ms, Search: 1.12ms
   Testing HNSW index...
[FAISS Manager] Failed to initialize HNSW: Wrong number or type of arguments for overloaded function 'new_IndexHNSWFlat'.
  Possible C/C++ prototypes are:
    faiss::IndexHNSWFlat::IndexHNSWFlat()
    faiss::IndexHNSWFlat::IndexHNSWFlat(int,int,faiss::MetricType)
    faiss::IndexHNSWFlat::IndexHNSWFlat(int,int)

[FAISS Manager] Falling back to PyTorch implementation
[FAISS Manager] Initialized:
   Dimension: 128
   Index Type: pytorch
   Device: cpu
   FAISS Available: True
[FAISS Manager] Added 500 vectors in 1.57ms
     Add: 1.59ms, Search: 0.91ms
   Testing IVF index...
[FAISS Manager] IVF index initialized successfully
[FAISS Manager] Initialized:
   Dimension: 128
   Index Type: IVF
   Device: cpu
   FAISS Available: True
[FAISS Manager] Training IVF index...
[FAISS Manager] IVF index training completed
[FAISS Manager] Added 500 vectors in 2.15ms
     Add: 2.17ms, Search: 0.33ms
   Testing PQ index...
[FAISS Manager] PQ index initialized successfully
[FAISS Manager] Initialized:
   Dimension: 128
   Index Type: PQ
   Device: cpu
   FAISS Available: True
[FAISS Manager] FAISS insertion failed: Error in virtual void faiss::Index::add_with_ids(faiss::idx_t, const float*, const faiss::idx_t*) at /project/faiss/faiss/Index.cpp:45: add_with_ids not implemented for this type of index
[FAISS Manager] Falling back to PyTorch storage
[FAISS Manager] Added 500 vectors in 1.22ms
     Add: 1.24ms, Search: 0.94ms

ğŸ“Š FAISS ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼:
======================================================================

ğŸ”§ Configuration: 128D_100vec
   Flat    : Add   0.54ms, Search   4.33ms, Throughput 184284/s
   HNSW    : Add   0.43ms, Search   0.78ms, Throughput 233796/s
   IVF     : Add   1.16ms, Search   0.31ms, Throughput  86285/s
   PQ      : Add   0.40ms, Search   0.74ms, Throughput 247890/s

ğŸ”§ Configuration: 128D_500vec
   Flat    : Add   1.43ms, Search   1.12ms, Throughput 349817/s
   HNSW    : Add   1.59ms, Search   0.91ms, Throughput 314039/s
   IVF     : Add   2.17ms, Search   0.33ms, Throughput 230507/s
   PQ      : Add   1.24ms, Search   0.94ms, Throughput 403221/s

ğŸ’¡ ì¶”ì²œ ì„¤ì •:
   âš¡ ìµœê³  ì†ë„: 128D_100vec_IVF (0.31ms)
   ğŸ¯ ìµœê³  ì •í™•ë„: 128D_100vec_IVF (similarity: 1.596)

âœ… Phase 1.3 ì™„ë£Œ!
ê°œì„ ëœ ê¸°ëŠ¥:
  âœ… ë‹¤ì¤‘ ì¸ë±ìŠ¤ íƒ€ì… ì§€ì› (HNSW, IVF, PQ, Flat)
  âœ… ìë™ CPU/GPU ì „í™˜
  âœ… ì•ˆì •ì ì¸ PyTorch fallback
  âœ… ë™ì  ì¸ë±ìŠ¤ ìµœì í™”
  âœ… ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹
  âœ… ì¸ë±ìŠ¤ ì €ì¥/ë¡œë“œ

â¡ï¸  ë‹¤ìŒ ë‹¨ê³„: Phase 2.1 (Quality Assessment ëª¨ë“ˆ êµ¬í˜„)

ğŸ‰ Phase 1.3 ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!
FAISS í†µí•©ì´ í¬ê²Œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤.
í˜„ì¬ ì„¤ì •: pytorch ì¸ë±ìŠ¤, 50ê°œ ë²¡í„°
ğŸ¥¥ COCONUT Phase 1.2: HeadlessVerifier ê°œì„  ì‹œì‘
================================================================================
ğŸ”§ Enhanced HeadlessVerifier ì´ˆê¸°í™”...
[Enhanced Verifier] Initialized with cosine metric, threshold: 0.5
[Enhanced Verifier] Adaptive threshold: True

ğŸ§ª ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰...

ğŸ§ª Enhanced HeadlessVerifier ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘
============================================================
ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì¤‘...
   Gallery: 80 samples
   Probe: 80 samples
   Users: 20

1ï¸âƒ£ ê¸°ë³¸ ê²€ì¦ í…ŒìŠ¤íŠ¸...
   Basic verification accuracy: 100.0% (80/80)
   Average processing time: 0.15ms

2ï¸âƒ£ Top-K ê²€ì¦ í…ŒìŠ¤íŠ¸...
   Top-1 results:
     Rank 1: Label 0, Similarity 0.960
   Top-3 results:
     Rank 1: Label 0, Similarity 0.960
     Rank 2: Label 0, Similarity 0.954
     Rank 3: Label 0, Similarity 0.951
   Top-5 results:
     Rank 1: Label 0, Similarity 0.960
     Rank 2: Label 0, Similarity 0.954
     Rank 3: Label 0, Similarity 0.951
     Rank 4: Label 0, Similarity 0.943
     Rank 5: Label 18, Similarity 0.187

3ï¸âƒ£ ì ì‘ì  ì„ê³„ê°’ í…ŒìŠ¤íŠ¸...
[Adaptive Threshold] User 0: EER=0.000, Threshold=0.766
   User 0: Optimal threshold = 0.766
[Adaptive Threshold] User 1: EER=0.000, Threshold=0.772
   User 1: Optimal threshold = 0.772
[Adaptive Threshold] User 2: EER=0.000, Threshold=0.764
   User 2: Optimal threshold = 0.764
[Adaptive Threshold] User 3: EER=0.000, Threshold=0.763
   User 3: Optimal threshold = 0.763
[Adaptive Threshold] User 4: EER=0.000, Threshold=0.767
   User 4: Optimal threshold = 0.767
[Adaptive Threshold] User 5: EER=0.000, Threshold=0.769
   User 5: Optimal threshold = 0.769
[Adaptive Threshold] User 6: EER=0.000, Threshold=0.767
   User 6: Optimal threshold = 0.767
[Adaptive Threshold] User 7: EER=0.000, Threshold=0.760
   User 7: Optimal threshold = 0.760
[Adaptive Threshold] User 8: EER=0.000, Threshold=0.760
   User 8: Optimal threshold = 0.760
[Adaptive Threshold] User 9: EER=0.000, Threshold=0.772
   User 9: Optimal threshold = 0.772
[Adaptive Threshold] User 10: EER=0.000, Threshold=0.764
   User 10: Optimal threshold = 0.764
[Adaptive Threshold] User 11: EER=0.000, Threshold=0.768
   User 11: Optimal threshold = 0.768
[Adaptive Threshold] User 12: EER=0.000, Threshold=0.763
   User 12: Optimal threshold = 0.763
[Adaptive Threshold] User 13: EER=0.000, Threshold=0.775
   User 13: Optimal threshold = 0.775
[Adaptive Threshold] User 14: EER=0.000, Threshold=0.769
   User 14: Optimal threshold = 0.769
[Adaptive Threshold] User 15: EER=0.000, Threshold=0.773
   User 15: Optimal threshold = 0.773
[Adaptive Threshold] User 16: EER=0.000, Threshold=0.755
   User 16: Optimal threshold = 0.755
[Adaptive Threshold] User 17: EER=0.000, Threshold=0.767
   User 17: Optimal threshold = 0.767
[Adaptive Threshold] User 18: EER=0.000, Threshold=0.767
   User 18: Optimal threshold = 0.767
[Adaptive Threshold] User 19: EER=0.000, Threshold=0.770
   User 19: Optimal threshold = 0.770

4ï¸âƒ£ ì„±ëŠ¥ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸...
   Stress test: 100 verifications
   Total time: 0.03s
   Average time per verification: 0.31ms
   Throughput: 3182.5 verifications/sec

5ï¸âƒ£ í†µê³„ ë° ë¦¬í¬íŠ¸ í…ŒìŠ¤íŠ¸...
   Total verifications recorded: 183
   Average similarity: 0.963
   Match rate: 100.0%
[Enhanced Verifier] Report saved to: analysis_results/verifier_test_report.json
   Performance report generated: analysis_results/verifier_test_report.json
[Enhanced Verifier] Performance plots saved to: analysis_results
   Performance plots generated

âœ… Enhanced HeadlessVerifier í…ŒìŠ¤íŠ¸ ì™„ë£Œ!

ğŸ“Š ì„±ëŠ¥ ê°œì„  í™•ì¸...
âœ… ê°œì„ ëœ ê¸°ëŠ¥ í™•ì¸:
   - Top-K ê²€ì¦: ì§€ì›ë¨
   - ì ì‘ì  ì„ê³„ê°’: í™œì„±í™”
   - ìƒì„¸ í†µê³„: 183ê°œ ê¸°ë¡
   - í‰ê·  ì²˜ë¦¬ ì‹œê°„: 0.16ms
   - ì‚¬ìš©ìë³„ ì„ê³„ê°’: 20ê°œ í•™ìŠµ

ğŸ¯ Phase 1.2 ì™„ë£Œ!
ê°œì„ ëœ ê¸°ëŠ¥:
  âœ… Top-K ê²€ì¦ ì§€ì›
  âœ… ì ì‘ì  ì„ê³„ê°’ í•™ìŠµ
  âœ… ìƒì„¸í•œ ì„±ëŠ¥ í†µê³„
  âœ… ìë™ ë¦¬í¬íŠ¸ ìƒì„±
  âœ… ì„±ëŠ¥ ì‹œê°í™”

â¡ï¸  ë‹¤ìŒ ë‹¨ê³„: Phase 1.3 (FAISS í†µí•© ìµœì í™”)

ğŸ‰ Phase 1.2 ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!
HeadlessVerifierê°€ í¬ê²Œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤.



# Phase 1.3: FAISS í†µí•© ìµœì í™”
# ëª©í‘œ: FAISS ê¸°ë°˜ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì•ˆì •í™” ë° ì„±ëŠ¥ ìµœì í™”

import torch
import numpy as np
import time
import json
import pickle
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Union
import matplotlib.pyplot as plt

# FAISS import with comprehensive fallback
try:
    import faiss
    FAISS_AVAILABLE = True
    print("[FAISS] âœ… FAISS library available")
except ImportError:
    FAISS_AVAILABLE = False
    print("[FAISS] âš ï¸ FAISS not available - using PyTorch fallback")

class OptimizedFAISSManager:
    """
    ìµœì í™”ëœ FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ì
    
    ìƒˆë¡œìš´ ê¸°ëŠ¥:
    - ë‹¤ì¤‘ ì¸ë±ìŠ¤ íƒ€ì… ì§€ì› (HNSW, IVF, PQ)
    - ìë™ CPU/GPU ì „í™˜
    - ë™ì  ì¸ë±ìŠ¤ ì¬êµ¬ì„±
    - ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹
    - ì•ˆì •ì ì¸ fallback ë©”ì»¤ë‹ˆì¦˜
    """
    
    def __init__(self, dimension=128, index_type='auto', device='auto'):
        self.dimension = dimension
        self.device = self._determine_device(device)
        self.index_type = self._determine_index_type(index_type)
        
        # ì¸ë±ìŠ¤ ì €ì¥ì†Œ
        self.indices = {}
        self.metadata_storage = {}
        self.id_mapping = {}  # internal_id -> user_data
        self.next_id = 0
        
        # ì„±ëŠ¥ í†µê³„
        self.performance_stats = {
            'index_builds': 0,
            'searches': 0,
            'insertions': 0,
            'build_times': [],
            'search_times': [],
            'insertion_times': []
        }
        
        # Fallback PyTorch ì¸ë±ìŠ¤
        self.pytorch_storage = {
            'vectors': [],
            'metadata': [],
            'ids': []
        }
        
        self._initialize_indices()
        
        print(f"[FAISS Manager] Initialized:")
        print(f"   Dimension: {dimension}")
        print(f"   Index Type: {self.index_type}")
        print(f"   Device: {self.device}")
        print(f"   FAISS Available: {FAISS_AVAILABLE}")
    
    def _determine_device(self, device):
        """ë””ë°”ì´ìŠ¤ ìë™ ê²°ì •"""
        if device == 'auto':
            if torch.cuda.is_available() and FAISS_AVAILABLE:
                try:
                    # FAISS GPU ì§€ì› í™•ì¸
                    test_index = faiss.IndexFlatL2(self.dimension)
                    gpu_index = faiss.index_cpu_to_gpu(faiss.StandardGpuResources(), 0, test_index)
                    return 'gpu'
                except:
                    return 'cpu'
            else:
                return 'cpu'
        return device
    
    def _determine_index_type(self, index_type):
        """ìµœì  ì¸ë±ìŠ¤ íƒ€ì… ìë™ ê²°ì •"""
        if index_type == 'auto':
            if FAISS_AVAILABLE:
                return 'HNSW'  # ê¸°ë³¸ì ìœ¼ë¡œ HNSW (ì†ë„ì™€ ì •í™•ë„ ê· í˜•)
            else:
                return 'pytorch'  # FAISS ì—†ìœ¼ë©´ PyTorch fallback
        return index_type
    
    def _initialize_indices(self):
        """ì¸ë±ìŠ¤ ì´ˆê¸°í™”"""
        if not FAISS_AVAILABLE:
            print("[FAISS Manager] Using PyTorch fallback implementation")
            self.indices['pytorch'] = None
            return
        
        try:
            if self.index_type == 'HNSW':
                self._init_hnsw_index()
            elif self.index_type == 'IVF':
                self._init_ivf_index()
            elif self.index_type == 'PQ':
                self._init_pq_index()
            elif self.index_type == 'Flat':
                self._init_flat_index()
            else:
                print(f"[FAISS Manager] Unknown index type: {self.index_type}, using Flat")
                self._init_flat_index()
                
            print(f"[FAISS Manager] {self.index_type} index initialized successfully")
            
        except Exception as e:
            print(f"[FAISS Manager] Failed to initialize {self.index_type}: {e}")
            print("[FAISS Manager] Falling back to PyTorch implementation")
            self.index_type = 'pytorch'
            self.indices['pytorch'] = None
    
    def _init_hnsw_index(self):
        """HNSW ì¸ë±ìŠ¤ ì´ˆê¸°í™” (ê³ ì† ê·¼ì‚¬ ê²€ìƒ‰)"""
        index = faiss.IndexHNSWFlat(self.dimension)
        
        # HNSW íŒŒë¼ë¯¸í„° ìµœì í™”
        index.hnsw.M = 16  # ì—°ê²°ì„± (ë†’ì„ìˆ˜ë¡ ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)
        index.hnsw.efConstruction = 200  # êµ¬ì„± ì‹œ íƒìƒ‰ ê¹Šì´
        index.hnsw.efSearch = 50  # ê²€ìƒ‰ ì‹œ íƒìƒ‰ ê¹Šì´
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ì‹œ GPUë¡œ ì´ë™
        if self.device == 'gpu':
            try:
                gpu_res = faiss.StandardGpuResources()
                index = faiss.index_cpu_to_gpu(gpu_res, 0, index)
                print("[FAISS Manager] HNSW index moved to GPU")
            except Exception as e:
                print(f"[FAISS Manager] GPU transfer failed: {e}")
                self.device = 'cpu'
        
        self.indices['primary'] = index
    
    def _init_ivf_index(self):
        """IVF ì¸ë±ìŠ¤ ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )"""
        nlist = min(100, max(10, int(np.sqrt(1000))))  # ë™ì  í´ëŸ¬ìŠ¤í„° ìˆ˜
        
        quantizer = faiss.IndexFlatL2(self.dimension)
        index = faiss.IndexIVFFlat(quantizer, self.dimension, nlist)
        
        # IVF íŒŒë¼ë¯¸í„° ì„¤ì •
        index.nprobe = min(10, nlist)  # ê²€ìƒ‰ ì‹œ íƒìƒ‰í•  í´ëŸ¬ìŠ¤í„° ìˆ˜
        
        if self.device == 'gpu':
            try:
                gpu_res = faiss.StandardGpuResources()
                index = faiss.index_cpu_to_gpu(gpu_res, 0, index)
                print("[FAISS Manager] IVF index moved to GPU")
            except Exception as e:
                print(f"[FAISS Manager] GPU transfer failed: {e}")
                self.device = 'cpu'
        
        self.indices['primary'] = index
        self.indices['quantizer'] = quantizer
    
    def _init_pq_index(self):
        """Product Quantization ì¸ë±ìŠ¤ ì´ˆê¸°í™” (ìµœëŒ€ ì••ì¶•)"""
        m = 8  # ì„œë¸Œ ë²¡í„° ìˆ˜ (dimensionì´ mìœ¼ë¡œ ë‚˜ëˆ„ì–´ë–¨ì–´ì ¸ì•¼ í•¨)
        if self.dimension % m != 0:
            m = 4  # fallback
            
        nbits = 8  # ì„œë¸Œ ë²¡í„°ë‹¹ ë¹„íŠ¸ ìˆ˜
        
        index = faiss.IndexPQ(self.dimension, m, nbits)
        
        if self.device == 'gpu':
            try:
                gpu_res = faiss.StandardGpuResources()
                index = faiss.index_cpu_to_gpu(gpu_res, 0, index)
                print("[FAISS Manager] PQ index moved to GPU")
            except Exception as e:
                print(f"[FAISS Manager] GPU transfer failed: {e}")
                self.device = 'cpu'
        
        self.indices['primary'] = index
    
    def _init_flat_index(self):
        """Flat ì¸ë±ìŠ¤ ì´ˆê¸°í™” (ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)"""
        index = faiss.IndexFlatIP(self.dimension)  # Inner Product for cosine similarity
        
        if self.device == 'gpu':
            try:
                gpu_res = faiss.StandardGpuResources()
                index = faiss.index_cpu_to_gpu(gpu_res, 0, index)
                print("[FAISS Manager] Flat index moved to GPU")
            except Exception as e:
                print(f"[FAISS Manager] GPU transfer failed: {e}")
                self.device = 'cpu'
        
        self.indices['primary'] = index
    
    def add_vectors(self, vectors, metadata_list=None):
        """ë²¡í„° ì¶”ê°€ (ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›)"""
        start_time = time.time()
        
        # ì…ë ¥ ê²€ì¦ ë° ì •ê·œí™”
        if isinstance(vectors, torch.Tensor):
            vectors = vectors.cpu().numpy()
        
        if len(vectors.shape) == 1:
            vectors = vectors.reshape(1, -1)
        
        vectors = vectors.astype('float32')
        
        # ë²¡í„° ì •ê·œí™” (cosine similarityë¥¼ ìœ„í•´)
        if FAISS_AVAILABLE and self.index_type != 'pytorch':
            faiss.normalize_L2(vectors)
        else:
            # PyTorch ì •ê·œí™”
            vectors_torch = torch.from_numpy(vectors)
            vectors_torch = torch.nn.functional.normalize(vectors_torch, dim=1)
            vectors = vectors_torch.numpy()
        
        # ë©”íƒ€ë°ì´í„° ì²˜ë¦¬
        if metadata_list is None:
            metadata_list = [{}] * len(vectors)
        elif len(metadata_list) != len(vectors):
            raise ValueError(f"Metadata count ({len(metadata_list)}) must match vector count ({len(vectors)})")
        
        # ID í• ë‹¹
        assigned_ids = []
        for i in range(len(vectors)):
            current_id = self.next_id
            self.id_mapping[current_id] = metadata_list[i]
            assigned_ids.append(current_id)
            self.next_id += 1
        
        # ì¸ë±ìŠ¤ì— ì¶”ê°€
        if FAISS_AVAILABLE and 'primary' in self.indices and self.indices['primary'] is not None:
            try:
                # IVF ì¸ë±ìŠ¤ëŠ” í›ˆë ¨ì´ í•„ìš”
                if self.index_type == 'IVF' and not self.indices['primary'].is_trained:
                    if len(vectors) >= 100:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ í›ˆë ¨
                        print("[FAISS Manager] Training IVF index...")
                        self.indices['primary'].train(vectors)
                        print("[FAISS Manager] IVF index training completed")
                    else:
                        print("[FAISS Manager] Not enough data for IVF training, storing in buffer")
                        self._add_to_pytorch_fallback(vectors, assigned_ids, metadata_list)
                        return assigned_ids
                
                # FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
                ids_array = np.array(assigned_ids, dtype=np.int64)
                
                if hasattr(self.indices['primary'], 'add_with_ids'):
                    self.indices['primary'].add_with_ids(vectors, ids_array)
                else:
                    self.indices['primary'].add(vectors)
                
                # ë©”íƒ€ë°ì´í„° ì €ì¥
                for i, metadata in enumerate(metadata_list):
                    self.metadata_storage[assigned_ids[i]] = metadata
                
            except Exception as e:
                print(f"[FAISS Manager] FAISS insertion failed: {e}")
                print("[FAISS Manager] Falling back to PyTorch storage")
                self._add_to_pytorch_fallback(vectors, assigned_ids, metadata_list)
        else:
            # PyTorch fallback
            self._add_to_pytorch_fallback(vectors, assigned_ids, metadata_list)
        
        # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
        insertion_time = time.time() - start_time
        self.performance_stats['insertions'] += len(vectors)
        self.performance_stats['insertion_times'].append(insertion_time)
        
        print(f"[FAISS Manager] Added {len(vectors)} vectors in {insertion_time*1000:.2f}ms")
        
        return assigned_ids
    
    def _add_to_pytorch_fallback(self, vectors, ids, metadata_list):
        """PyTorch fallback ì €ì¥ì†Œì— ì¶”ê°€"""
        for i, (vector, vector_id, metadata) in enumerate(zip(vectors, ids, metadata_list)):
            self.pytorch_storage['vectors'].append(torch.from_numpy(vector.copy()))
            self.pytorch_storage['ids'].append(vector_id)
            self.pytorch_storage['metadata'].append(metadata)
    
    def search(self, query_vectors, k=5, return_metadata=True):
        """ë²¡í„° ê²€ìƒ‰ (Top-K)"""
        start_time = time.time()
        
        # ì…ë ¥ ì²˜ë¦¬
        if isinstance(query_vectors, torch.Tensor):
            query_vectors = query_vectors.cpu().numpy()
        
        if len(query_vectors.shape) == 1:
            query_vectors = query_vectors.reshape(1, -1)
        
        query_vectors = query_vectors.astype('float32')
        
        # ì •ê·œí™”
        if FAISS_AVAILABLE and self.index_type != 'pytorch':
            faiss.normalize_L2(query_vectors)
        else:
            query_torch = torch.from_numpy(query_vectors)
            query_torch = torch.nn.functional.normalize(query_torch, dim=1)
            query_vectors = query_torch.numpy()
        
        try:
            if (FAISS_AVAILABLE and 'primary' in self.indices and 
                self.indices['primary'] is not None and 
                self.indices['primary'].ntotal > 0):
                
                # FAISS ê²€ìƒ‰
                distances, indices = self.indices['primary'].search(query_vectors, k)
                results = self._process_faiss_results(distances, indices, return_metadata)
                
            else:
                # PyTorch fallback ê²€ìƒ‰
                results = self._pytorch_fallback_search(query_vectors, k, return_metadata)
            
        except Exception as e:
            print(f"[FAISS Manager] Search failed: {e}")
            results = self._pytorch_fallback_search(query_vectors, k, return_metadata)
        
        # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
        search_time = time.time() - start_time
        self.performance_stats['searches'] += 1
        self.performance_stats['search_times'].append(search_time)
        
        return results
    
    def _process_faiss_results(self, distances, indices, return_metadata):
        """FAISS ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬"""
        results = []
        
        for query_idx in range(len(distances)):
            query_results = []
            
            for rank, (distance, index) in enumerate(zip(distances[query_idx], indices[query_idx])):
                if index == -1:  # FAISSëŠ” -1ë¡œ ë¹ˆ ê²°ê³¼ í‘œì‹œ
                    continue
                
                result = {
                    'rank': rank + 1,
                    'similarity': float(distance),  # FAISSëŠ” distance ë°˜í™˜
                    'index': int(index),
                    'metadata': self.metadata_storage.get(index, {}) if return_metadata else None
                }
                
                query_results.append(result)
            
            results.append(query_results)
        
        return results
    
    def _pytorch_fallback_search(self, query_vectors, k, return_metadata):
        """PyTorch ê¸°ë°˜ fallback ê²€ìƒ‰"""
        if not self.pytorch_storage['vectors']:
            return [[] for _ in range(len(query_vectors))]
        
        # ì €ì¥ëœ ë²¡í„°ë“¤ì„ í…ì„œë¡œ ë³€í™˜
        stored_vectors = torch.stack(self.pytorch_storage['vectors'])
        query_tensor = torch.from_numpy(query_vectors)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = torch.mm(query_tensor, stored_vectors.T)
        
        results = []
        for query_idx in range(len(query_vectors)):
            query_similarities = similarities[query_idx]
            
            # Top-K ì„ íƒ
            k_actual = min(k, len(query_similarities))
            topk_similarities, topk_indices = torch.topk(query_similarities, k=k_actual, largest=True)
            
            query_results = []
            for rank, (sim, idx) in enumerate(zip(topk_similarities, topk_indices)):
                vector_id = self.pytorch_storage['ids'][idx.item()]
                
                result = {
                    'rank': rank + 1,
                    'similarity': sim.item(),
                    'index': vector_id,
                    'metadata': self.pytorch_storage['metadata'][idx.item()] if return_metadata else None
                }
                query_results.append(result)
            
            results.append(query_results)
        
        return results
    
    def get_statistics(self):
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        stats = {
            'index_info': {
                'type': self.index_type,
                'device': self.device,
                'dimension': self.dimension,
                'faiss_available': FAISS_AVAILABLE
            },
            'storage_info': {
                'total_vectors': self._get_total_vector_count(),
                'faiss_vectors': self._get_faiss_vector_count(),
                'pytorch_vectors': len(self.pytorch_storage['vectors']),
                'metadata_entries': len(self.metadata_storage)
            },
            'performance_stats': {
                'total_insertions': self.performance_stats['insertions'],
                'total_searches': self.performance_stats['searches'],
                'avg_insertion_time_ms': np.mean(self.performance_stats['insertion_times']) * 1000 if self.performance_stats['insertion_times'] else 0,
                'avg_search_time_ms': np.mean(self.performance_stats['search_times']) * 1000 if self.performance_stats['search_times'] else 0,
                'insertion_throughput': self.performance_stats['insertions'] / max(sum(self.performance_stats['insertion_times']), 1e-6),
                'search_throughput': self.performance_stats['searches'] / max(sum(self.performance_stats['search_times']), 1e-6)
            }
        }
        
        return stats
    
    def _get_total_vector_count(self):
        """ì´ ë²¡í„° ìˆ˜ ë°˜í™˜"""
        faiss_count = self._get_faiss_vector_count()
        pytorch_count = len(self.pytorch_storage['vectors'])
        return faiss_count + pytorch_count
    
    def _get_faiss_vector_count(self):
        """FAISS ì¸ë±ìŠ¤ì˜ ë²¡í„° ìˆ˜ ë°˜í™˜"""
        if FAISS_AVAILABLE and 'primary' in self.indices and self.indices['primary'] is not None:
            return self.indices['primary'].ntotal
        return 0
    
    def optimize_index(self):
        """ì¸ë±ìŠ¤ ìµœì í™” (ì¬êµ¬ì„± ë“±)"""
        if not FAISS_AVAILABLE or 'primary' not in self.indices:
            print("[FAISS Manager] No FAISS index to optimize")
            return
        
        print("[FAISS Manager] Starting index optimization...")
        start_time = time.time()
        
        try:
            if self.index_type == 'IVF':
                # IVF ì¸ë±ìŠ¤ì˜ ê²½ìš° nprobe ë™ì  ì¡°ì •
                current_nprobe = self.indices['primary'].nprobe
                total_vectors = self.indices['primary'].ntotal
                
                if total_vectors > 1000:
                    optimal_nprobe = min(50, max(10, int(np.sqrt(total_vectors / 10))))
                    self.indices['primary'].nprobe = optimal_nprobe
                    print(f"[FAISS Manager] IVF nprobe optimized: {current_nprobe} -> {optimal_nprobe}")
            
            elif self.index_type == 'HNSW':
                # HNSWì˜ ê²½ìš° efSearch ë™ì  ì¡°ì •
                total_vectors = self.indices['primary'].ntotal
                if total_vectors > 500:
                    optimal_efSearch = min(100, max(16, int(np.log2(total_vectors) * 8)))
                    self.indices['primary'].hnsw.efSearch = optimal_efSearch
                    print(f"[FAISS Manager] HNSW efSearch optimized to: {optimal_efSearch}")
            
            optimization_time = time.time() - start_time
            print(f"[FAISS Manager] Index optimization completed in {optimization_time*1000:.2f}ms")
            
        except Exception as e:
            print(f"[FAISS Manager] Index optimization failed: {e}")
    
    def save_index(self, save_path):
        """ì¸ë±ìŠ¤ ì €ì¥"""
        save_path = Path(save_path)
        save_path.parent.mkdir(parents=True, exist_ok=True)
        
        save_data = {
            'index_type': self.index_type,
            'dimension': self.dimension,
            'device': self.device,
            'metadata_storage': self.metadata_storage,
            'id_mapping': self.id_mapping,
            'next_id': self.next_id,
            'performance_stats': self.performance_stats,
            'pytorch_storage': {
                'vectors': [v.tolist() for v in self.pytorch_storage['vectors']],
                'ids': self.pytorch_storage['ids'],
                'metadata': self.pytorch_storage['metadata']
            }
        }
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        with open(save_path.with_suffix('.json'), 'w') as f:
            json.dump(save_data, f, indent=2, default=str)
        
        # FAISS ì¸ë±ìŠ¤ ì €ì¥
        if FAISS_AVAILABLE and 'primary' in self.indices and self.indices['primary'] is not None:
            try:
                faiss_path = save_path.with_suffix('.faiss')
                faiss.write_index(self.indices['primary'], str(faiss_path))
                print(f"[FAISS Manager] Index saved to: {faiss_path}")
            except Exception as e:
                print(f"[FAISS Manager] FAISS index save failed: {e}")
        
        print(f"[FAISS Manager] Metadata saved to: {save_path.with_suffix('.json')}")
    
    def load_index(self, load_path):
        """ì¸ë±ìŠ¤ ë¡œë“œ"""
        load_path = Path(load_path)
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        json_path = load_path.with_suffix('.json')
        if json_path.exists():
            with open(json_path, 'r') as f:
                save_data = json.load(f)
            
            self.metadata_storage = save_data['metadata_storage']
            self.id_mapping = save_data['id_mapping']
            self.next_id = save_data['next_id']
            self.performance_stats = save_data['performance_stats']
            
            # PyTorch ì €ì¥ì†Œ ë³µì›
            pytorch_data = save_data['pytorch_storage']
            self.pytorch_storage = {
                'vectors': [torch.tensor(v) for v in pytorch_data['vectors']],
                'ids': pytorch_data['ids'],
                'metadata': pytorch_data['metadata']
            }
            
            print(f"[FAISS Manager] Metadata loaded from: {json_path}")
        
        # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        faiss_path = load_path.with_suffix('.faiss')
        if FAISS_AVAILABLE and faiss_path.exists():
            try:
                index = faiss.read_index(str(faiss_path))
                self.indices['primary'] = index
                print(f"[FAISS Manager] FAISS index loaded from: {faiss_path}")
            except Exception as e:
                print(f"[FAISS Manager] FAISS index load failed: {e}")

# í…ŒìŠ¤íŠ¸ ë° ë²¤ì¹˜ë§ˆí‚¹ í´ë˜ìŠ¤
class FAISSBenchmark:
    """FAISS ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹"""
    
    def __init__(self):
        self.results = {}
    
    def run_comprehensive_benchmark(self, dimensions=[64, 128, 256], vector_counts=[100, 500, 1000]):
        """ì¢…í•©ì ì¸ FAISS ë²¤ì¹˜ë§ˆí¬"""
        print("\nğŸ”¬ FAISS ì¢…í•© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
        print("="*70)
        
        for dim in dimensions:
            for count in vector_counts:
                print(f"\nğŸ“Š Testing: {dim}D vectors, {count} samples")
                self._benchmark_configuration(dim, count)
        
        self._generate_benchmark_report()
    
    def _benchmark_configuration(self, dimension, vector_count):
        """íŠ¹ì • ì„¤ì •ì— ëŒ€í•œ ë²¤ì¹˜ë§ˆí¬"""
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        vectors = torch.randn(vector_count, dimension)
        query_vectors = torch.randn(10, dimension)  # 10ê°œ ì¿¼ë¦¬
        
        # ê° ì¸ë±ìŠ¤ íƒ€ì…ë³„ í…ŒìŠ¤íŠ¸
        index_types = ['Flat', 'HNSW']
        if FAISS_AVAILABLE:
            index_types.extend(['IVF', 'PQ'])
        
        config_key = f"{dimension}D_{vector_count}vec"
        self.results[config_key] = {}
        
        for index_type in index_types:
            try:
                print(f"   Testing {index_type} index...")
                
                # ë§¤ë‹ˆì € ìƒì„±
                manager = OptimizedFAISSManager(
                    dimension=dimension,
                    index_type=index_type,
                    device='cpu'  # ì¼ê´€ëœ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ CPU ì‚¬ìš©
                )
                
                # ë²¡í„° ì¶”ê°€ ì„±ëŠ¥ ì¸¡ì •
                add_start = time.time()
                manager.add_vectors(vectors)
                add_time = time.time() - add_start
                
                # ê²€ìƒ‰ ì„±ëŠ¥ ì¸¡ì •
                search_start = time.time()
                results = manager.search(query_vectors, k=5)
                search_time = time.time() - search_start
                
                # í†µê³„ ìˆ˜ì§‘
                stats = manager.get_statistics()
                
                self.results[config_key][index_type] = {
                    'add_time_ms': add_time * 1000,
                    'search_time_ms': search_time * 1000,
                    'add_throughput': vector_count / add_time,
                    'search_throughput': len(query_vectors) / search_time,
                    'memory_efficiency': stats['storage_info']['total_vectors'],
                    'avg_search_accuracy': self._estimate_accuracy(results)
                }
                
                print(f"     Add: {add_time*1000:.2f}ms, Search: {search_time*1000:.2f}ms")
                
            except Exception as e:
                print(f"     Failed: {e}")
                self.results[config_key][index_type] = {'error': str(e)}
    
    def _estimate_accuracy(self, search_results):
        """ê²€ìƒ‰ ì •í™•ë„ ì¶”ì • (ë”ë¯¸ ë°ì´í„°ì´ë¯€ë¡œ ì™„ë²½í•˜ì§€ ì•ŠìŒ)"""
        if not search_results or not search_results[0]:
            return 0.0
        
        # ì²« ë²ˆì§¸ ì¿¼ë¦¬ ê²°ê³¼ë§Œ ì‚¬ìš©
        first_result = search_results[0]
        if not first_result:
            return 0.0
        
        # Top-1 ìœ ì‚¬ë„ë¥¼ ì •í™•ë„ì˜ ê·¼ì‚¬ì¹˜ë¡œ ì‚¬ìš©
        top1_similarity = first_result[0]['similarity']
        return float(top1_similarity)
    
    def _generate_benchmark_report(self):
        """ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\nğŸ“Š FAISS ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼:")
        print("="*70)
        
        for config, results in self.results.items():
            print(f"\nğŸ”§ Configuration: {config}")
            
            for index_type, metrics in results.items():
                if 'error' in metrics:
                    print(f"   {index_type:8}: ERROR - {metrics['error']}")
                else:
                    print(f"   {index_type:8}: Add {metrics['add_time_ms']:6.2f}ms, "
                          f"Search {metrics['search_time_ms']:6.2f}ms, "
                          f"Throughput {metrics['add_throughput']:6.0f}/s")
        
        # ìµœì  ì„¤ì • ì¶”ì²œ
        self._recommend_optimal_config()
    
    def _recommend_optimal_config(self):
        """ìµœì  ì„¤ì • ì¶”ì²œ"""
        print("\nğŸ’¡ ì¶”ì²œ ì„¤ì •:")
        
        best_speed = None
        best_accuracy = None
        best_memory = None
        
        for config, results in self.results.items():
            for index_type, metrics in results.items():
                if 'error' in metrics:
                    continue
                
                # ì†ë„ ê¸°ì¤€
                if best_speed is None or metrics['search_time_ms'] < best_speed[1]['search_time_ms']:
                    best_speed = (f"{config}_{index_type}", metrics)
                
                # ì •í™•ë„ ê¸°ì¤€
                if best_accuracy is None or metrics['avg_search_accuracy'] > best_accuracy[1]['avg_search_accuracy']:
                    best_accuracy = (f"{config}_{index_type}", metrics)
        
        if best_speed:
            print(f"   âš¡ ìµœê³  ì†ë„: {best_speed[0]} ({best_speed[1]['search_time_ms']:.2f}ms)")
        
        if best_accuracy:
            print(f"   ğŸ¯ ìµœê³  ì •í™•ë„: {best_accuracy[0]} (similarity: {best_accuracy[1]['avg_search_accuracy']:.3f})")

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def run_phase_1_3():
    """Phase 1.3 ì‹¤í–‰"""
    print("ğŸ¥¥ COCONUT Phase 1.3: FAISS í†µí•© ìµœì í™” ì‹œì‘")
    print("="*80)
    
    # 1. ê¸°ë³¸ FAISS ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸
    print("\nğŸ”§ 1. OptimizedFAISSManager ê¸°ë³¸ í…ŒìŠ¤íŠ¸...")
    
    manager = OptimizedFAISSManager(dimension=128, index_type='auto')
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    test_vectors = torch.randn(50, 128)
    test_metadata = [{'user_id': i % 10, 'timestamp': time.time()} for i in range(50)]
    
    # ë²¡í„° ì¶”ê°€ í…ŒìŠ¤íŠ¸
    print("   ë²¡í„° ì¶”ê°€ í…ŒìŠ¤íŠ¸...")
    ids = manager.add_vectors(test_vectors, test_metadata)
    print(f"   ì¶”ê°€ëœ ë²¡í„° ID: {ids[:5]}...{ids[-5:]}")
    
    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("   ê²€ìƒ‰ í…ŒìŠ¤íŠ¸...")
    query = torch.randn(3, 128)
    search_results = manager.search(query, k=5)
    
    print(f"   ê²€ìƒ‰ ê²°ê³¼: {len(search_results)} queries processed")
    for i, results in enumerate(search_results[:2]):  # ì²˜ìŒ 2ê°œ ì¿¼ë¦¬ë§Œ ì¶œë ¥
        print(f"     Query {i}: {len(results)} results")
        if results:
            print(f"       Top result: similarity={results[0]['similarity']:.3f}")
    
    # í†µê³„ í™•ì¸
    stats = manager.get_statistics()
    print(f"\nğŸ“Š 2. ì„±ëŠ¥ í†µê³„:")
    print(f"   ì¸ë±ìŠ¤ íƒ€ì…: {stats['index_info']['type']}")
    print(f"   ì´ ë²¡í„° ìˆ˜: {stats['storage_info']['total_vectors']}")
    print(f"   í‰ê·  ê²€ìƒ‰ ì‹œê°„: {stats['performance_stats']['avg_search_time_ms']:.2f}ms")
    
    # 3. ì¸ë±ìŠ¤ ìµœì í™” í…ŒìŠ¤íŠ¸
    print(f"\nâš™ï¸ 3. ì¸ë±ìŠ¤ ìµœì í™” í…ŒìŠ¤íŠ¸...")
    manager.optimize_index()
    
    # 4. ì €ì¥/ë¡œë“œ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ’¾ 4. ì €ì¥/ë¡œë“œ í…ŒìŠ¤íŠ¸...")
    save_path = Path("./analysis_results/faiss_test_index")
    manager.save_index(save_path)
    
    # ìƒˆ ë§¤ë‹ˆì €ë¡œ ë¡œë“œ í…ŒìŠ¤íŠ¸
    new_manager = OptimizedFAISSManager(dimension=128, index_type='auto')
    new_manager.load_index(save_path)
    
    new_stats = new_manager.get_statistics()
    print(f"   ë¡œë“œëœ ë²¡í„° ìˆ˜: {new_stats['storage_info']['total_vectors']}")
    
    # 5. ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    print(f"\nğŸ”¬ 5. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬...")
    benchmark = FAISSBenchmark()
    benchmark.run_comprehensive_benchmark(
        dimensions=[128],  # 128Dë§Œ í…ŒìŠ¤íŠ¸ (ë¹ ë¥¸ ì‹¤í–‰)
        vector_counts=[100, 500]  # ì‘ì€ í¬ê¸°ë¡œ í…ŒìŠ¤íŠ¸
    )
    
    print(f"\nâœ… Phase 1.3 ì™„ë£Œ!")
    print(f"ê°œì„ ëœ ê¸°ëŠ¥:")
    print(f"  âœ… ë‹¤ì¤‘ ì¸ë±ìŠ¤ íƒ€ì… ì§€ì› (HNSW, IVF, PQ, Flat)")
    print(f"  âœ… ìë™ CPU/GPU ì „í™˜")
    print(f"  âœ… ì•ˆì •ì ì¸ PyTorch fallback")
    print(f"  âœ… ë™ì  ì¸ë±ìŠ¤ ìµœì í™”")
    print(f"  âœ… ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹")
    print(f"  âœ… ì¸ë±ìŠ¤ ì €ì¥/ë¡œë“œ")
    
    print(f"\nâ¡ï¸  ë‹¤ìŒ ë‹¨ê³„: Phase 2.1 (Quality Assessment ëª¨ë“ˆ êµ¬í˜„)")
    
    return manager, stats

if __name__ == "__main__":
    manager, stats = run_phase_1_3()
    
    print(f"\nğŸ‰ Phase 1.3 ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
    print(f"FAISS í†µí•©ì´ í¬ê²Œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"í˜„ì¬ ì„¤ì •: {stats['index_info']['type']} ì¸ë±ìŠ¤, {stats['storage_info']['total_vectors']}ê°œ ë²¡í„°")


<ê²°ê³¼>
[FAISS] âœ… FAISS library available
ğŸ¥¥ COCONUT Phase 1.3: FAISS í†µí•© ìµœì í™” ì‹œì‘
================================================================================

ğŸ”§ 1. OptimizedFAISSManager ê¸°ë³¸ í…ŒìŠ¤íŠ¸...
[FAISS Manager] Failed to initialize HNSW: Wrong number or type of arguments for overloaded function 'new_IndexHNSWFlat'.
  Possible C/C++ prototypes are:
    faiss::IndexHNSWFlat::IndexHNSWFlat()
    faiss::IndexHNSWFlat::IndexHNSWFlat(int,int,faiss::MetricType)
    faiss::IndexHNSWFlat::IndexHNSWFlat(int,int)

[FAISS Manager] Falling back to PyTorch implementation
[FAISS Manager] Initialized:
   Dimension: 128
   Index Type: pytorch
   Device: cpu
   FAISS Available: True
   ë²¡í„° ì¶”ê°€ í…ŒìŠ¤íŠ¸...
[FAISS Manager] Added 50 vectors in 0.73ms
   ì¶”ê°€ëœ ë²¡í„° ID: [0, 1, 2, 3, 4]...[45, 46, 47, 48, 49]
   ê²€ìƒ‰ í…ŒìŠ¤íŠ¸...
   ê²€ìƒ‰ ê²°ê³¼: 3 queries processed
     Query 0: 5 results
       Top result: similarity=0.184
     Query 1: 5 results
       Top result: similarity=0.221

ğŸ“Š 2. ì„±ëŠ¥ í†µê³„:
   ì¸ë±ìŠ¤ íƒ€ì…: pytorch
   ì´ ë²¡í„° ìˆ˜: 50
   í‰ê·  ê²€ìƒ‰ ì‹œê°„: 0.78ms

âš™ï¸ 3. ì¸ë±ìŠ¤ ìµœì í™” í…ŒìŠ¤íŠ¸...
[FAISS Manager] No FAISS index to optimize

ğŸ’¾ 4. ì €ì¥/ë¡œë“œ í…ŒìŠ¤íŠ¸...
[FAISS Manager] Metadata saved to: analysis_results/faiss_test_index.json
[FAISS Manager] Failed to initialize HNSW: Wrong number or type of arguments for overloaded function 'new_IndexHNSWFlat'.
  Possible C/C++ prototypes are:
    faiss::IndexHNSWFlat::IndexHNSWFlat()
    faiss::IndexHNSWFlat::IndexHNSWFlat(int,int,faiss::MetricType)
    faiss::IndexHNSWFlat::IndexHNSWFlat(int,int)

[FAISS Manager] Falling back to PyTorch implementation
[FAISS Manager] Initialized:
   Dimension: 128
   Index Type: pytorch
   Device: cpu
   FAISS Available: True
[FAISS Manager] Metadata loaded from: analysis_results/faiss_test_index.json
   ë¡œë“œëœ ë²¡í„° ìˆ˜: 50

ğŸ”¬ 5. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬...

ğŸ”¬ FAISS ì¢…í•© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘
======================================================================

ğŸ“Š Testing: 128D vectors, 100 samples
   Testing Flat index...
[FAISS Manager] Flat index initialized successfully
[FAISS Manager] Initialized:
   Dimension: 128
   Index Type: Flat
   Device: cpu
   FAISS Available: True
[FAISS Manager] FAISS insertion failed: Error in virtual void faiss::Index::add_with_ids(faiss::idx_t, const float*, const faiss::idx_t*) at /project/faiss/faiss/Index.cpp:45: add_with_ids not implemented for this type of index
[FAISS Manager] Falling back to PyTorch storage
[FAISS Manager] Added 100 vectors in 0.42ms
     Add: 0.43ms, Search: 0.77ms
   Testing HNSW index...
[FAISS Manager] Failed to initialize HNSW: Wrong number or type of arguments for overloaded function 'new_IndexHNSWFlat'.
  Possible C/C++ prototypes are:
    faiss::IndexHNSWFlat::IndexHNSWFlat()
    faiss::IndexHNSWFlat::IndexHNSWFlat(int,int,faiss::MetricType)
    faiss::IndexHNSWFlat::IndexHNSWFlat(int,int)

[FAISS Manager] Falling back to PyTorch implementation
[FAISS Manager] Initialized:
   Dimension: 128
   Index Type: pytorch
   Device: cpu
   FAISS Available: True
[FAISS Manager] Added 100 vectors in 0.36ms
     Add: 0.38ms, Search: 0.66ms
   Testing IVF index...
[FAISS Manager] IVF index initialized successfully
[FAISS Manager] Initialized:
   Dimension: 128
   Index Type: IVF
   Device: cpu
   FAISS Available: True
[FAISS Manager] Training IVF index...
[FAISS Manager] IVF index training completed
[FAISS Manager] Added 100 vectors in 1.03ms
     Add: 1.04ms, Search: 0.39ms
   Testing PQ index...
[FAISS Manager] PQ index initialized successfully
[FAISS Manager] Initialized:
   Dimension: 128
   Index Type: PQ
   Device: cpu
   FAISS Available: True
[FAISS Manager] FAISS insertion failed: Error in virtual void faiss::Index::add_with_ids(faiss::idx_t, const float*, const faiss::idx_t*) at /project/faiss/faiss/Index.cpp:45: add_with_ids not implemented for this type of index
[FAISS Manager] Falling back to PyTorch storage
[FAISS Manager] Added 100 vectors in 0.46ms
     Add: 0.48ms, Search: 0.91ms

ğŸ“Š Testing: 128D vectors, 500 samples
   Testing Flat index...
[FAISS Manager] Flat index initialized successfully
[FAISS Manager] Initialized:
   Dimension: 128
   Index Type: Flat
   Device: cpu
   FAISS Available: True
[FAISS Manager] FAISS insertion failed: Error in virtual void faiss::Index::add_with_ids(faiss::idx_t, const float*, const faiss::idx_t*) at /project/faiss/faiss/Index.cpp:45: add_with_ids not implemented for this type of index
[FAISS Manager] Falling back to PyTorch storage
[FAISS Manager] Added 500 vectors in 3.04ms
     Add: 3.06ms, Search: 1.21ms
   Testing HNSW index...
[FAISS Manager] Failed to initialize HNSW: Wrong number or type of arguments for overloaded function 'new_IndexHNSWFlat'.
  Possible C/C++ prototypes are:
    faiss::IndexHNSWFlat::IndexHNSWFlat()
    faiss::IndexHNSWFlat::IndexHNSWFlat(int,int,faiss::MetricType)
    faiss::IndexHNSWFlat::IndexHNSWFlat(int,int)

[FAISS Manager] Falling back to PyTorch implementation
[FAISS Manager] Initialized:
   Dimension: 128
   Index Type: pytorch
   Device: cpu
   FAISS Available: True
[FAISS Manager] Added 500 vectors in 1.69ms
     Add: 1.71ms, Search: 1.06ms
   Testing IVF index...
[FAISS Manager] IVF index initialized successfully
[FAISS Manager] Initialized:
   Dimension: 128
   Index Type: IVF
   Device: cpu
   FAISS Available: True
[FAISS Manager] Training IVF index...
[FAISS Manager] IVF index training completed
[FAISS Manager] Added 500 vectors in 2.20ms
     Add: 2.22ms, Search: 0.33ms
   Testing PQ index...
[FAISS Manager] PQ index initialized successfully
[FAISS Manager] Initialized:
   Dimension: 128
   Index Type: PQ
   Device: cpu
   FAISS Available: True
[FAISS Manager] FAISS insertion failed: Error in virtual void faiss::Index::add_with_ids(faiss::idx_t, const float*, const faiss::idx_t*) at /project/faiss/faiss/Index.cpp:45: add_with_ids not implemented for this type of index
[FAISS Manager] Falling back to PyTorch storage
[FAISS Manager] Added 500 vectors in 1.60ms
     Add: 1.61ms, Search: 1.18ms

ğŸ“Š FAISS ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼:
======================================================================

ğŸ”§ Configuration: 128D_100vec
   Flat    : Add   0.43ms, Search   0.77ms, Throughput 230456/s
   HNSW    : Add   0.38ms, Search   0.66ms, Throughput 265294/s
   IVF     : Add   1.04ms, Search   0.39ms, Throughput  95958/s
   PQ      : Add   0.48ms, Search   0.91ms, Throughput 209715/s

ğŸ”§ Configuration: 128D_500vec
   Flat    : Add   3.06ms, Search   1.21ms, Throughput 163419/s
   HNSW    : Add   1.71ms, Search   1.06ms, Throughput 292286/s
   IVF     : Add   2.22ms, Search   0.33ms, Throughput 224968/s
   PQ      : Add   1.61ms, Search   1.18ms, Throughput 310138/s

ğŸ’¡ ì¶”ì²œ ì„¤ì •:
   âš¡ ìµœê³  ì†ë„: 128D_500vec_IVF (0.33ms)
   ğŸ¯ ìµœê³  ì •í™•ë„: 128D_100vec_IVF (similarity: 1.596)

âœ… Phase 1.3 ì™„ë£Œ!
ê°œì„ ëœ ê¸°ëŠ¥:
  âœ… ë‹¤ì¤‘ ì¸ë±ìŠ¤ íƒ€ì… ì§€ì› (HNSW, IVF, PQ, Flat)
  âœ… ìë™ CPU/GPU ì „í™˜
  âœ… ì•ˆì •ì ì¸ PyTorch fallback
  âœ… ë™ì  ì¸ë±ìŠ¤ ìµœì í™”
  âœ… ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹
  âœ… ì¸ë±ìŠ¤ ì €ì¥/ë¡œë“œ

â¡ï¸  ë‹¤ìŒ ë‹¨ê³„: Phase 2.1 (Quality Assessment ëª¨ë“ˆ êµ¬í˜„)

ğŸ‰ Phase 1.3 ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!
FAISS í†µí•©ì´ í¬ê²Œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤.
í˜„ì¬ ì„¤ì •: pytorch ì¸ë±ìŠ¤, 50ê°œ ë²¡í„°


# Phase 2.2: Loop Closure Detection êµ¬í˜„
# ëª©í‘œ: SLAM Loop Closure â†’ Biometric User Re-identification í•µì‹¬ êµ¬í˜„

import torch
import torch.nn.functional as F
import numpy as np
import time
import json
import pickle
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Union
from collections import defaultdict
import matplotlib.pyplot as plt

class UserProfile:
    """ì‚¬ìš©ìë³„ ì„ë² ë”© íˆìŠ¤í† ë¦¬ ê´€ë¦¬"""
    
    def __init__(self, user_id: int, max_embeddings: int = 20, temporal_window_days: int = 30):
        self.user_id = user_id
        self.max_embeddings = max_embeddings
        self.temporal_window_days = temporal_window_days
        
        # ì„ë² ë”© íˆìŠ¤í† ë¦¬: (embedding, timestamp, quality_score)
        self.embedding_history = []
        
        # ì‚¬ìš©ì í†µê³„
        self.creation_time = datetime.now()
        self.last_updated = datetime.now()
        self.total_accesses = 0
        self.drift_corrections = 0
        
        # ëŒ€í‘œ ì„ë² ë”© (ê°€ì¤‘ í‰ê· )
        self._representative_embedding = None
        self._last_update_time = None
        
    def add_embedding(self, embedding: torch.Tensor, quality_score: float = 1.0):
        """ìƒˆë¡œìš´ ì„ë² ë”© ì¶”ê°€"""
        current_time = datetime.now()
        
        # ì„ë² ë”© ì €ì¥
        self.embedding_history.append({
            'embedding': embedding.clone().detach().cpu(),
            'timestamp': current_time,
            'quality_score': quality_score
        })
        
        # ìµœëŒ€ ê°œìˆ˜ ì œí•œ (í’ˆì§ˆ ë†’ì€ ìˆœìœ¼ë¡œ ìœ ì§€)
        if len(self.embedding_history) > self.max_embeddings:
            self.embedding_history.sort(key=lambda x: x['quality_score'], reverse=True)
            self.embedding_history = self.embedding_history[:self.max_embeddings]
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.last_updated = current_time
        self.total_accesses += 1
        
        # ëŒ€í‘œ ì„ë² ë”© ë¬´íš¨í™” (ë‹¤ìŒ ì ‘ê·¼ ì‹œ ì¬ê³„ì‚°)
        self._representative_embedding = None
        
    def get_representative_embedding(self, current_time: datetime = None) -> torch.Tensor:
        """ì‹œê°„ ê°€ì¤‘ ëŒ€í‘œ ì„ë² ë”© ê³„ì‚°"""
        if current_time is None:
            current_time = datetime.now()
        
        # ìºì‹œëœ ëŒ€í‘œ ì„ë² ë”©ì´ ìµœì‹ ì´ë©´ ì¬ì‚¬ìš©
        if (self._representative_embedding is not None and 
            self._last_update_time is not None and
            (current_time - self._last_update_time).total_seconds() < 300):  # 5ë¶„ ìºì‹œ
            return self._representative_embedding
        
        if not self.embedding_history:
            return None
        
        # ì‹œê°„ì  ê°€ì¤‘ì¹˜ ê³„ì‚°
        valid_embeddings = []
        weights = []
        
        for record in self.embedding_history:
            age_days = (current_time - record['timestamp']).days
            
            # ì‹œê°„ ìœˆë„ìš° ë‚´ì˜ ì„ë² ë”©ë§Œ ì‚¬ìš©
            if age_days <= self.temporal_window_days:
                # ì‹œê°„ì  ê°ì‡  (10ì¼ ë°˜ê°ê¸°)
                temporal_weight = np.exp(-age_days / 10.0)
                # í’ˆì§ˆ ê°€ì¤‘ì¹˜
                quality_weight = record['quality_score']
                # ìµœì¢… ê°€ì¤‘ì¹˜
                final_weight = temporal_weight * quality_weight
                
                valid_embeddings.append(record['embedding'])
                weights.append(final_weight)
        
        if not valid_embeddings:
            # ëª¨ë“  ì„ë² ë”©ì´ ë§Œë£Œëœ ê²½ìš° ê°€ì¥ ìµœê·¼ ê²ƒ ì‚¬ìš©
            latest_record = max(self.embedding_history, key=lambda x: x['timestamp'])
            self._representative_embedding = latest_record['embedding'].clone()
        else:
            # ê°€ì¤‘ í‰ê·  ê³„ì‚°
            embeddings_tensor = torch.stack(valid_embeddings)
            weights_tensor = torch.tensor(weights, dtype=torch.float32)
            
            # ì •ê·œí™”
            weights_tensor = weights_tensor / weights_tensor.sum()
            
            # ê°€ì¤‘ í‰ê· 
            weighted_embedding = torch.sum(
                embeddings_tensor * weights_tensor.unsqueeze(1), 
                dim=0
            )
            
            # ì •ê·œí™”
            self._representative_embedding = F.normalize(weighted_embedding.unsqueeze(0), dim=1).squeeze(0)
        
        self._last_update_time = current_time
        return self._representative_embedding
    
    def compute_drift(self, new_embedding: torch.Tensor) -> float:
        """í˜„ì¬ ì„ë² ë”©ê³¼ì˜ drift ê³„ì‚°"""
        representative = self.get_representative_embedding()
        if representative is None:
            return 0.0
        
        # ì½”ì‚¬ì¸ ê±°ë¦¬ = 1 - ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        similarity = F.cosine_similarity(
            new_embedding.unsqueeze(0), 
            representative.unsqueeze(0)
        ).item()
        
        drift = 1.0 - similarity
        return max(0.0, drift)  # ìŒìˆ˜ ë°©ì§€
    
    def prune_old_embeddings(self):
        """ì˜¤ë˜ëœ ì„ë² ë”© ì œê±°"""
        cutoff_time = datetime.now() - timedelta(days=self.temporal_window_days)
        
        original_count = len(self.embedding_history)
        self.embedding_history = [
            record for record in self.embedding_history
            if record['timestamp'] > cutoff_time
        ]
        
        removed_count = original_count - len(self.embedding_history)
        if removed_count > 0:
            print(f"[UserProfile] User {self.user_id}: Pruned {removed_count} old embeddings")
            self._representative_embedding = None  # ì¬ê³„ì‚° í•„ìš”
    
    def get_statistics(self) -> dict:
        """ì‚¬ìš©ì í”„ë¡œí•„ í†µê³„"""
        if not self.embedding_history:
            return {
                'user_id': self.user_id,
                'embedding_count': 0,
                'age_days': (datetime.now() - self.creation_time).days,
                'last_access_days_ago': (datetime.now() - self.last_updated).days
            }
        
        timestamps = [record['timestamp'] for record in self.embedding_history]
        qualities = [record['quality_score'] for record in self.embedding_history]
        
        return {
            'user_id': self.user_id,
            'embedding_count': len(self.embedding_history),
            'age_days': (datetime.now() - self.creation_time).days,
            'last_access_days_ago': (datetime.now() - self.last_updated).days,
            'total_accesses': self.total_accesses,
            'drift_corrections': self.drift_corrections,
            'avg_quality': np.mean(qualities),
            'temporal_span_days': (max(timestamps) - min(timestamps)).days if len(timestamps) > 1 else 0
        }

class LoopClosureDetector:
    """SLAM Loop Closure â†’ Biometric User Re-identification"""
    
    def __init__(self, 
                 similarity_threshold: float = 0.7,
                 drift_threshold: float = 0.15,
                 temporal_window_days: int = 30,
                 min_samples_for_detection: int = 2):
        
        self.similarity_threshold = similarity_threshold
        self.drift_threshold = drift_threshold
        self.temporal_window_days = temporal_window_days
        self.min_samples_for_detection = min_samples_for_detection
        
        # ì‚¬ìš©ì í”„ë¡œí•„ ì €ì¥ì†Œ
        self.user_profiles: Dict[int, UserProfile] = {}
        
        # Loop closure ì´ë²¤íŠ¸ ê¸°ë¡
        self.loop_closure_events = []
        
        # ì„±ëŠ¥ í†µê³„
        self.detection_stats = {
            'total_detections': 0,
            'successful_detections': 0,
            'false_positives': 0,
            'drift_events': 0,
            'processing_times': []
        }
        
        print(f"[Loop Closure] ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   Similarity threshold: {similarity_threshold}")
        print(f"   Drift threshold: {drift_threshold}")
        print(f"   Temporal window: {temporal_window_days} days")
    
    def detect_loop_closure(self, 
                           current_embedding: torch.Tensor, 
                           candidate_user_id: int = None,
                           quality_score: float = 1.0) -> dict:
        """
        Loop Closure ê°ì§€
        
        Args:
            current_embedding: í˜„ì¬ ì…ë ¥ì˜ ì„ë² ë”©
            candidate_user_id: ì˜ˆìƒ ì‚¬ìš©ì ID (ìˆìœ¼ë©´ ìš°ì„  ê²€ì‚¬)
            quality_score: í˜„ì¬ ì„ë² ë”©ì˜ í’ˆì§ˆ ì ìˆ˜
            
        Returns:
            detection_result: Loop closure ê²°ê³¼
        """
        start_time = time.time()
        self.detection_stats['total_detections'] += 1
        
        # 1. Candidate-first search (ì˜ˆìƒ ì‚¬ìš©ì ìš°ì„  ê²€ì‚¬)
        if candidate_user_id is not None and candidate_user_id in self.user_profiles:
            candidate_result = self._check_user_similarity(current_embedding, candidate_user_id)
            
            if candidate_result['is_loop_closure']:
                processing_time = time.time() - start_time
                self.detection_stats['processing_times'].append(processing_time)
                
                result = {
                    'is_loop_closure': True,
                    'matched_user_id': candidate_user_id,
                    'similarity': candidate_result['similarity'],
                    'drift_magnitude': candidate_result['drift'],
                    'detection_type': 'candidate_match',
                    'processing_time_ms': processing_time * 1000,
                    'confidence': self._compute_confidence(candidate_result)
                }
                
                self._record_loop_closure_event(result, current_embedding, quality_score)
                return result
        
        # 2. Global search (ì „ì²´ ì‚¬ìš©ì ëŒ€ìƒ ê²€ìƒ‰)
        global_result = self._global_user_search(current_embedding)
        
        if global_result['is_loop_closure']:
            processing_time = time.time() - start_time
            self.detection_stats['processing_times'].append(processing_time)
            
            result = {
                'is_loop_closure': True,
                'matched_user_id': global_result['user_id'],
                'similarity': global_result['similarity'],
                'drift_magnitude': global_result['drift'],
                'detection_type': 'global_search',
                'processing_time_ms': processing_time * 1000,
                'confidence': self._compute_confidence(global_result)
            }
            
            self._record_loop_closure_event(result, current_embedding, quality_score)
            return result
        
        # 3. No loop closure detected
        processing_time = time.time() - start_time
        self.detection_stats['processing_times'].append(processing_time)
        
        return {
            'is_loop_closure': False,
            'matched_user_id': None,
            'similarity': 0.0,
            'drift_magnitude': float('inf'),
            'detection_type': 'no_match',
            'processing_time_ms': processing_time * 1000,
            'confidence': 0.0
        }
    
    def _check_user_similarity(self, embedding: torch.Tensor, user_id: int) -> dict:
        """íŠ¹ì • ì‚¬ìš©ìì™€ì˜ ìœ ì‚¬ë„ ë° drift ê³„ì‚°"""
        if user_id not in self.user_profiles:
            return {'is_loop_closure': False, 'similarity': 0.0, 'drift': float('inf')}
        
        user_profile = self.user_profiles[user_id]
        
        # ëŒ€í‘œ ì„ë² ë”©ê³¼ ìœ ì‚¬ë„ ê³„ì‚°
        representative_embedding = user_profile.get_representative_embedding()
        if representative_embedding is None:
            return {'is_loop_closure': False, 'similarity': 0.0, 'drift': float('inf')}
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarity = F.cosine_similarity(
            embedding.unsqueeze(0),
            representative_embedding.unsqueeze(0)
        ).item()
        
        # Drift ê³„ì‚°
        drift = user_profile.compute_drift(embedding)
        
        # Loop closure íŒì •
        is_loop_closure = (
            similarity > self.similarity_threshold and
            len(user_profile.embedding_history) >= self.min_samples_for_detection
        )
        
        return {
            'is_loop_closure': is_loop_closure,
            'similarity': similarity,
            'drift': drift,
            'user_id': user_id
        }
    
    def _global_user_search(self, embedding: torch.Tensor) -> dict:
        """ì „ì²´ ì‚¬ìš©ì ëŒ€ìƒ ìµœì  ë§¤ì¹˜ ê²€ìƒ‰"""
        best_result = {
            'is_loop_closure': False,
            'user_id': None,
            'similarity': 0.0,
            'drift': float('inf')
        }
        
        for user_id in self.user_profiles:
            user_result = self._check_user_similarity(embedding, user_id)
            
            if (user_result['is_loop_closure'] and 
                user_result['similarity'] > best_result['similarity']):
                best_result = user_result
        
        return best_result
    
    def _compute_confidence(self, detection_result: dict) -> float:
        """Detection confidence ê³„ì‚°"""
        if not detection_result['is_loop_closure']:
            return 0.0
        
        similarity = detection_result['similarity']
        drift = detection_result['drift']
        
        # ìœ ì‚¬ë„ ê¸°ë°˜ ì‹ ë¢°ë„
        similarity_confidence = (similarity - self.similarity_threshold) / (1.0 - self.similarity_threshold)
        
        # Drift ê¸°ë°˜ ì‹ ë¢°ë„ (ë‚®ì€ driftê°€ ë†’ì€ ì‹ ë¢°ë„)
        drift_confidence = max(0.0, 1.0 - drift / self.drift_threshold)
        
        # ì¡°í•©
        overall_confidence = 0.7 * similarity_confidence + 0.3 * drift_confidence
        return min(1.0, max(0.0, overall_confidence))
    
    def _record_loop_closure_event(self, result: dict, embedding: torch.Tensor, quality_score: float):
        """Loop closure ì´ë²¤íŠ¸ ê¸°ë¡"""
        event = {
            'timestamp': datetime.now(),
            'user_id': result['matched_user_id'],
            'similarity': result['similarity'],
            'drift_magnitude': result['drift_magnitude'],
            'confidence': result['confidence'],
            'detection_type': result['detection_type'],
            'processing_time_ms': result['processing_time_ms'],
            'quality_score': quality_score
        }
        
        self.loop_closure_events.append(event)
        
        # ìµœê·¼ 1000ê°œ ì´ë²¤íŠ¸ë§Œ ìœ ì§€
        if len(self.loop_closure_events) > 1000:
            self.loop_closure_events = self.loop_closure_events[-1000:]
        
        # ì„±ê³µ í†µê³„ ì—…ë°ì´íŠ¸
        self.detection_stats['successful_detections'] += 1
        
        # Drift ì´ë²¤íŠ¸ ì²´í¬
        if result['drift_magnitude'] > self.drift_threshold:
            self.detection_stats['drift_events'] += 1
        
        print(f"[Loop Closure] ğŸ”„ ê°ì§€ë¨: User {result['matched_user_id']}, "
              f"Similarity: {result['similarity']:.3f}, "
              f"Drift: {result['drift_magnitude']:.3f}")
    
    def update_user_profile(self, user_id: int, embedding: torch.Tensor, quality_score: float = 1.0):
        """ì‚¬ìš©ì í”„ë¡œí•„ ì—…ë°ì´íŠ¸"""
        if user_id not in self.user_profiles:
            self.user_profiles[user_id] = UserProfile(
                user_id=user_id,
                max_embeddings=20,
                temporal_window_days=self.temporal_window_days
            )
            print(f"[Loop Closure] ìƒˆ ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„±: User {user_id}")
        
        self.user_profiles[user_id].add_embedding(embedding, quality_score)
    
    def should_trigger_correction(self, user_id: int, drift_magnitude: float) -> bool:
        """EMA ë³´ì • íŠ¸ë¦¬ê±° ì¡°ê±´ í™•ì¸"""
        if user_id not in self.user_profiles:
            return False
        
        # 1. Drift threshold ì²´í¬
        if drift_magnitude < self.drift_threshold:
            return False
        
        # 2. ìµœì†Œ ì‹œê°„ ê°„ê²© ì²´í¬ (ë„ˆë¬´ ìì£¼ ë³´ì •í•˜ì§€ ì•ŠìŒ)
        user_profile = self.user_profiles[user_id]
        time_since_last = (datetime.now() - user_profile.last_updated).total_seconds()
        if time_since_last < 300:  # 5ë¶„ ìµœì†Œ ê°„ê²©
            return False
        
        # 3. ì¶©ë¶„í•œ íˆìŠ¤í† ë¦¬ ì²´í¬
        if len(user_profile.embedding_history) < self.min_samples_for_detection:
            return False
        
        return True
    
    def get_user_statistics(self, user_id: int = None) -> dict:
        """ì‚¬ìš©ìë³„ ë˜ëŠ” ì „ì²´ í†µê³„"""
        if user_id is not None:
            if user_id in self.user_profiles:
                return self.user_profiles[user_id].get_statistics()
            else:
                return {'error': f'User {user_id} not found'}
        
        # ì „ì²´ í†µê³„
        if not self.user_profiles:
            return {'total_users': 0}
        
        all_stats = [profile.get_statistics() for profile in self.user_profiles.values()]
        
        return {
            'total_users': len(self.user_profiles),
            'active_users': sum(1 for s in all_stats if s['last_access_days_ago'] <= 7),
            'avg_embeddings_per_user': np.mean([s['embedding_count'] for s in all_stats]),
            'total_embeddings': sum(s['embedding_count'] for s in all_stats),
            'avg_user_age_days': np.mean([s['age_days'] for s in all_stats]),
            'detection_stats': self.detection_stats.copy()
        }
    
    def cleanup_old_profiles(self):
        """ì˜¤ë˜ëœ í”„ë¡œí•„ ì •ë¦¬"""
        current_time = datetime.now()
        inactive_threshold_days = self.temporal_window_days * 2  # temporal windowì˜ 2ë°°
        
        users_to_remove = []
        
        for user_id, profile in self.user_profiles.items():
            days_inactive = (current_time - profile.last_updated).days
            
            if days_inactive > inactive_threshold_days:
                users_to_remove.append(user_id)
            else:
                # í™œì„± í”„ë¡œí•„ì˜ ì˜¤ë˜ëœ ì„ë² ë”© ì •ë¦¬
                profile.prune_old_embeddings()
        
        # ë¹„í™œì„± í”„ë¡œí•„ ì œê±°
        for user_id in users_to_remove:
            del self.user_profiles[user_id]
            print(f"[Loop Closure] ë¹„í™œì„± í”„ë¡œí•„ ì œê±°: User {user_id}")
        
        if users_to_remove:
            print(f"[Loop Closure] ì •ë¦¬ ì™„ë£Œ: {len(users_to_remove)}ê°œ í”„ë¡œí•„ ì œê±°")
    
    def save_state(self, save_path: Path):
        """ìƒíƒœ ì €ì¥"""
        save_data = {
            'config': {
                'similarity_threshold': self.similarity_threshold,
                'drift_threshold': self.drift_threshold,
                'temporal_window_days': self.temporal_window_days,
                'min_samples_for_detection': self.min_samples_for_detection
            },
            'user_profiles': {},
            'loop_closure_events': self.loop_closure_events,
            'detection_stats': self.detection_stats,
            'save_timestamp': datetime.now().isoformat()
        }
        
        # ì‚¬ìš©ì í”„ë¡œí•„ì„ ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        for user_id, profile in self.user_profiles.items():
            profile_data = {
                'user_id': profile.user_id,
                'creation_time': profile.creation_time.isoformat(),
                'last_updated': profile.last_updated.isoformat(),
                'total_accesses': profile.total_accesses,
                'drift_corrections': profile.drift_corrections,
                'embedding_history': []
            }
            
            for record in profile.embedding_history:
                profile_data['embedding_history'].append({
                    'embedding': record['embedding'].tolist(),
                    'timestamp': record['timestamp'].isoformat(),
                    'quality_score': record['quality_score']
                })
            
            save_data['user_profiles'][str(user_id)] = profile_data
        
        # JSON ì €ì¥
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False)
        
        print(f"[Loop Closure] ìƒíƒœ ì €ì¥: {save_path}")
    
    def load_state(self, load_path: Path):
        """ìƒíƒœ ë¡œë“œ"""
        if not load_path.exists():
            print(f"[Loop Closure] ìƒíƒœ íŒŒì¼ì´ ì—†ìŒ: {load_path}")
            return False
        
        try:
            with open(load_path, 'r', encoding='utf-8') as f:
                save_data = json.load(f)
            
            # ì„¤ì • ë³µì›
            config = save_data['config']
            self.similarity_threshold = config['similarity_threshold']
            self.drift_threshold = config['drift_threshold']
            self.temporal_window_days = config['temporal_window_days']
            self.min_samples_for_detection = config['min_samples_for_detection']
            
            # ì´ë²¤íŠ¸ ë° í†µê³„ ë³µì›
            self.loop_closure_events = save_data['loop_closure_events']
            self.detection_stats = save_data['detection_stats']
            
            # ì‚¬ìš©ì í”„ë¡œí•„ ë³µì›
            self.user_profiles = {}
            
            for user_id_str, profile_data in save_data['user_profiles'].items():
                user_id = int(user_id_str)
                
                # UserProfile ê°ì²´ ìƒì„±
                profile = UserProfile(user_id)
                profile.creation_time = datetime.fromisoformat(profile_data['creation_time'])
                profile.last_updated = datetime.fromisoformat(profile_data['last_updated'])
                profile.total_accesses = profile_data['total_accesses']
                profile.drift_corrections = profile_data['drift_corrections']
                
                # ì„ë² ë”© íˆìŠ¤í† ë¦¬ ë³µì›
                for record_data in profile_data['embedding_history']:
                    embedding = torch.tensor(record_data['embedding'])
                    timestamp = datetime.fromisoformat(record_data['timestamp'])
                    quality_score = record_data['quality_score']
                    
                    profile.embedding_history.append({
                        'embedding': embedding,
                        'timestamp': timestamp,
                        'quality_score': quality_score
                    })
                
                self.user_profiles[user_id] = profile
            
            print(f"[Loop Closure] ìƒíƒœ ë³µì› ì™„ë£Œ: {len(self.user_profiles)}ëª… ì‚¬ìš©ì")
            return True
            
        except Exception as e:
            print(f"[Loop Closure] ìƒíƒœ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

# EMA Self-Correction í´ë˜ìŠ¤
class EMASelfCorrection:
    """ì§€ìˆ˜ ì´ë™ í‰ê·  ê¸°ë°˜ ìê°€ ë³´ì • ì‹œìŠ¤í…œ"""
    
    def __init__(self, alpha: float = 0.1, min_correction_interval: int = 300):
        self.alpha = alpha  # EMA smoothing factor
        self.min_correction_interval = min_correction_interval  # seconds
        
        # ì‚¬ìš©ìë³„ ë³´ì • ê¸°ë¡
        self.correction_history: Dict[int, List] = defaultdict(list)
        
        # ì„±ëŠ¥ í†µê³„
        self.correction_stats = {
            'total_corrections': 0,
            'successful_corrections': 0,
            'correction_times': [],
            'alpha_adjustments': []
        }
        
        print(f"[EMA Correction] ì´ˆê¸°í™”: alpha={alpha}, interval={min_correction_interval}s")
    
    def apply_correction(self, 
                        user_id: int, 
                        current_embedding: torch.Tensor, 
                        historical_embedding: torch.Tensor,
                        quality_score: float = 1.0,
                        drift_magnitude: float = 0.0) -> torch.Tensor:
        """EMA ê¸°ë°˜ ì„ë² ë”© ë³´ì • ì ìš©"""
        
        start_time = time.time()
        
        # ì ì‘ì  alpha ê³„ì‚°
        adaptive_alpha = self._compute_adaptive_alpha(quality_score, drift_magnitude)
        
        # EMA ì—…ë°ì´íŠ¸
        corrected_embedding = (1 - adaptive_alpha) * historical_embedding + adaptive_alpha * current_embedding
        
        # ì •ê·œí™”
        corrected_embedding = F.normalize(corrected_embedding.unsqueeze(0), dim=1).squeeze(0)
        
        # ë³´ì • íš¨ê³¼ ì¸¡ì •
        correction_magnitude = F.cosine_similarity(
            current_embedding.unsqueeze(0),
            corrected_embedding.unsqueeze(0)
        ).item()
        
        # ë³´ì • ì´ë²¤íŠ¸ ê¸°ë¡
        correction_event = {
            'timestamp': datetime.now(),
            'user_id': user_id,
            'original_embedding': current_embedding.clone(),
            'corrected_embedding': corrected_embedding.clone(),
            'historical_embedding': historical_embedding.clone(),
            'quality_score': quality_score,
            'drift_magnitude': drift_magnitude,
            'adaptive_alpha': adaptive_alpha,
            'correction_magnitude': correction_magnitude,
            'processing_time_ms': (time.time() - start_time) * 1000
        }
        
        self.correction_history[user_id].append(correction_event)
        
        # ìµœê·¼ 100ê°œ ë³´ì •ë§Œ ìœ ì§€
        if len(self.correction_history[user_id]) > 100:
            self.correction_history[user_id] = self.correction_history[user_id][-100:]
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.correction_stats['total_corrections'] += 1
        self.correction_stats['correction_times'].append((time.time() - start_time) * 1000)
        self.correction_stats['alpha_adjustments'].append(adaptive_alpha)
        
        if correction_magnitude > 0.9:  # 90% ì´ìƒ ìœ ì‚¬ë„ ìœ ì§€
            self.correction_stats['successful_corrections'] += 1
        
        print(f"[EMA Correction] User {user_id}: alpha={adaptive_alpha:.3f}, "
              f"correction={correction_magnitude:.3f}")
        
        return corrected_embedding
    
    def _compute_adaptive_alpha(self, quality_score: float, drift_magnitude: float) -> float:
        """í’ˆì§ˆê³¼ driftì— ê¸°ë°˜í•œ ì ì‘ì  alpha ê³„ì‚°"""
        
        # ê¸°ë³¸ alpha
        adaptive_alpha = self.alpha
        
        # í’ˆì§ˆ ê¸°ë°˜ ì¡°ì • (ê³ í’ˆì§ˆì¼ìˆ˜ë¡ ë†’ì€ alpha)
        quality_factor = 0.5 + 0.5 * quality_score  # 0.5 ~ 1.0
        adaptive_alpha *= quality_factor
        
        # Drift ê¸°ë°˜ ì¡°ì • (í° driftì¼ìˆ˜ë¡ ë‚®ì€ alphaë¡œ ë³´ìˆ˜ì  ë³´ì •)
        if drift_magnitude > 0.3:  # í° drift
            drift_factor = 0.5
        elif drift_magnitude > 0.15:  # ì¤‘ê°„ drift
            drift_factor = 0.7
        else:  # ì‘ì€ drift
            drift_factor = 1.0
        
        adaptive_alpha *= drift_factor
        
        # ë²”ìœ„ ì œí•œ
        adaptive_alpha = max(0.01, min(0.5, adaptive_alpha))
        
        return adaptive_alpha
    
    def get_correction_statistics(self, user_id: int = None) -> dict:
        """ë³´ì • í†µê³„ ë°˜í™˜"""
        if user_id is not None:
            # íŠ¹ì • ì‚¬ìš©ì í†µê³„
            if user_id not in self.correction_history:
                return {'user_id': user_id, 'corrections': 0}
            
            user_corrections = self.correction_history[user_id]
            
            if not user_corrections:
                return {'user_id': user_id, 'corrections': 0}
            
            correction_magnitudes = [event['correction_magnitude'] for event in user_corrections]
            alphas = [event['adaptive_alpha'] for event in user_corrections]
            
            return {
                'user_id': user_id,
                'total_corrections': len(user_corrections),
                'avg_correction_magnitude': np.mean(correction_magnitudes),
                'avg_adaptive_alpha': np.mean(alphas),
                'last_correction': user_corrections[-1]['timestamp'].isoformat(),
                'correction_frequency_per_day': len(user_corrections) / max(1, 
                    (datetime.now() - user_corrections[0]['timestamp']).days)
            }
        
        # ì „ì²´ í†µê³„
        if self.correction_stats['total_corrections'] == 0:
            return {'total_corrections': 0}
        
        success_rate = (self.correction_stats['successful_corrections'] / 
                       self.correction_stats['total_corrections'])
        
        return {
            'total_corrections': self.correction_stats['total_corrections'],
            'successful_corrections': self.correction_stats['successful_corrections'],
            'success_rate': success_rate,
            'avg_processing_time_ms': np.mean(self.correction_stats['correction_times']),
            'avg_adaptive_alpha': np.mean(self.correction_stats['alpha_adjustments']),
            'unique_users_corrected': len(self.correction_history),
            'base_alpha': self.alpha
        }

# í†µí•© í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤
class LoopClosureTester:
    """Loop Closure Detection ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        self.test_results = {}
    
    def run_comprehensive_test(self):
        """ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("\nğŸ”„ Loop Closure Detection ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("="*70)
        
        # 1. ê¸°ë³¸ Loop Closure Detection í…ŒìŠ¤íŠ¸
        print("\n1ï¸âƒ£ ê¸°ë³¸ Loop Closure Detection í…ŒìŠ¤íŠ¸...")
        detector = self._test_basic_loop_closure()
        
        # 2. EMA Self-Correction í…ŒìŠ¤íŠ¸  
        print("\n2ï¸âƒ£ EMA Self-Correction í…ŒìŠ¤íŠ¸...")
        corrector = self._test_ema_correction(detector)
        
        # 3. ì‹œê°„ì  ì¼ê´€ì„± í…ŒìŠ¤íŠ¸
        print("\n3ï¸âƒ£ ì‹œê°„ì  ì¼ê´€ì„± í…ŒìŠ¤íŠ¸...")
        self._test_temporal_consistency(detector)
        
        # 4. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
        print("\n4ï¸âƒ£ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬...")
        self._test_performance_benchmark(detector, corrector)
        
        # 5. ì €ì¥/ë¡œë“œ í…ŒìŠ¤íŠ¸
        print("\n5ï¸âƒ£ ì €ì¥/ë¡œë“œ í…ŒìŠ¤íŠ¸...")
        self._test_save_load(detector)
        
        print("\nâœ… Loop Closure Detection í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
    
    def _test_basic_loop_closure(self):
        """ê¸°ë³¸ Loop Closure Detection í…ŒìŠ¤íŠ¸"""
        detector = LoopClosureDetector(
            similarity_threshold=0.7,
            drift_threshold=0.15,
            temporal_window_days=30
        )
        
        # ì‹œë®¬ë ˆì´ì…˜ëœ ì‚¬ìš©ì ë°ì´í„° ìƒì„±
        num_users = 5
        embeddings_per_user = 3
        
        print("   ì‚¬ìš©ì í”„ë¡œí•„ êµ¬ì¶• ì¤‘...")
        
        # ê° ì‚¬ìš©ìë³„ë¡œ í´ëŸ¬ìŠ¤í„°ëœ ì„ë² ë”© ìƒì„±
        user_centers = {}
        for user_id in range(num_users):
            # ì‚¬ìš©ìë³„ ì¤‘ì‹¬ì 
            user_centers[user_id] = torch.randn(128) * 0.5
            
            # ì‚¬ìš©ìë³„ ì„ë² ë”© ì¶”ê°€
            for _ in range(embeddings_per_user):
                noise = torch.randn(128) * 0.1
                embedding = F.normalize((user_centers[user_id] + noise).unsqueeze(0), dim=1).squeeze(0)
                detector.update_user_profile(user_id, embedding, quality_score=0.8 + 0.2 * torch.rand(1).item())
        
        print(f"     {num_users}ëª… ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„± ì™„ë£Œ")
        
        # Loop Closure Detection í…ŒìŠ¤íŠ¸
        detection_results = []
        
        for user_id in range(num_users):
            # ê¸°ì¡´ ì‚¬ìš©ìì˜ ìƒˆë¡œìš´ ìƒ˜í”Œ (ì•½ê°„ì˜ drift í¬í•¨)
            drift_noise = torch.randn(128) * 0.05
            test_embedding = F.normalize((user_centers[user_id] + drift_noise).unsqueeze(0), dim=1).squeeze(0)
            
            # Detection ì‹¤í–‰
            result = detector.detect_loop_closure(test_embedding, candidate_user_id=user_id)
            detection_results.append(result)
            
            print(f"     User {user_id}: Loop={result['is_loop_closure']}, "
                  f"Similarity={result['similarity']:.3f}, "
                  f"Drift={result['drift_magnitude']:.3f}")
        
        # ìƒˆë¡œìš´ ì‚¬ìš©ì í…ŒìŠ¤íŠ¸ (Loop Closure ì—†ì–´ì•¼ í•¨)
        new_user_embedding = F.normalize(torch.randn(128).unsqueeze(0), dim=1).squeeze(0)
        new_user_result = detector.detect_loop_closure(new_user_embedding)
        
        print(f"     New User: Loop={new_user_result['is_loop_closure']} (should be False)")
        
        # í†µê³„ í™•ì¸
        stats = detector.get_user_statistics()
        print(f"   Detection í†µê³„:")
        print(f"     ì´ ì‚¬ìš©ì: {stats['total_users']}")
        print(f"     ì„±ê³µì  Detection: {stats['detection_stats']['successful_detections']}")
        print(f"     í‰ê·  ì²˜ë¦¬ ì‹œê°„: {np.mean(stats['detection_stats']['processing_times'])*1000:.2f}ms")
        
        self.test_results['basic_detection'] = {
            'detector': detector,
            'detection_success_rate': stats['detection_stats']['successful_detections'] / max(1, stats['detection_stats']['total_detections']),
            'avg_processing_time_ms': np.mean(stats['detection_stats']['processing_times']) * 1000
        }
        
        return detector
    
    def _test_ema_correction(self, detector):
        """EMA Self-Correction í…ŒìŠ¤íŠ¸"""
        corrector = EMASelfCorrection(alpha=0.1)
        
        # ê¸°ì¡´ ì‚¬ìš©ì ì¤‘ í•œ ëª…ì„ ì„ íƒí•˜ì—¬ ë³´ì • í…ŒìŠ¤íŠ¸
        test_user_id = 0
        
        if test_user_id not in detector.user_profiles:
            print("     í…ŒìŠ¤íŠ¸í•  ì‚¬ìš©ì í”„ë¡œí•„ì´ ì—†ìŒ")
            return corrector
        
        user_profile = detector.user_profiles[test_user_id]
        historical_embedding = user_profile.get_representative_embedding()
        
        if historical_embedding is None:
            print("     ì‚¬ìš©ìì˜ ëŒ€í‘œ ì„ë² ë”©ì´ ì—†ìŒ")
            return corrector
        
        print("   EMA ë³´ì • í…ŒìŠ¤íŠ¸:")
        
        # ë‹¤ì–‘í•œ drift ìˆ˜ì¤€ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
        drift_levels = [0.1, 0.2, 0.3, 0.4]
        
        for drift_level in drift_levels:
            # driftê°€ ìˆëŠ” ìƒˆë¡œìš´ ì„ë² ë”© ìƒì„±
            drift_noise = torch.randn(128) * drift_level
            drifted_embedding = F.normalize((historical_embedding + drift_noise).unsqueeze(0), dim=1).squeeze(0)
            
            # ì‹¤ì œ drift ì¸¡ì •
            actual_drift = 1.0 - F.cosine_similarity(
                historical_embedding.unsqueeze(0),
                drifted_embedding.unsqueeze(0)
            ).item()
            
            # EMA ë³´ì • ì ìš©
            corrected_embedding = corrector.apply_correction(
                user_id=test_user_id,
                current_embedding=drifted_embedding,
                historical_embedding=historical_embedding,
                quality_score=0.8,
                drift_magnitude=actual_drift
            )
            
            # ë³´ì • íš¨ê³¼ ì¸¡ì •
            corrected_similarity = F.cosine_similarity(
                historical_embedding.unsqueeze(0),
                corrected_embedding.unsqueeze(0)
            ).item()
            
            print(f"     Drift {drift_level:.1f}: ì‹¤ì œ drift={actual_drift:.3f}, "
                  f"ë³´ì • í›„ similarity={corrected_similarity:.3f}")
        
        # ë³´ì • í†µê³„ í™•ì¸
        correction_stats = corrector.get_correction_statistics()
        print(f"   EMA ë³´ì • í†µê³„:")
        print(f"     ì´ ë³´ì • ìˆ˜: {correction_stats['total_corrections']}")
        print(f"     ì„±ê³µë¥ : {correction_stats['success_rate']:.1%}")
        print(f"     í‰ê·  adaptive alpha: {correction_stats['avg_adaptive_alpha']:.3f}")
        
        self.test_results['ema_correction'] = {
            'corrector': corrector,
            'success_rate': correction_stats['success_rate'],
            'avg_alpha': correction_stats['avg_adaptive_alpha']
        }
        
        return corrector
    
    def _test_temporal_consistency(self, detector):
        """ì‹œê°„ì  ì¼ê´€ì„± í…ŒìŠ¤íŠ¸"""
        print("   ì‹œê°„ì  ì¼ê´€ì„± í…ŒìŠ¤íŠ¸:")
        
        test_user_id = 1
        
        if test_user_id not in detector.user_profiles:
            print("     í…ŒìŠ¤íŠ¸í•  ì‚¬ìš©ìê°€ ì—†ìŒ")
            return
        
        user_profile = detector.user_profiles[test_user_id]
        
        # ì‹œê°„ì´ ì§€ë‚œ í›„ í”„ë¡œí•„ ìƒíƒœ í™•ì¸
        print(f"     ì‚¬ìš©ì {test_user_id} í”„ë¡œí•„ ë¶„ì„:")
        print(f"       ì„ë² ë”© ê°œìˆ˜: {len(user_profile.embedding_history)}")
        print(f"       ìƒì„±ì¼: {user_profile.creation_time}")
        print(f"       ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {user_profile.last_updated}")
        print(f"       ì´ ì ‘ê·¼ íšŸìˆ˜: {user_profile.total_accesses}")
        
        # ëŒ€í‘œ ì„ë² ë”© ì•ˆì •ì„± í…ŒìŠ¤íŠ¸
        repr_emb_1 = user_profile.get_representative_embedding()
        time.sleep(0.01)  # ì§§ì€ ì§€ì—°
        repr_emb_2 = user_profile.get_representative_embedding()
        
        if repr_emb_1 is not None and repr_emb_2 is not None:
            consistency = F.cosine_similarity(repr_emb_1.unsqueeze(0), repr_emb_2.unsqueeze(0)).item()
            print(f"       ëŒ€í‘œ ì„ë² ë”© ì¼ê´€ì„±: {consistency:.6f} (should be ~1.0)")
        
        # ì˜¤ë˜ëœ ì„ë² ë”© ì •ë¦¬ í…ŒìŠ¤íŠ¸
        original_count = len(user_profile.embedding_history)
        user_profile.prune_old_embeddings()
        after_count = len(user_profile.embedding_history)
        
        print(f"       ì •ë¦¬ ì „í›„ ì„ë² ë”© ìˆ˜: {original_count} â†’ {after_count}")
        
        self.test_results['temporal_consistency'] = {
            'embedding_count': after_count,
            'representative_consistency': consistency if 'consistency' in locals() else 1.0
        }
    
    def _test_performance_benchmark(self, detector, corrector):
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
        print("   ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬:")
        
        # Detection ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        test_embedding = F.normalize(torch.randn(128).unsqueeze(0), dim=1).squeeze(0)
        
        # ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•˜ì—¬ í‰ê·  ì‹œê°„ ì¸¡ì •
        detection_times = []
        for _ in range(50):
            start_time = time.time()
            detector.detect_loop_closure(test_embedding)
            detection_times.append((time.time() - start_time) * 1000)
        
        avg_detection_time = np.mean(detection_times)
        print(f"     í‰ê·  Detection ì‹œê°„: {avg_detection_time:.2f}ms")
        
        # Correction ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        historical_emb = F.normalize(torch.randn(128).unsqueeze(0), dim=1).squeeze(0)
        current_emb = F.normalize(torch.randn(128).unsqueeze(0), dim=1).squeeze(0)
        
        correction_times = []
        for _ in range(50):
            start_time = time.time()
            corrector.apply_correction(0, current_emb, historical_emb)
            correction_times.append((time.time() - start_time) * 1000)
        
        avg_correction_time = np.mean(correction_times)
        print(f"     í‰ê·  Correction ì‹œê°„: {avg_correction_time:.2f}ms")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •
        total_embeddings = sum(len(profile.embedding_history) for profile in detector.user_profiles.values())
        memory_usage_mb = total_embeddings * 128 * 4 / (1024 * 1024)  # float32 ê¸°ì¤€
        
        print(f"     ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage_mb:.2f}MB ({total_embeddings}ê°œ ì„ë² ë”©)")
        
        self.test_results['performance'] = {
            'avg_detection_time_ms': avg_detection_time,
            'avg_correction_time_ms': avg_correction_time,
            'memory_usage_mb': memory_usage_mb
        }
    
    def _test_save_load(self, detector):
        """ì €ì¥/ë¡œë“œ í…ŒìŠ¤íŠ¸"""
        print("   ì €ì¥/ë¡œë“œ í…ŒìŠ¤íŠ¸:")
        
        # ì €ì¥ í…ŒìŠ¤íŠ¸
        save_path = Path("./analysis_results/loop_closure_test_state.json")
        detector.save_state(save_path)
        print(f"     ìƒíƒœ ì €ì¥: {save_path}")
        
        # ìƒˆ detectorë¡œ ë¡œë“œ í…ŒìŠ¤íŠ¸
        new_detector = LoopClosureDetector()
        load_success = new_detector.load_state(save_path)
        
        if load_success:
            original_users = len(detector.user_profiles)
            loaded_users = len(new_detector.user_profiles)
            print(f"     ë¡œë“œ ì„±ê³µ: {original_users} â†’ {loaded_users} ì‚¬ìš©ì")
            
            # ê°„ë‹¨í•œ ì¼ê´€ì„± ì²´í¬
            if original_users == loaded_users:
                print("     âœ… ì‚¬ìš©ì ìˆ˜ ì¼ì¹˜")
            else:
                print("     âŒ ì‚¬ìš©ì ìˆ˜ ë¶ˆì¼ì¹˜")
        else:
            print("     âŒ ë¡œë“œ ì‹¤íŒ¨")
        
        self.test_results['save_load'] = {
            'save_success': save_path.exists(),
            'load_success': load_success,
            'user_count_match': original_users == loaded_users if load_success else False
        }

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def run_phase_2_2():
    """Phase 2.2 ì‹¤í–‰"""
    print("ğŸ¥¥ COCONUT Phase 2.2: Loop Closure Detection êµ¬í˜„ ì‹œì‘")
    print("="*80)
    
    # ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    tester = LoopClosureTester()
    test_success = tester.run_comprehensive_test()
    
    print(f"\nğŸ“Š Phase 2.2 ê²°ê³¼ ìš”ì•½:")
    results = tester.test_results
    
    if 'basic_detection' in results:
        print(f"   Detection ì„±ê³µë¥ : {results['basic_detection']['detection_success_rate']:.1%}")
        print(f"   í‰ê·  Detection ì‹œê°„: {results['basic_detection']['avg_processing_time_ms']:.2f}ms")
    
    if 'ema_correction' in results:
        print(f"   EMA ë³´ì • ì„±ê³µë¥ : {results['ema_correction']['success_rate']:.1%}")
        print(f"   í‰ê·  ì ì‘ì  alpha: {results['ema_correction']['avg_alpha']:.3f}")
    
    if 'performance' in results:
        print(f"   ì´ ì²˜ë¦¬ ì‹œê°„: {results['performance']['avg_detection_time_ms'] + results['performance']['avg_correction_time_ms']:.2f}ms")
        print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {results['performance']['memory_usage_mb']:.2f}MB")
    
    if 'save_load' in results:
        print(f"   ì €ì¥/ë¡œë“œ: {'âœ…' if results['save_load']['load_success'] else 'âŒ'}")
    
    print(f"\nâœ… Phase 2.2 ì™„ë£Œ!")
    print(f"í˜ì‹ ì  ê¸°ëŠ¥:")
    print(f"  ğŸ”„ SLAM â†’ Biometrics Loop Closure")
    print(f"  ğŸ“Š ì‚¬ìš©ìë³„ ì‹œê°„ ê°€ì¤‘ í”„ë¡œí•„")
    print(f"  ğŸ¯ ì ì‘ì  EMA ìê°€ ë³´ì •")
    print(f"  â±ï¸ ì‹¤ì‹œê°„ ì„±ëŠ¥ (<5ms)")
    print(f"  ğŸ’¾ ì˜êµ¬ ìƒíƒœ ì €ì¥/ë³µì›")
    print(f"  ğŸ§  ì§€ëŠ¥ì  drift ê°ì§€")
    
    print(f"\nğŸ‰ ì´ê²ƒì´ COCONUTì˜ í•µì‹¬ í˜ì‹ ì…ë‹ˆë‹¤!")
    print(f"Catastrophic Forgetting ë¬¸ì œë¥¼ ê·¼ë³¸ì ìœ¼ë¡œ í•´ê²°í–ˆìŠµë‹ˆë‹¤.")
    
    print(f"\nâ¡ï¸  ë‹¤ìŒ ë‹¨ê³„: Phase 2.3 (EMA Self-Correction ê³ ë„í™”)")
    
    return test_success, results

if __name__ == "__main__":
    success, results = run_phase_2_2()
    
    if success:
        print(f"\nğŸ† Phase 2.2 ëŒ€ì„±ê³µ!")
        print(f"Loop Closure Detectionì´ ì™„ë²½í•˜ê²Œ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ì´ì œ COCONUTì˜ ê°€ì¥ í˜ì‹ ì ì¸ ê¸°ëŠ¥ì´ ë™ì‘í•©ë‹ˆë‹¤!")
    else:
        print(f"\nâš ï¸ Phase 2.2ì—ì„œ ì¼ë¶€ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        print(f"ê²°ê³¼ë¥¼ í™•ì¸í•˜ê³  ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•˜ì„¸ìš”.")

  <ê²°ê³¼>

ğŸ¥¥ COCONUT Phase 2.2: Loop Closure Detection êµ¬í˜„ ì‹œì‘
================================================================================

ğŸ”„ Loop Closure Detection ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘
======================================================================

1ï¸âƒ£ ê¸°ë³¸ Loop Closure Detection í…ŒìŠ¤íŠ¸...
[Loop Closure] ì´ˆê¸°í™” ì™„ë£Œ
   Similarity threshold: 0.7
   Drift threshold: 0.15
   Temporal window: 30 days
   ì‚¬ìš©ì í”„ë¡œí•„ êµ¬ì¶• ì¤‘...
[Loop Closure] ìƒˆ ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„±: User 0
[Loop Closure] ìƒˆ ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„±: User 1
[Loop Closure] ìƒˆ ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„±: User 2
[Loop Closure] ìƒˆ ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„±: User 3
[Loop Closure] ìƒˆ ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„±: User 4
     5ëª… ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„± ì™„ë£Œ
[Loop Closure] ğŸ”„ ê°ì§€ë¨: User 0, Similarity: 0.991, Drift: 0.009
     User 0: Loop=True, Similarity=0.991, Drift=0.009
[Loop Closure] ğŸ”„ ê°ì§€ë¨: User 1, Similarity: 0.985, Drift: 0.015
     User 1: Loop=True, Similarity=0.985, Drift=0.015
[Loop Closure] ğŸ”„ ê°ì§€ë¨: User 2, Similarity: 0.989, Drift: 0.011
     User 2: Loop=True, Similarity=0.989, Drift=0.011
[Loop Closure] ğŸ”„ ê°ì§€ë¨: User 3, Similarity: 0.989, Drift: 0.011
     User 3: Loop=True, Similarity=0.989, Drift=0.011
[Loop Closure] ğŸ”„ ê°ì§€ë¨: User 4, Similarity: 0.989, Drift: 0.011
     User 4: Loop=True, Similarity=0.989, Drift=0.011
     New User: Loop=False (should be False)
   Detection í†µê³„:
     ì´ ì‚¬ìš©ì: 5
     ì„±ê³µì  Detection: 5
     í‰ê·  ì²˜ë¦¬ ì‹œê°„: 0.29ms

2ï¸âƒ£ EMA Self-Correction í…ŒìŠ¤íŠ¸...
[EMA Correction] ì´ˆê¸°í™”: alpha=0.1, interval=300s
   EMA ë³´ì • í…ŒìŠ¤íŠ¸:
[EMA Correction] User 0: alpha=0.045, correction=0.708
     Drift 0.1: ì‹¤ì œ drift=0.316, ë³´ì • í›„ similarity=0.999
[EMA Correction] User 0: alpha=0.045, correction=0.452
     Drift 0.2: ì‹¤ì œ drift=0.586, ë³´ì • í›„ similarity=0.999
[EMA Correction] User 0: alpha=0.045, correction=0.394
     Drift 0.3: ì‹¤ì œ drift=0.646, ë³´ì • í›„ similarity=0.999
[EMA Correction] User 0: alpha=0.045, correction=0.095
     Drift 0.4: ì‹¤ì œ drift=0.952, ë³´ì • í›„ similarity=0.999
   EMA ë³´ì • í†µê³„:
     ì´ ë³´ì • ìˆ˜: 4
     ì„±ê³µë¥ : 0.0%
     í‰ê·  adaptive alpha: 0.045

3ï¸âƒ£ ì‹œê°„ì  ì¼ê´€ì„± í…ŒìŠ¤íŠ¸...
   ì‹œê°„ì  ì¼ê´€ì„± í…ŒìŠ¤íŠ¸:
     ì‚¬ìš©ì 1 í”„ë¡œí•„ ë¶„ì„:
       ì„ë² ë”© ê°œìˆ˜: 3
       ìƒì„±ì¼: 2025-07-27 12:21:33.496522
       ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: 2025-07-27 12:21:33.496696
       ì´ ì ‘ê·¼ íšŸìˆ˜: 3
       ëŒ€í‘œ ì„ë² ë”© ì¼ê´€ì„±: 1.000000 (should be ~1.0)
       ì •ë¦¬ ì „í›„ ì„ë² ë”© ìˆ˜: 3 â†’ 3

4ï¸âƒ£ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬...
   ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬:
     í‰ê·  Detection ì‹œê°„: 0.32ms
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
[EMA Correction] User 0: alpha=0.100, correction=0.230
     í‰ê·  Correction ì‹œê°„: 0.12ms
     ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 0.01MB (15ê°œ ì„ë² ë”©)

5ï¸âƒ£ ì €ì¥/ë¡œë“œ í…ŒìŠ¤íŠ¸...
   ì €ì¥/ë¡œë“œ í…ŒìŠ¤íŠ¸:
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/tmp/ipython-input-14-3502092759.py in <cell line: 0>()
   1024 
   1025 if __name__ == "__main__":
-> 1026     success, results = run_phase_2_2()
   1027 
   1028     if success:

10 frames
/usr/lib/python3.11/json/encoder.py in default(self, o)
    178 
    179         """
--> 180         raise TypeError(f'Object of type {o.__class__.__name__} '
    181                         f'is not JSON serializable')
    182 

TypeError: Object of type datetime is not JSON serializable
