# framework/evaluate_authentication.py
"""
End-to-End ì¸ì¦ í‰ê°€ ì‹œìŠ¤í…œ
ì˜¨ë¼ì¸ í•™ìŠµ í›„ í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ ì „ì²´ ì„±ëŠ¥ í‰ê°€
"""

import torch
import numpy as np
from pathlib import Path
import json
import time
from datetime import datetime
from typing import Dict, List, Tuple
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import metrics

from datasets.palm_dataset import MyDataset
from torch.utils.data import DataLoader
from framework.ccnet_style_authentication import CCNetStyleAuthenticator

class EndToEndEvaluator:
    """
    ì˜¨ë¼ì¸ í•™ìŠµ í›„ End-to-End í‰ê°€
    
    1. í…ŒìŠ¤íŠ¸ì…‹ ë¡œë“œ
    2. CCNet ìŠ¤íƒ€ì¼ ì¸ì¦ ìˆ˜í–‰
    3. ìƒì„¸í•œ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
    4. ì‹œê°í™” ë° ë¦¬í¬íŠ¸ ìƒì„±
    """
    
    def __init__(self, model, node_manager, test_file_path: str, device='cuda'):
        """
        Args:
            model: í•™ìŠµëœ CCNet ëª¨ë¸
            node_manager: User Node Manager
            test_file_path: í…ŒìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ (txt)
            device: ì—°ì‚° ë””ë°”ì´ìŠ¤
        """
        self.model = model
        self.node_manager = node_manager
        self.test_file_path = test_file_path
        self.device = device
        
        # CCNet ìŠ¤íƒ€ì¼ ì¸ì¦ê¸°
        self.authenticator = CCNetStyleAuthenticator(node_manager, model, device)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ
        self.test_dataset = MyDataset(txt=test_file_path, train=False)
        
        print(f"[Evaluator] âœ… Initialized")
        print(f"[Evaluator] Test samples: {len(self.test_dataset)}")
        print(f"[Evaluator] Registered users: {len(node_manager.nodes)}")
        
    def run_full_evaluation(self, batch_size: int = 32, 
                          save_results: bool = True,
                          output_dir: str = "./evaluation_results") -> Dict:
        """
        ì „ì²´ í‰ê°€ ì‹¤í–‰
        
        Returns:
            ì¢…í•© í‰ê°€ ê²°ê³¼
        """
        print("\n" + "="*80)
        print("ğŸ” END-TO-END AUTHENTICATION EVALUATION")
        print("="*80)
        
        start_time = time.time()
        
        # 1. ë°ì´í„° ì¤€ë¹„
        print("\n[Step 1/5] Preparing test data...")
        test_samples, test_labels = self._prepare_test_data()
        
        # 2. ì„ê³„ê°’ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ (ì„ íƒì‚¬í•­)
        print("\n[Step 2/5] Calibrating threshold...")
        calibration_result = self._calibrate_threshold(test_samples[:500], test_labels[:500])
        
        # 3. ì „ì²´ í…ŒìŠ¤íŠ¸ì…‹ í‰ê°€
        print("\n[Step 3/5] Evaluating full test set...")
        eval_results = self._evaluate_test_set(test_samples, test_labels)
        
        # 4. ìƒì„¸ ë¶„ì„
        print("\n[Step 4/5] Analyzing results...")
        analysis_results = self._analyze_results(eval_results)
        
        # 5. ê²°ê³¼ ì €ì¥ ë° ì‹œê°í™”
        if save_results:
            print("\n[Step 5/5] Saving results and visualizations...")
            self._save_results(eval_results, analysis_results, output_dir)
        
        total_time = time.time() - start_time
        
        # ì¢…í•© ê²°ê³¼
        summary = {
            'test_samples': len(test_samples),
            'registered_users': len(self.node_manager.nodes),
            'accuracy': analysis_results['accuracy'],
            'eer': analysis_results['eer'],
            'rank1_accuracy': analysis_results['rank_accuracies']['rank_1'],
            'far': analysis_results['far'],
            'frr': analysis_results['frr'],
            'avg_verification_time_ms': analysis_results['avg_time_ms'],
            'total_evaluation_time': total_time,
            'calibration_result': calibration_result,
            'threshold_used': self.authenticator.distance_threshold
        }
        
        # ê²°ê³¼ ì¶œë ¥
        self._print_summary(summary)
        
        return summary
    
    def _prepare_test_data(self) -> Tuple[List[torch.Tensor], List[int]]:
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„"""
        test_samples = []
        test_labels = []
        
        # ë°ì´í„°ì…‹ì—ì„œ ìƒ˜í”Œ ì¶”ì¶œ
        for idx in tqdm(range(len(self.test_dataset)), desc="Loading test data"):
            data, label = self.test_dataset[idx]
            # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë§Œ ì‚¬ìš© (CCNetì€ pairë¡œ ë¡œë“œí•˜ë¯€ë¡œ)
            test_samples.append(data[0])
            test_labels.append(label if isinstance(label, int) else label.item())
        
        print(f"  Loaded {len(test_samples)} test samples")
        print(f"  Unique users in test set: {len(set(test_labels))}")
        
        return test_samples, test_labels
    
    def _calibrate_threshold(self, samples: List[torch.Tensor], 
                           labels: List[int]) -> Dict:
        """ì„ê³„ê°’ ìë™ ì¡°ì •"""
        calibration_data = list(zip(samples, labels))
        result = self.authenticator.calibrate_threshold(
            calibration_data, 
            target_far=0.01  # 1% FAR
        )
        return result
    
    def _evaluate_test_set(self, test_samples: List[torch.Tensor], 
                          test_labels: List[int]) -> Dict:
        """ì „ì²´ í…ŒìŠ¤íŠ¸ì…‹ í‰ê°€"""
        all_results = []
        registered_users = set(self.node_manager.nodes.keys())
        
        # ì§„í–‰ë¥  í‘œì‹œí•˜ë©° í‰ê°€
        for sample, true_label in tqdm(zip(test_samples, test_labels), 
                                     total=len(test_samples),
                                     desc="Evaluating"):
            # ì¸ì¦ ìˆ˜í–‰
            auth_result = self.authenticator.verify_user(sample, top_k=10)
            
            # ê²°ê³¼ ì €ì¥
            result_entry = {
                'true_label': true_label,
                'is_registered': true_label in registered_users,
                'prediction': auth_result['matched_user'] if auth_result['is_match'] else None,
                'is_match': auth_result['is_match'],
                'distance': auth_result['distance'],
                'confidence': auth_result['confidence'],
                'top_5_results': auth_result['top_k_results'][:5],
                'computation_time': auth_result['computation_time']
            }
            all_results.append(result_entry)
        
        return all_results
    
    def _analyze_results(self, eval_results: List[Dict]) -> Dict:
        """ê²°ê³¼ ìƒì„¸ ë¶„ì„"""
        # ê¸°ë³¸ í†µê³„
        total = len(eval_results)
        correct = 0
        false_accepts = 0
        false_rejects = 0
        true_rejects = 0
        
        # ê±°ë¦¬ ë¶„í¬
        genuine_distances = []
        imposter_distances = []
        
        # Rank ì •í™•ë„ ê³„ì‚°ìš©
        rank_correct = {r: 0 for r in range(1, 6)}
        
        # ì‹œê°„ í†µê³„
        computation_times = []
        
        for result in eval_results:
            computation_times.append(result['computation_time'])
            
            if result['is_registered']:
                # ë“±ë¡ëœ ì‚¬ìš©ì
                if result['is_match'] and result['prediction'] == result['true_label']:
                    correct += 1
                    genuine_distances.append(result['distance'])
                elif result['is_match'] and result['prediction'] != result['true_label']:
                    false_accepts += 1
                    imposter_distances.append(result['distance'])
                else:  # not is_match
                    false_rejects += 1
                    genuine_distances.append(result['distance'])
                
                # Rank ì •í™•ë„
                for rank, (user_id, _) in enumerate(result['top_5_results'], 1):
                    if user_id == result['true_label']:
                        for r in range(rank, 6):
                            rank_correct[r] += 1
                        break
            else:
                # ë¯¸ë“±ë¡ ì‚¬ìš©ì
                if not result['is_match']:
                    correct += 1
                    true_rejects += 1
                    imposter_distances.append(result['distance'])
                else:
                    false_accepts += 1
                    imposter_distances.append(result['distance'])
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        accuracy = (correct / total) * 100
        
        # ë“±ë¡ëœ ì‚¬ìš©ìë§Œ ê³ ë ¤í•œ ë©”íŠ¸ë¦­
        registered_count = sum(1 for r in eval_results if r['is_registered'])
        if registered_count > 0:
            far = (false_accepts / total) * 100
            frr = (false_rejects / registered_count) * 100
        else:
            far = frr = 0
        
        # EER ê³„ì‚°
        if genuine_distances and imposter_distances:
            eer, eer_threshold = self._calculate_eer(genuine_distances, imposter_distances)
        else:
            eer, eer_threshold = 0, 0
        
        # Rank ì •í™•ë„
        rank_accuracies = {}
        if registered_count > 0:
            for rank in range(1, 6):
                rank_accuracies[f'rank_{rank}'] = (rank_correct[rank] / registered_count) * 100
        
        # ì‹œê°„ í†µê³„
        avg_time_ms = np.mean(computation_times) * 1000
        
        return {
            'total_samples': total,
            'registered_samples': registered_count,
            'correct': correct,
            'accuracy': accuracy,
            'false_accepts': false_accepts,
            'false_rejects': false_rejects,
            'true_rejects': true_rejects,
            'far': far,
            'frr': frr,
            'eer': eer,
            'eer_threshold': eer_threshold,
            'genuine_distances': genuine_distances,
            'imposter_distances': imposter_distances,
            'rank_accuracies': rank_accuracies,
            'avg_time_ms': avg_time_ms,
            'min_time_ms': np.min(computation_times) * 1000,
            'max_time_ms': np.max(computation_times) * 1000
        }
    
    def _calculate_eer(self, genuine_distances: List[float], 
                      imposter_distances: List[float]) -> Tuple[float, float]:
        """EER ê³„ì‚°"""
        # ë¼ë²¨ ìƒì„± (genuine=1, imposter=0)
        labels = [1] * len(genuine_distances) + [0] * len(imposter_distances)
        scores = genuine_distances + imposter_distances
        
        # ê±°ë¦¬ë¥¼ ìŒìˆ˜ë¡œ ë³€í™˜ (ë‚®ì€ ê±°ë¦¬ = ë†’ì€ ì ìˆ˜)
        scores = [-s for s in scores]
        
        # ROC ì»¤ë¸Œ
        fpr, tpr, thresholds = metrics.roc_curve(labels, scores, pos_label=1)
        
        # EER ì§€ì  ì°¾ê¸°
        fnr = 1 - tpr
        eer_idx = np.argmin(np.abs(fpr - fnr))
        eer = (fpr[eer_idx] + fnr[eer_idx]) / 2
        eer_threshold = -thresholds[eer_idx]  # ë‹¤ì‹œ ì–‘ìˆ˜ë¡œ
        
        return eer * 100, eer_threshold
    
    def _save_results(self, eval_results: List[Dict], 
                     analysis: Dict, output_dir: str):
        """ê²°ê³¼ ì €ì¥ ë° ì‹œê°í™”"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # 1. JSON ê²°ê³¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        summary_data = {
            'timestamp': timestamp,
            'test_file': self.test_file_path,
            'analysis': analysis,
            'threshold_used': self.authenticator.distance_threshold,
            'node_manager_stats': self.node_manager.get_statistics()
        }
        
        with open(output_path / f'evaluation_summary_{timestamp}.json', 'w') as f:
            json.dump(summary_data, f, indent=2)
        
        # 2. ìƒì„¸ ê²°ê³¼ ì €ì¥ (ì˜µì…˜)
        # with open(output_path / f'detailed_results_{timestamp}.json', 'w') as f:
        #     json.dump(eval_results, f, indent=2)
        
        # 3. ì‹œê°í™”
        self._create_visualizations(analysis, output_path, timestamp)
        
        print(f"  âœ… Results saved to: {output_path}")
    
    def _create_visualizations(self, analysis: Dict, 
                              output_path: Path, timestamp: str):
        """ê²°ê³¼ ì‹œê°í™”"""
        plt.style.use('seaborn-v0_8-darkgrid')
        
        # 1. ê±°ë¦¬ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
        plt.figure(figsize=(12, 6))
        
        if analysis['genuine_distances']:
            plt.hist(analysis['genuine_distances'], bins=50, alpha=0.7, 
                    label=f'Genuine (n={len(analysis["genuine_distances"])})', 
                    color='green', density=True)
        
        if analysis['imposter_distances']:
            plt.hist(analysis['imposter_distances'], bins=50, alpha=0.7,
                    label=f'Imposter (n={len(analysis["imposter_distances"])})', 
                    color='red', density=True)
        
        plt.axvline(self.authenticator.distance_threshold, color='blue', 
                   linestyle='--', linewidth=2,
                   label=f'Threshold = {self.authenticator.distance_threshold:.3f}')
        
        plt.xlabel('Cosine Distance (arccos/Ï€)')
        plt.ylabel('Density')
        plt.title('Distance Distribution')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(output_path / f'distance_distribution_{timestamp}.png', dpi=300)
        plt.close()
        
        # 2. ROC ì»¤ë¸Œ
        if analysis['genuine_distances'] and analysis['imposter_distances']:
            plt.figure(figsize=(8, 8))
            
            # ROC ê³„ì‚°
            labels = ([1] * len(analysis['genuine_distances']) + 
                     [0] * len(analysis['imposter_distances']))
            scores = ([-d for d in analysis['genuine_distances']] + 
                     [-d for d in analysis['imposter_distances']])
            
            fpr, tpr, _ = metrics.roc_curve(labels, scores, pos_label=1)
            auc_score = metrics.auc(fpr, tpr)
            
            plt.plot(fpr * 100, tpr * 100, 'b-', linewidth=2, 
                    label=f'ROC (AUC = {auc_score:.4f})')
            plt.plot([0, 100], [0, 100], 'k--', alpha=0.5, label='Random')
            
            # EER ì  í‘œì‹œ
            eer_point = analysis['eer']
            plt.plot(eer_point, 100 - eer_point, 'ro', markersize=10, 
                    label=f'EER = {eer_point:.2f}%')
            
            plt.xlabel('False Acceptance Rate (%)')
            plt.ylabel('Genuine Acceptance Rate (%)')
            plt.title('ROC Curve')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.xlim([0, 20])  # ê´€ì‹¬ ì˜ì—­ í™•ëŒ€
            plt.ylim([80, 100])
            plt.tight_layout()
            plt.savefig(output_path / f'roc_curve_{timestamp}.png', dpi=300)
            plt.close()
        
        # 3. Rank ì •í™•ë„ ë§‰ëŒ€ ê·¸ë˜í”„
        if analysis['rank_accuracies']:
            plt.figure(figsize=(10, 6))
            
            ranks = list(range(1, 6))
            accuracies = [analysis['rank_accuracies'][f'rank_{r}'] for r in ranks]
            
            bars = plt.bar(ranks, accuracies, color='skyblue', edgecolor='navy')
            
            # ê°’ í‘œì‹œ
            for bar, acc in zip(bars, accuracies):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                        f'{acc:.1f}%', ha='center', va='bottom')
            
            plt.xlabel('Rank')
            plt.ylabel('Accuracy (%)')
            plt.title('Rank-N Accuracy')
            plt.ylim([0, 105])
            plt.grid(True, axis='y', alpha=0.3)
            plt.tight_layout()
            plt.savefig(output_path / f'rank_accuracy_{timestamp}.png', dpi=300)
            plt.close()
    
    def _print_summary(self, summary: Dict):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ“Š EVALUATION SUMMARY")
        print("="*80)
        
        print(f"\nğŸ” Dataset Information:")
        print(f"  - Test samples: {summary['test_samples']}")
        print(f"  - Registered users: {summary['registered_users']}")
        print(f"  - Distance threshold: {summary['threshold_used']:.4f}")
        
        print(f"\nğŸ“ˆ Performance Metrics:")
        print(f"  - Overall Accuracy: {summary['accuracy']:.2f}%")
        print(f"  - Rank-1 Accuracy: {summary['rank1_accuracy']:.2f}%")
        print(f"  - EER: {summary['eer']:.2f}%")
        print(f"  - FAR: {summary['far']:.2f}%")
        print(f"  - FRR: {summary['frr']:.2f}%")
        
        print(f"\nâš¡ Speed Performance:")
        print(f"  - Avg verification time: {summary['avg_verification_time_ms']:.2f} ms")
        print(f"  - Total evaluation time: {summary['total_evaluation_time']:.1f} seconds")
        
        print("\n" + "="*80)


def run_end_to_end_evaluation(model, node_manager, config):
    """
    ì˜¨ë¼ì¸ í•™ìŠµ í›„ ì‹¤í–‰í•  End-to-End í‰ê°€
    
    Args:
        model: í•™ìŠµëœ ëª¨ë¸
        node_manager: User Node Manager
        config: ì„¤ì • ê°ì²´
    """
    # í…ŒìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
    test_file = getattr(config.dataset, 'test_set_file', None)
    if not test_file:
        print("âš ï¸ No test file specified in config")
        return None
    
    # í‰ê°€ê¸° ìƒì„±
    evaluator = EndToEndEvaluator(
        model=model,
        node_manager=node_manager,
        test_file_path=test_file,
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )
    
    # ì „ì²´ í‰ê°€ ì‹¤í–‰
    results = evaluator.run_full_evaluation(
        batch_size=32,
        save_results=True,
        output_dir="./evaluation_results"
    )
    
    return results


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    print("End-to-End Evaluation Module")
    print("This module evaluates authentication performance after online learning")