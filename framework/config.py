# framework/config.py - ë°°ì¹˜ êµ¬ì„± ì œì–´ ì™„ì „ ë²„ì „

"""
CoCoNut Framework Configuration Classes

DESIGN PHILOSOPHY:
- Controlled batch composition for optimal continual learning
- Separate batch sizes for pretraining vs continual learning
- Precise positive/hard sample ratios
- Extensible sampling strategies
"""

import dataclasses
from pathlib import Path
from typing import Optional, List

# === ì—°ì†í•™ìŠµ ì‹¤í—˜ìš© ì„¤ì •ë“¤ ===
@dataclasses.dataclass
class ContinualLearnerConfig:
    """ì—°ì† í•™ìŠµê¸° ì„¤ì • (ğŸ”¥ ë°°ì¹˜ êµ¬ì„± ì œì–´ ì¶”ê°€)"""
    config_file: Path
    adaptation: bool
    adaptation_epochs: int
    sync_frequency: int
    replay_weight: float
    new_data_weight: float
    
    # ê¸°ë³¸ í•™ìŠµ ì„¤ì •
    intermediate_save_frequency: Optional[int] = 100
    learning_rate: Optional[float] = 0.001
    
    # ğŸ”¥ NEW: ì—°ì†í•™ìŠµ ì „ìš© ë°°ì¹˜ ì‚¬ì´ì¦ˆ (PalmRecognizerì™€ ë¶„ë¦¬)
    continual_batch_size: Optional[int] = 10     # ì—°ì†í•™ìŠµ ë°°ì¹˜ ì‚¬ì´ì¦ˆ
    batch_size: Optional[int] = None             # í˜¸í™˜ì„± ìœ ì§€ (deprecated)
    
    # ğŸ”¥ NEW: ë°°ì¹˜ êµ¬ì„± ë¹„ìœ¨ ì œì–´
    target_positive_ratio: Optional[float] = 0.3      # 30% positive pairs
    hard_mining_ratio: Optional[float] = 0.3          # 30% hard samples  
    enable_hard_mining: Optional[bool] = True         # Hard mining í™œì„±í™”
    
    # í˜¸í™˜ì„± ìœ ì§€
    hard_mining_ratio_legacy: Optional[float] = None  # ê¸°ì¡´ ì´ë¦„ ì§€ì›
    
    def __post_init__(self):
        """ì„¤ì • ê²€ì¦ ë° í˜¸í™˜ì„± ì²˜ë¦¬"""
        # 1. í˜¸í™˜ì„± ì²˜ë¦¬: batch_size â†’ continual_batch_size
        if self.batch_size is not None and self.continual_batch_size == 10:
            self.continual_batch_size = self.batch_size
            print(f"[Config] Using legacy batch_size: {self.continual_batch_size}")
        
        # 2. í˜¸í™˜ì„± ì²˜ë¦¬: hard_mining_ratio_legacy
        if self.hard_mining_ratio_legacy is not None:
            self.hard_mining_ratio = self.hard_mining_ratio_legacy
            print(f"[Config] Using legacy hard_mining_ratio: {self.hard_mining_ratio}")
        
        # 3. ë¹„ìœ¨ ê²€ì¦
        if not (0.0 <= self.target_positive_ratio <= 1.0):
            raise ValueError(f"target_positive_ratio must be 0.0-1.0, got {self.target_positive_ratio}")
        
        if not (0.0 <= self.hard_mining_ratio <= 1.0):
            raise ValueError(f"hard_mining_ratio must be 0.0-1.0, got {self.hard_mining_ratio}")
        
        # 4. ë°°ì¹˜ êµ¬ì„± ê²€ì¦
        # Positive pairsëŠ” 2ê°œì”©ì´ë¯€ë¡œ, ì‹¤ì œ positive sample ìˆ˜ëŠ” ratio * 2
        max_positive_samples = self.target_positive_ratio * self.continual_batch_size
        max_hard_samples = self.hard_mining_ratio * self.continual_batch_size
        total_reserved = max_positive_samples + max_hard_samples
        
        if total_reserved > self.continual_batch_size:
            print(f"[Config] Warning: positive + hard samples ({total_reserved:.1f}) > batch_size ({self.continual_batch_size})")
            print(f"[Config] Some overlap may occur, adjusting ratios...")
            
            # ìë™ ì¡°ì •
            adjustment_factor = self.continual_batch_size / total_reserved * 0.9
            self.target_positive_ratio *= adjustment_factor
            self.hard_mining_ratio *= adjustment_factor
            
            print(f"[Config] Adjusted ratios: positive={self.target_positive_ratio:.2f}, hard={self.hard_mining_ratio:.2f}")
        
        # 5. ìµœì¢… ê³„íš ì¶œë ¥
        planned_positive = int(self.continual_batch_size * self.target_positive_ratio)
        planned_hard = int(self.continual_batch_size * self.hard_mining_ratio)
        planned_regular = self.continual_batch_size - planned_positive - planned_hard
        
        print(f"[Config] ğŸ¯ Continual Learning Batch Plan (size: {self.continual_batch_size}):")
        print(f"   Positive samples: {planned_positive} ({self.target_positive_ratio:.1%})")
        print(f"   Hard samples: {planned_hard} ({self.hard_mining_ratio:.1%})")
        print(f"   Regular samples: {planned_regular} ({planned_regular/self.continual_batch_size:.1%})")

@dataclasses.dataclass
class ReplayBufferConfig:
    """ì§€ëŠ¥í˜• ë¦¬í”Œë ˆì´ ë²„í¼ ì„¤ì • (ğŸ”¥ ìƒ˜í”Œë§ ì „ëµ ì œì–´ ì¶”ê°€)"""
    config_file: Path
    maximize_diversity: bool
    max_buffer_size: int
    similarity_threshold: float
    storage_path: str
    feature_extraction_for_diversity: bool
    
    # ê¸°ì¡´ ì„¤ì •
    enable_smart_sampling: Optional[bool] = True
    diversity_update_frequency: Optional[int] = 10
    model_save_path: Optional[str] = "./results/models/"
    
    # ğŸ”¥ NEW: ìƒ˜í”Œë§ ì „ëµ ì œì–´
    sampling_strategy: Optional[str] = "controlled"    # "controlled", "balanced", "original"
    force_positive_pairs: Optional[bool] = True        # í•­ìƒ positive pairs ë³´ì¥
    min_positive_pairs: Optional[int] = 1              # ìµœì†Œ positive pair ìˆ˜  
    max_positive_ratio: Optional[float] = 0.5          # ìµœëŒ€ positive ratio ì œí•œ
    
    def __post_init__(self):
        """ìƒ˜í”Œë§ ì „ëµ ê²€ì¦"""
        valid_strategies = ["controlled", "balanced", "original"]
        if self.sampling_strategy not in valid_strategies:
            raise ValueError(f"sampling_strategy must be one of {valid_strategies}, got {self.sampling_strategy}")
        
        if not (0.0 <= self.max_positive_ratio <= 1.0):
            raise ValueError(f"max_positive_ratio must be 0.0-1.0, got {self.max_positive_ratio}")
        
        print(f"[Config] ğŸ¯ Replay Buffer Sampling:")
        print(f"   Strategy: {self.sampling_strategy}")
        print(f"   Force positive pairs: {self.force_positive_pairs}")
        print(f"   Min positive pairs: {self.min_positive_pairs}")
        print(f"   Max positive ratio: {self.max_positive_ratio:.1%}")

@dataclasses.dataclass  
class LossConfig:
    """ì†ì‹¤ í•¨ìˆ˜ ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)"""
    config_file: Path
    temp: float
    type: Optional[str] = "SupConLoss"
    # ì‚¬ì „ í›ˆë ¨ìš© (í•˜ì´ë¸Œë¦¬ë“œ ì†ì‹¤)
    weight1: Optional[float] = 0.8  # ArcFace ê°€ì¤‘ì¹˜
    weight2: Optional[float] = 0.2  # SupCon ê°€ì¤‘ì¹˜

@dataclasses.dataclass
class ModelSavingConfig:
    """ëª¨ë¸ ì €ì¥ ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)"""
    config_file: Path
    final_save_path: str = "/content/drive/MyDrive/CoCoNut_STAR"
    intermediate_save_frequency: int = 100
    enable_intermediate_save: bool = True
    include_timestamp: bool = True
    auto_generate_readme: bool = True

@dataclasses.dataclass
class DataAugmentationConfig:
    """ë°ì´í„° ì¦ê°• ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)"""
    config_file: Path
    enable_augmentation: bool = True
    augmentation_probability: float = 0.4
    enable_geometric: bool = True
    geometric_probability: float = 0.3
    max_rotation_degrees: int = 3
    max_translation_ratio: float = 0.05
    enable_resolution_adaptation: bool = True
    resolution_probability: float = 0.3
    intermediate_resolutions: Optional[List] = None
    resize_methods: Optional[List] = None
    enable_noise: bool = True
    noise_probability: float = 0.3
    noise_std_range: Optional[List] = None
    
    def __post_init__(self):
        """ê¸°ë³¸ê°’ ì„¤ì •"""
        if self.intermediate_resolutions is None:
            self.intermediate_resolutions = [[64, 64], [96, 96], [160, 160]]
        if self.resize_methods is None:
            self.resize_methods = ["nearest", "bilinear", "bicubic", "lanczos"]
        if self.noise_std_range is None:
            self.noise_std_range = [0.01, 0.03]

# === ì‚¬ì „ í›ˆë ¨ìš© ì„¤ì •ë“¤ (ê¸°ì¡´ê³¼ ë™ì¼) ===
@dataclasses.dataclass
class TrainingConfig:
    """ì‚¬ì „ í›ˆë ¨ìš© Training ì„¤ì •"""
    config_file: Path
    batch_size: int
    epoch_num: int
    lr: float
    redstep: int
    gpu_id: int

@dataclasses.dataclass  
class PathsConfig:
    """ì‚¬ì „ í›ˆë ¨ìš© ê²½ë¡œ ì„¤ì •"""
    config_file: Path
    checkpoint_path: str
    results_path: str
    save_interval: int
    test_interval: int