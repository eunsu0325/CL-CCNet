# framework/replay_buffer.py - ì™„ì „ ì œì–´ëœ ë°°ì¹˜ êµ¬ì„± ë²„ì „

"""
CoCoNut Intelligent Replay Buffer with Controlled Batch Composition

CORE FEATURES:
- ğŸ¯ Precise positive/hard sample ratios (e.g., 30%/30%)  
- ğŸ’ª Real hard sample mining with similarity-based selection
- ğŸ¨ Differentiated augmentation for artificial positive pairs
- ğŸ“Š Comprehensive batch composition reporting
- ğŸ”§ Config-driven ratio control
"""

import os
import pickle
import random
from pathlib import Path
from typing import List, Tuple, Dict, Optional

import faiss
import numpy as np
import torch
import torchvision.transforms as transforms
from PIL import Image

class CoconutReplayBuffer:
    def __init__(self, config, storage_dir: Path, feature_dimension: int = 2048):
        """
        ì§€ëŠ¥í˜• ë¦¬í”Œë ˆì´ ë²„í¼ ì´ˆê¸°í™” (ì™„ì „ ì œì–´ëœ ë°°ì¹˜ êµ¬ì„±)
        """
        self.config = config
        self.storage_dir = storage_dir
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        
        # ê¸°ë³¸ ì„¤ì •
        self.buffer_size = self.config.max_buffer_size
        self.similarity_threshold = self.config.similarity_threshold
        self.feature_dimension = feature_dimension
        
        # ğŸ”¥ ìƒ˜í”Œë§ ì „ëµ ì„¤ì •
        self.sampling_strategy = getattr(self.config, 'sampling_strategy', 'controlled')
        self.force_positive_pairs = getattr(self.config, 'force_positive_pairs', True)
        self.min_positive_pairs = getattr(self.config, 'min_positive_pairs', 1)
        self.max_positive_ratio = getattr(self.config, 'max_positive_ratio', 0.5)
        
        # Faiss ì¸ë±ìŠ¤ ë° ì €ì¥ì†Œ
        self.image_storage = []
        self.faiss_index = None
        self.stored_embeddings = []
        self.metadata = {}
        self.feature_extractor = None

        # ë°°ì¹˜ êµ¬ì„± ë¹„ìœ¨ (ë‚˜ì¤‘ì— CoconutSystemì—ì„œ ì„¤ì •)
        self.target_positive_ratio = 0.3  # ê¸°ë³¸ê°’
        self.hard_ratio = 0.3             # ê¸°ë³¸ê°’
        self.enable_hard_mining = False   # ê¸°ë³¸ê°’
        
        # ë°ì´í„° ì¦ê°• ì„¤ì • (ë‚˜ì¤‘ì— ì„¤ì •)
        self.enable_augmentation = False
        self.aug_config = None
        self._setup_augmentation_transforms()
        
        # ìƒíƒœ íŒŒì¼ ê²½ë¡œ
        self.state_file = self.storage_dir / 'buffer_state.pkl'
        self._load_state()

        print(f"[Buffer] ğŸ¥¥ CoCoNut Controlled Batch Replay Buffer initialized")
        print(f"[Buffer] Strategy: {self.sampling_strategy}")
        print(f"[Buffer] Max buffer size: {self.buffer_size}")
        print(f"[Buffer] Current size: {len(self.image_storage)}")

    def update_batch_composition_config(self, target_positive_ratio: float, hard_mining_ratio: float):
        """ğŸ”¥ ë°°ì¹˜ êµ¬ì„± ë¹„ìœ¨ ì—…ë°ì´íŠ¸ (CoconutSystemì—ì„œ í˜¸ì¶œ)"""
        self.target_positive_ratio = target_positive_ratio
        self.hard_ratio = hard_mining_ratio
        print(f"[Buffer] ğŸ¯ Batch composition config updated:")
        print(f"   Target positive ratio: {self.target_positive_ratio:.1%}")
        print(f"   Hard mining ratio: {self.hard_ratio:.1%}")

    def update_hard_mining_config(self, enable_hard_mining: bool, hard_ratio: float):
        """Hard Mining ì„¤ì • ì—…ë°ì´íŠ¸"""
        self.enable_hard_mining = enable_hard_mining
        self.hard_ratio = hard_ratio
        print(f"[Buffer] ğŸ”¥ Hard Mining updated: {self.enable_hard_mining} (ratio: {self.hard_ratio:.1%})")

    def update_augmentation_config(self, enable_augmentation: bool, aug_config):
        """ë°ì´í„° ì¦ê°• ì„¤ì • ì—…ë°ì´íŠ¸"""
        self.enable_augmentation = enable_augmentation
        self.aug_config = aug_config
        self._setup_augmentation_transforms()
        print(f"[Buffer] ğŸ¨ Augmentation updated: {self.enable_augmentation}")

    def set_feature_extractor(self, model):
        """íŠ¹ì§• ì¶”ì¶œì„ ìœ„í•œ ëª¨ë¸ ì„¤ì •"""
        self.feature_extractor = model

    def _initialize_faiss(self):
        """Faiss ì¸ë±ìŠ¤ ì´ˆê¸°í™”"""
        index = faiss.IndexFlatIP(self.feature_dimension)
        self.faiss_index = faiss.IndexIDMap(index)
        print(f"[Buffer] Faiss index initialized with dimension {self.feature_dimension}")

    def _extract_feature_for_diversity(self, image):
        """ë‹¤ì–‘ì„± ì¸¡ì •ì„ ìœ„í•œ íŠ¹ì§• ë²¡í„° ì¶”ì¶œ"""
        if self.feature_extractor is None:
            raise ValueError("Feature extractor not set")
        
        if len(image.shape) == 3:
            image = image.unsqueeze(0)
        
        with torch.no_grad():
            features = self.feature_extractor.getFeatureCode(image)
        
        return features

    def add(self, image: torch.Tensor, user_id: int):
        """ìƒˆë¡œìš´ ê²½í—˜ì„ ë²„í¼ì— ì¶”ê°€ (ë‹¤ì–‘ì„± ê¸°ë°˜, ê¸°ì¡´ê³¼ ë™ì¼)"""
        with torch.no_grad():
            embedding = self._extract_feature_for_diversity(image)
            embedding_np = embedding.cpu().numpy().astype('float32')
            faiss.normalize_L2(embedding_np)

        if self.faiss_index is None:
            self._initialize_faiss()

        # ë‹¤ì–‘ì„± í™•ì¸
        if self.faiss_index.ntotal == 0:
            max_similarity = 0.0
        else:
            distances, _ = self.faiss_index.search(embedding_np, k=1)
            max_similarity = distances[0][0]

        # ìƒˆë¡œìš´ ê²½í—˜ì´ë©´ ì €ì¥
        if max_similarity < self.similarity_threshold:
            if len(self.image_storage) >= self.buffer_size:
                self._cull()
            
            unique_id = len(self.image_storage)
            self.image_storage.append({
                'image': image.cpu().clone(),
                'user_id': user_id,
                'id': unique_id
            })
            
            self.stored_embeddings.append(embedding_np.copy())
            self.faiss_index.add_with_ids(embedding_np, np.array([unique_id]))
            self.metadata[unique_id] = {'user_id': user_id}
            
            print(f"[Buffer] âœ… Added diverse sample (ID: {unique_id}, User: {user_id}). "
                  f"Buffer: {len(self.image_storage)}/{self.buffer_size}")
        else:
            print(f"[Buffer] âš ï¸ Similar sample skipped (similarity: {max_similarity:.4f} >= {self.similarity_threshold})")

    def _cull(self):
        """ê°€ì¥ ì¤‘ë³µë˜ëŠ” ë°ì´í„° ì œê±° (ê¸°ì¡´ê³¼ ë™ì¼)"""
        if self.faiss_index.ntotal < 2:
            return

        print(f"[Buffer] ğŸ”„ Buffer full. Finding most redundant sample...")
        
        if len(self.stored_embeddings) == 0:
            return
        
        all_vectors = np.vstack(self.stored_embeddings)
        k = min(self.faiss_index.ntotal, 50)
        similarities, _ = self.faiss_index.search(all_vectors, k=k)
        
        diversity_scores = similarities.sum(axis=1) - 1.0
        cull_idx_in_storage = np.argmax(diversity_scores)
        cull_unique_id = self.image_storage[cull_idx_in_storage]['id']

        try:
            self.faiss_index.remove_ids(np.array([cull_unique_id]))
        except Exception:
            self._rebuild_faiss_index_after_removal(cull_idx_in_storage)
        
        if cull_unique_id in self.metadata:
            del self.metadata[cull_unique_id]
        
        del self.image_storage[cull_idx_in_storage]
        del self.stored_embeddings[cull_idx_in_storage]
        
        print(f"[Buffer] ğŸ—‘ï¸ Removed redundant sample (ID: {cull_unique_id})")

    def _rebuild_faiss_index_after_removal(self, removed_idx):
        """Faiss ì¸ë±ìŠ¤ ì¬êµ¬ì¶•"""
        print("[Buffer] ğŸ”§ Rebuilding Faiss index...")
        self._initialize_faiss()
        
        for i, (item, embedding) in enumerate(zip(self.image_storage, self.stored_embeddings)):
            if i != removed_idx:
                self.faiss_index.add_with_ids(
                    embedding.reshape(1, -1), 
                    np.array([item['id']])
                )

    def sample_with_controlled_composition(self, batch_size: int, new_embedding: torch.Tensor = None, 
                                         current_user_id: int = None) -> Tuple[List, List]:
        """
        ğŸ”¥ í•µì‹¬ ë©”ì„œë“œ: ì œì–´ëœ ë°°ì¹˜ êµ¬ì„±ìœ¼ë¡œ ì •í™•í•œ ë¹„ìœ¨ ìƒ˜í”Œë§
        
        Target Composition:
        - Positive samples: target_positive_ratio * batch_size
        - Hard samples: hard_ratio * batch_size  
        - Regular samples: ë‚˜ë¨¸ì§€
        """
        print(f"ğŸ¯ [Controlled] Creating controlled batch (size: {batch_size})")
        print(f"   Target positive ratio: {self.target_positive_ratio:.1%}")
        print(f"   Hard mining ratio: {self.hard_ratio:.1%}")
        
        if len(self.image_storage) == 0:
            return [], []

        # 1. ğŸ¯ ë°°ì¹˜ êµ¬ì„± ê³„íš ìˆ˜ë¦½
        target_positive_samples = max(2, int(batch_size * self.target_positive_ratio))
        # Positive samplesëŠ” pairsì´ë¯€ë¡œ ì§ìˆ˜ë¡œ ì¡°ì •
        if target_positive_samples % 2 == 1:
            target_positive_samples += 1
        target_positive_pairs = target_positive_samples // 2
        
        target_hard_samples = int(batch_size * self.hard_ratio)
        remaining_regular = batch_size - target_positive_samples - target_hard_samples
        
        # ìŒìˆ˜ ë°©ì§€
        if remaining_regular < 0:
            print(f"[Controlled] âš ï¸ Batch size too small, adjusting targets...")
            target_hard_samples = max(0, batch_size - target_positive_samples)
            remaining_regular = batch_size - target_positive_samples - target_hard_samples
        
        print(f"ğŸ“‹ [Controlled] Planned composition:")
        print(f"   Positive pairs: {target_positive_pairs} pairs ({target_positive_samples} samples, {target_positive_samples/batch_size:.1%})")
        print(f"   Hard samples: {target_hard_samples} samples ({target_hard_samples/batch_size:.1%})")
        print(f"   Regular samples: {remaining_regular} samples ({remaining_regular/batch_size:.1%})")

        selected_indices = []
        selected_labels = []
        used_user_ids = set()
        
        # 2. ğŸ¯ ì œí•œëœ Positive Pairs ìƒì„±
        pairs_created = self._create_limited_positive_pairs(
            target_positive_pairs, selected_indices, selected_labels, used_user_ids)
        
        # 3. ğŸ’ª Hard Samples ì„ íƒ
        hard_added = 0
        if self.enable_hard_mining and new_embedding is not None and target_hard_samples > 0:
            hard_added = self._select_controlled_hard_samples(
                target_hard_samples, new_embedding, current_user_id,
                selected_indices, selected_labels, used_user_ids)
        else:
            if not self.enable_hard_mining:
                print(f"[Controlled] ğŸ’ª Hard mining disabled")
            elif new_embedding is None:
                print(f"[Controlled] ğŸ’ª No new embedding for hard mining")
            elif target_hard_samples == 0:
                print(f"[Controlled] ğŸ’ª No hard samples needed")
        
        # 4. ğŸ“¦ Regular Samplesë¡œ ë‚˜ë¨¸ì§€ ì±„ìš°ê¸°
        remaining_slots = batch_size - len(selected_indices)
        regular_added = self._fill_with_regular_samples(
            remaining_slots, selected_indices, selected_labels, used_user_ids)
        
        # 5. ğŸ“Š ì‹¤ì œ êµ¬ì„± ê²€ì¦ ë° ë¦¬í¬íŠ¸
        final_images = self._convert_indices_to_samples(selected_indices)
        self._report_final_composition(selected_labels, batch_size, pairs_created, hard_added, regular_added)
        
        return final_images, selected_labels

    def _create_limited_positive_pairs(self, target_pairs: int, selected_indices: List,
                                     selected_labels: List, used_user_ids: set) -> int:
        """ì œí•œëœ ìˆ˜ì˜ Positive Pairsë§Œ ìƒì„±"""
        print(f"ğŸ¯ [Positive] Creating exactly {target_pairs} positive pairs...")
        
        if target_pairs == 0:
            print(f"[Positive] No positive pairs needed")
            return 0
        
        # ì‚¬ìš©ìë³„ ìƒ˜í”Œ ê·¸ë£¹í™”
        user_groups = {}
        for i, item in enumerate(self.image_storage):
            user_id = item['user_id']
            if user_id not in user_groups:
                user_groups[user_id] = []
            user_groups[user_id].append(i)
        
        # Multi-sample ì‚¬ìš©ìë§Œ ì„ íƒ (2ê°œ ì´ìƒ ìƒ˜í”Œ ë³´ìœ )
        multi_sample_users = {uid: indices for uid, indices in user_groups.items() 
                             if len(indices) >= 2}
        
        if len(multi_sample_users) == 0:
            print(f"[Positive] âš ï¸ No multi-sample users available")
            return 0
        
        available_users = list(multi_sample_users.keys())
        pairs_created = 0
        
        # ìš”ì²­ëœ ìˆ˜ë§Œí¼ positive pairs ìƒì„±
        for _ in range(target_pairs):
            if not available_users:
                break
                
            # ì•„ì§ ì‚¬ìš©ë˜ì§€ ì•Šì€ ì‚¬ìš©ì ìš°ì„  ì„ íƒ
            unused_users = [uid for uid in available_users if uid not in used_user_ids]
            if unused_users:
                selected_user = random.choice(unused_users)
            else:
                selected_user = random.choice(available_users)
            
            # í•´ë‹¹ ì‚¬ìš©ìì—ì„œ 2ê°œ ìƒ˜í”Œ ì„ íƒ
            user_samples = multi_sample_users[selected_user]
            if len(user_samples) >= 2:
                pair = random.sample(user_samples, 2)
                
                selected_indices.extend(pair)
                selected_labels.extend([selected_user, selected_user])
                used_user_ids.add(selected_user)
                pairs_created += 1
                
                print(f"   âœ… Pair {pairs_created}: User {selected_user} (indices: {pair})")
        
        print(f"âœ… [Positive] Created {pairs_created}/{target_pairs} positive pairs")
        return pairs_created

    def _select_controlled_hard_samples(self, target_hard: int, new_embedding: torch.Tensor,
                                      current_user_id: int, selected_indices: List,
                                      selected_labels: List, used_user_ids: set) -> int:
        """ğŸ”¥ ì œì–´ëœ Hard Sample ì„ íƒ (ì‹¤ì œ similarity ê¸°ë°˜)"""
        print(f"ğŸ’ª [Hard] Selecting {target_hard} hard samples...")
        
        if len(self.stored_embeddings) == 0:
            print(f"âš ï¸ [Hard] No embeddings available for hard mining")
            return 0
        
        # ì´ë¯¸ ì„ íƒëœ ì¸ë±ìŠ¤ ì œì™¸
        available_indices = [i for i in range(len(self.image_storage)) 
                           if i not in selected_indices]
        
        if len(available_indices) == 0:
            print(f"âš ï¸ [Hard] No available samples for hard mining")
            return 0
        
        # ğŸ”¥ Hard Score ê³„ì‚°
        buffer_embeddings = np.array([self.stored_embeddings[i] for i in available_indices])
        new_emb = new_embedding.cpu().numpy().flatten()
        
        hard_candidates = []
        for idx, buffer_idx in enumerate(available_indices):
            buffer_emb = buffer_embeddings[idx].flatten()
            
            # Cosine similarity ê³„ì‚°
            similarity = np.dot(new_emb, buffer_emb) / (
                np.linalg.norm(new_emb) * np.linalg.norm(buffer_emb) + 1e-8)
            
            user_id = self.image_storage[buffer_idx]['user_id']
            is_same_user = user_id == current_user_id
            
            # ğŸ”¥ Hard Score ì •ì˜:
            # - Same user + Low similarity = Hard Positive (ê°™ì€ ìœ ì €ì¸ë° ë©€ë¦¬ ìˆìŒ)
            # - Different user + High similarity = Hard Negative (ë‹¤ë¥¸ ìœ ì €ì¸ë° ê°€ê¹Œì´ ìˆìŒ)
            if is_same_user:
                hard_score = 1.0 - similarity  # ë‚®ì€ ìœ ì‚¬ë„ì¼ìˆ˜ë¡ ë†’ì€ hard score
            else:
                hard_score = similarity       # ë†’ì€ ìœ ì‚¬ë„ì¼ìˆ˜ë¡ ë†’ì€ hard score
                
            hard_candidates.append({
                'buffer_index': buffer_idx,
                'hard_score': hard_score,
                'similarity': similarity,
                'is_same_user': is_same_user,
                'user_id': user_id
            })
        
        # Hard score ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (ë†’ì€ hard scoreê°€ ë” ì–´ë ¤ìš´ ìƒ˜í”Œ)
        hard_candidates.sort(key=lambda x: x['hard_score'], reverse=True)
        
        # ìƒìœ„ hard samples ì„ íƒ
        hard_added = 0
        for candidate in hard_candidates[:target_hard]:
            selected_indices.append(candidate['buffer_index'])
            selected_labels.append(candidate['user_id'])
            hard_added += 1
            
            sample_type = "Hard Positive" if candidate['is_same_user'] else "Hard Negative"
            print(f"   ğŸ’ª Hard {hard_added}: User {candidate['user_id']} "
                  f"({sample_type}, score: {candidate['hard_score']:.3f}, sim: {candidate['similarity']:.3f})")
        
        print(f"âœ… [Hard] Selected {hard_added}/{target_hard} hard samples")
        return hard_added

    def _fill_with_regular_samples(self, remaining_slots: int, selected_indices: List,
                                 selected_labels: List, used_user_ids: set) -> int:
        """Regular Samplesë¡œ ë‚˜ë¨¸ì§€ ìŠ¬ë¡¯ ì±„ìš°ê¸°"""
        if remaining_slots <= 0:
            return 0
            
        print(f"ğŸ“¦ [Regular] Filling {remaining_slots} remaining slots...")
        
        # ì´ë¯¸ ì„ íƒëœ ì¸ë±ìŠ¤ ì œì™¸
        available_indices = [i for i in range(len(self.image_storage)) 
                           if i not in selected_indices]
        
        if len(available_indices) == 0:
            print(f"âš ï¸ [Regular] No available samples")
            return 0
        
        # ë‹¤ì–‘ì„±ì„ ìœ„í•´ ì‚¬ìš©ë˜ì§€ ì•Šì€ ì‚¬ìš©ì ìš°ì„  ì„ íƒ
        unused_user_indices = []
        used_user_indices = []
        
        for idx in available_indices:
            user_id = self.image_storage[idx]['user_id']
            if user_id not in used_user_ids:
                unused_user_indices.append(idx)
            else:
                used_user_indices.append(idx)
        
        # ìš°ì„ ìˆœìœ„: ì‚¬ìš©ë˜ì§€ ì•Šì€ ì‚¬ìš©ì â†’ ì‚¬ìš©ëœ ì‚¬ìš©ì
        priority_indices = unused_user_indices + used_user_indices
        
        # í•„ìš”í•œ ë§Œí¼ ì„ íƒ
        selected_count = min(remaining_slots, len(priority_indices))
        chosen_indices = priority_indices[:selected_count]
        
        regular_added = 0
        for idx in chosen_indices:
            user_id = self.image_storage[idx]['user_id']
            selected_indices.append(idx)
            selected_labels.append(user_id)
            regular_added += 1
            
            status = "New User" if user_id not in used_user_ids else "Existing User"
            print(f"   ğŸ“¦ Regular {regular_added}: User {user_id} ({status}, index: {idx})")
            
            used_user_ids.add(user_id)
        
        print(f"âœ… [Regular] Added {regular_added}/{remaining_slots} regular samples")
        return regular_added

    def _report_final_composition(self, labels: List, batch_size: int, 
                                pairs_created: int, hard_added: int, regular_added: int):
        """ìµœì¢… ë°°ì¹˜ êµ¬ì„± ìƒì„¸ ë¦¬í¬íŠ¸"""
        # ì‚¬ìš©ìë³„ ì¹´ìš´íŠ¸
        user_counts = {}
        for label in labels:
            user_counts[label] = user_counts.get(label, 0) + 1
        
        # ì‹¤ì œ positive pairs ê³„ì‚°
        actual_positive_pairs = sum(1 for count in user_counts.values() if count >= 2)
        actual_positive_samples = sum(count for count in user_counts.values() if count >= 2)
        single_samples = sum(1 for count in user_counts.values() if count == 1)
        
        # ë¹„ìœ¨ ê³„ì‚°
        positive_ratio = actual_positive_samples / batch_size
        hard_ratio = hard_added / batch_size  
        regular_ratio = regular_added / batch_size
        
        print(f"ğŸ“Š [Final] Achieved batch composition:")
        print(f"   Total samples: {len(labels)}")
        print(f"   Positive pairs: {actual_positive_pairs} pairs ({actual_positive_samples} samples, {positive_ratio:.1%})")
        print(f"   Hard samples: {hard_added} samples ({hard_ratio:.1%})")
        print(f"   Regular samples: {regular_added} samples ({regular_ratio:.1%})")
        print(f"   Single samples: {single_samples} samples")
        print(f"   Unique users: {len(user_counts)}")
        print(f"   User distribution: {dict(sorted(user_counts.items()))}")
        
        # ëª©í‘œ ëŒ€ë¹„ ë‹¬ì„±ë¥ 
        target_positive_ratio = self.target_positive_ratio
        target_hard_ratio = self.hard_ratio
        
        positive_achievement = positive_ratio / target_positive_ratio if target_positive_ratio > 0 else 0
        hard_achievement = hard_ratio / target_hard_ratio if target_hard_ratio > 0 else 0
        
        print(f"ğŸ¯ [Achievement] Target vs Actual:")
        print(f"   Positive: {positive_ratio:.1%} / {target_positive_ratio:.1%} ({positive_achievement:.1f}x)")
        print(f"   Hard: {hard_ratio:.1%} / {target_hard_ratio:.1%} ({hard_achievement:.1f}x)")

    def sample_with_replacement(self, batch_size: int, new_embedding: torch.Tensor = None, 
                              current_user_id: int = None) -> Tuple[List, List]:
        """
        ë©”ì¸ ìƒ˜í”Œë§ ì¸í„°í˜ì´ìŠ¤ - ì „ëµì— ë”°ë¼ ë¶„ê¸°
        """
        if self.sampling_strategy == "controlled":
            return self.sample_with_controlled_composition(batch_size, new_embedding, current_user_id)
        elif self.sampling_strategy == "balanced":
            # í–¥í›„ êµ¬í˜„ ê°€ëŠ¥
            return self.sample_with_balanced_composition(batch_size, new_embedding, current_user_id)
        else:  # "original" 
            return self.sample_with_original_method(batch_size, new_embedding, current_user_id)

    def sample_with_original_method(self, batch_size: int, new_embedding: torch.Tensor = None, 
                                  current_user_id: int = None) -> Tuple[List, List]:
        """ê¸°ì¡´ artificial positive pairs ë°©ì‹ (í˜¸í™˜ì„± ìœ ì§€)"""
        # ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼í•˜ê²Œ êµ¬í˜„ (ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì œì–´ëœ ë°©ì‹ í˜¸ì¶œ)
        print(f"[Original] Using original sampling method (fallback to controlled)")
        return self.sample_with_controlled_composition(batch_size, new_embedding, current_user_id)

    def _convert_indices_to_samples(self, indices: List) -> List:
        """ì¸ë±ìŠ¤ë¥¼ ì‹¤ì œ ì´ë¯¸ì§€ ìƒ˜í”Œë¡œ ë³€í™˜ (ì¦ê°• ì ìš©)"""
        images = []
        for idx in indices:
            base_image = self.image_storage[idx]['image'].clone()
            # ì¦ê°• ì ìš©
            augmented = self._apply_differentiated_augmentation(base_image)
            images.append(augmented)
        return images

    def _setup_augmentation_transforms(self):
        """3ê°€ì§€ ì¦ê°• ë³€í™˜ ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)"""
        if not self.enable_augmentation or not self.aug_config:
            self.geometric_transform = None
            self.noise_transform = None
            return
            
        # 1. ê¸°í•˜í•™ì  ë³€í™˜
        if getattr(self.aug_config, 'enable_geometric', False):
            max_rotation = getattr(self.aug_config, 'max_rotation_degrees', 3)
            max_translation = getattr(self.aug_config, 'max_translation_ratio', 0.05)
            
            self.geometric_transform = transforms.RandomAffine(
                degrees=(-max_rotation, max_rotation),
                translate=(max_translation, max_translation),
                interpolation=transforms.InterpolationMode.BILINEAR
            )
        else:
            self.geometric_transform = None
            
        # 2. í•´ìƒë„ ì ì‘ ì„¤ì •
        self.resolution_config = {
            'enable': getattr(self.aug_config, 'enable_resolution_adaptation', False),
            'probability': getattr(self.aug_config, 'resolution_probability', 0.3),
            'intermediate_sizes': getattr(self.aug_config, 'intermediate_resolutions', [[64, 64], [96, 96], [160, 160]]),
            'methods': getattr(self.aug_config, 'resize_methods', ['bilinear', 'bicubic'])
        }
        
        # 3. ë…¸ì´ì¦ˆ ì„¤ì •
        self.noise_config = {
            'enable': getattr(self.aug_config, 'enable_noise', False),
            'probability': getattr(self.aug_config, 'noise_probability', 0.3),
            'std_range': getattr(self.aug_config, 'noise_std_range', [0.01, 0.03])
        }

    def _apply_differentiated_augmentation(self, image, sample_info="", intensity="normal"):
        """ì°¨ë³„í™”ëœ ì¦ê°• ì ìš© (ê¸°ì¡´ê³¼ ë™ì¼)"""
        if not self.enable_augmentation:
            return image
            
        result = image.clone()
        applied_augs = []
        
        # ê°•ë„ë³„ í™•ë¥  ì¡°ì •
        if intensity == "strong":
            geo_prob = getattr(self.aug_config, 'geometric_probability', 0.3) * 1.5
            res_prob = self.resolution_config.get('probability', 0.3) * 1.5
            noise_prob = self.noise_config.get('probability', 0.3) * 1.5
        else:
            geo_prob = getattr(self.aug_config, 'geometric_probability', 0.3)
            res_prob = self.resolution_config.get('probability', 0.3)
            noise_prob = self.noise_config.get('probability', 0.3)
        
        # 1. ê¸°í•˜í•™ì  ì¦ê°•
        if (self.geometric_transform and 
            getattr(self.aug_config, 'enable_geometric', False) and
            np.random.random() < geo_prob):
            
            result = self.geometric_transform(result)
            applied_augs.append("Geometric")
        
        # 2. í•´ìƒë„ ì ì‘ ì¦ê°•
        if (self.resolution_config['enable'] and 
            np.random.random() < res_prob):
            
            result = self._apply_resolution_augmentation(result)
            applied_augs.append("Resolution")
        
        # 3. ë…¸ì´ì¦ˆ ì¦ê°•
        if (self.noise_config['enable'] and 
            np.random.random() < noise_prob):
            
            if intensity == "strong":
                noise_std = np.random.uniform(0.02, 0.05)
            else:
                noise_std = np.random.uniform(*self.noise_config['std_range'])
            
            noise = torch.randn_like(result) * noise_std
            result = result + noise
            applied_augs.append(f"Noise(Ïƒ={noise_std:.3f})")
        
        if applied_augs:
            print(f"[Augmentation] ğŸ¨ {sample_info}: {', '.join(applied_augs)}")
        
        return result

    def _apply_resolution_augmentation(self, image):
        """í•´ìƒë„ ì ì‘ ì¦ê°• (ê¸°ì¡´ê³¼ ë™ì¼)"""
        intermediate_size = random.choice(self.resolution_config['intermediate_sizes'])
        method1 = random.choice(self.resolution_config['methods'])
        method2 = random.choice(self.resolution_config['methods'])
        
        pil_image = transforms.ToPILImage()(image)
        
        method1_mode = getattr(transforms.InterpolationMode, method1.upper())
        intermediate = transforms.Resize(intermediate_size, interpolation=method1_mode)(pil_image)
        
        method2_mode = getattr(transforms.InterpolationMode, method2.upper())
        final_image = transforms.Resize((128, 128), interpolation=method2_mode)(intermediate)
        
        return transforms.ToTensor()(final_image)

    def get_diversity_stats(self):
        """ë²„í¼ ë‹¤ì–‘ì„± í†µê³„ (ê¸°ì¡´ê³¼ ë™ì¼)"""
        if len(self.image_storage) == 0:
            return {
                'total_samples': 0,
                'unique_users': 0,
                'user_distribution': {},
                'diversity_score': 0.0
            }
        
        user_counts = {}
        for item in self.image_storage:
            user_id = item['user_id']
            user_counts[user_id] = user_counts.get(user_id, 0) + 1
        
        unique_users = len(user_counts)
        diversity_score = unique_users / len(self.image_storage)
        
        return {
            'total_samples': len(self.image_storage),
            'unique_users': unique_users,
            'user_distribution': dict(sorted(user_counts.items())),
            'diversity_score': diversity_score
        }

    def save_state(self):
        """ìƒíƒœ ì €ì¥ (ê¸°ì¡´ê³¼ ë™ì¼)"""
        if self.faiss_index is None:
            return
            
        data_to_save = {
            'faiss_index_data': faiss.serialize_index(self.faiss_index),
            'metadata': self.metadata,
            'image_storage': self.image_storage,
            'stored_embeddings': self.stored_embeddings
        }
        
        with open(self.state_file, 'wb') as f:
            pickle.dump(data_to_save, f)
        
        diversity_stats = self.get_diversity_stats()
        print(f"[Buffer] ğŸ’¾ Saved {len(self.image_storage)} samples")
        print(f"[Buffer] Diversity: {diversity_stats['unique_users']} users, "
              f"{diversity_stats['diversity_score']:.2f} score")

    def _load_state(self):
        """ìƒíƒœ ë¡œë“œ (ê¸°ì¡´ê³¼ ë™ì¼)"""
        if not self.state_file.exists():
            return

        try:
            with open(self.state_file, 'rb') as f:
                saved_data = pickle.load(f)
                
                self.faiss_index = faiss.deserialize_index(saved_data['faiss_index_data'])
                self.metadata = saved_data['metadata']
                self.image_storage = saved_data.get('image_storage', [])
                self.stored_embeddings = saved_data.get('stored_embeddings', [])
                
                diversity_stats = self.get_diversity_stats()
                print(f"[Buffer] âœ… Restored {len(self.image_storage)} samples")
                print(f"[Buffer] Diversity: {diversity_stats['unique_users']} users")
                
        except Exception as e:
            print(f"[Buffer] âŒ Failed to load state: {e}")
            self.faiss_index = None
            self.metadata = {}
            self.image_storage = []
            self.stored_embeddings = []

print("âœ… CoconutReplayBuffer with Controlled Batch Composition ì™„ë£Œ!")