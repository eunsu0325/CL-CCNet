# framework/replay_buffer.py - ë¡œê¹… ë‹¨ìˆœí™”ëœ ë²„ì „
"""
CoCoNut Intelligent Replay Buffer 

CORE CONTRIBUTION:
- Diversity-based sample selection using Faiss similarity search
- Original images stored for exact replay (no degradation)
- Metadata management for rich context information
- Efficient culling strategy to maintain buffer size
- Proper batch size support with replacement sampling

DESIGN PHILOSOPHY:
- Focus on buffer intelligence, not learning complexity
- Faiss-accelerated for practical deployment
- Memory-efficient with automatic duplicate removal
"""

import os
import pickle
from pathlib import Path

import faiss
import numpy as np
import torch

class CoconutReplayBuffer:
    def __init__(self, config, storage_dir: Path, feature_dimension: int = 2048):
        """
        CoCoNutì˜ ì§€ëŠ¥í˜• ë¦¬í”Œë ˆì´ ë²„í¼ ì´ˆê¸°í™”
        
        CORE INNOVATION:
        - Faiss index for efficient high-dimensional similarity search
        - Diversity threshold prevents redundant samples
        - Original image storage ensures perfect replay quality
        - Support for replacement sampling to guarantee batch sizes
        """
        self.config = config
        self.storage_dir = storage_dir
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        
        self.buffer_size = self.config.max_buffer_size
        self.similarity_threshold = self.config.similarity_threshold
        self.feature_dimension = feature_dimension

        # Faiss: ê³ ì°¨ì› ë²¡í„°ì˜ ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìœ„í•œ ì¸ë±ìŠ¤
        self.image_storage = []  # ì›ë³¸ ì´ë¯¸ì§€ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        self.faiss_index = None  # ë‹¤ì–‘ì„± ì¸¡ì •ìš©
        
        # ì„ë² ë”©ì„ ë³„ë„ë¡œ ì €ì¥í•˜ì—¬ reconstruction ë¬¸ì œ í•´ê²°
        self.stored_embeddings = []

        # ë²„í¼ì— ì €ì¥ëœ ë°ì´í„°ì˜ ë©”íƒ€ì •ë³´ ê´€ë¦¬
        self.metadata = {}

        # íŠ¹ì§• ì¶”ì¶œì„ ìœ„í•œ ëª¨ë¸ ì°¸ì¡°
        self.feature_extractor = None

        # ì‹œìŠ¤í…œ ì¬ì‹œì‘ì„ ìœ„í•œ ìƒíƒœ íŒŒì¼ ê²½ë¡œ
        self.state_file = self.storage_dir / 'buffer_state.pkl'
        self._load_state()

        print(f"[Buffer] ğŸ¥¥ CoCoNut Replay Buffer initialized")
        print(f"[Buffer] Max size: {self.buffer_size}, Similarity threshold: {self.similarity_threshold}")
        print(f"[Buffer] Current buffer size: {self.faiss_index.ntotal if self.faiss_index else 0}")

    def set_feature_extractor(self, model):
        """íŠ¹ì§• ì¶”ì¶œì„ ìœ„í•œ ëª¨ë¸ ì„¤ì •"""
        self.feature_extractor = model

    def _initialize_faiss(self):
        """Faiss ì¸ë±ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        index = faiss.IndexFlatIP(self.feature_dimension)
        self.faiss_index = faiss.IndexIDMap(index)
        print(f"[Buffer] Faiss index initialized with dimension {self.feature_dimension}")

    def _extract_feature_for_diversity(self, image):
        """ë‹¤ì–‘ì„± ì¸¡ì •ì„ ìœ„í•œ íŠ¹ì§• ë²¡í„° ì¶”ì¶œ"""
        if self.feature_extractor is None:
            raise ValueError("Feature extractor not set. Call set_feature_extractor() first.")
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€ (í•„ìš”í•œ ê²½ìš°)
        if len(image.shape) == 3:
            image = image.unsqueeze(0)
        
        # CCNetì˜ getFeatureCode ë©”ì„œë“œ ì‚¬ìš©
        with torch.no_grad():
            features = self.feature_extractor.getFeatureCode(image)
        
        return features

    def add(self, image: torch.Tensor, user_id: int):
        """
        ìƒˆë¡œìš´ ê²½í—˜(ì´ë¯¸ì§€)ì„ ë²„í¼ì— ì¶”ê°€í• ì§€ ê²°ì •í•˜ê³  ì¶”ê°€í•©ë‹ˆë‹¤.
        
        DIVERSITY-BASED ADDITION (í•µì‹¬ ê¸°ì—¬):
        1. Extract feature embedding from new image
        2. Calculate similarity with existing samples using Faiss
        3. Add only if sufficiently diverse (below threshold)
        """
        # 1. ë‹¤ì–‘ì„± ì¸¡ì •ì„ ìœ„í•œ ì„ë² ë”© ì¶”ì¶œ
        with torch.no_grad():
            embedding = self._extract_feature_for_diversity(image)
            embedding_np = embedding.cpu().numpy().astype('float32')
            faiss.normalize_L2(embedding_np)

        if self.faiss_index is None:
            self._initialize_faiss()

        # 2. ë‹¤ì–‘ì„± í™•ì¸
        if self.faiss_index.ntotal == 0:
            max_similarity = 0.0
        else:
            distances, _ = self.faiss_index.search(embedding_np, k=1)
            max_similarity = distances[0][0]

        # 3. ìƒˆë¡œìš´ ê²½í—˜ì´ë©´ ì›ë³¸ ì´ë¯¸ì§€ ì €ì¥
        if max_similarity < self.similarity_threshold:
            if len(self.image_storage) >= self.buffer_size:
                self._cull()  # ì˜¤ë˜ëœ ì´ë¯¸ì§€ ì œê±°
            
            unique_id = len(self.image_storage)
            self.image_storage.append({
                'image': image.cpu().clone(),  # ì›ë³¸ ì´ë¯¸ì§€ ì €ì¥
                'user_id': user_id,
                'id': unique_id
            })
            
            # ì„ë² ë”©ë„ ë³„ë„ë¡œ ì €ì¥
            self.stored_embeddings.append(embedding_np.copy())
            
            # ë‹¤ì–‘ì„± ì¸¡ì •ìš© faiss ì¸ë±ìŠ¤ë„ ì—…ë°ì´íŠ¸
            self.faiss_index.add_with_ids(embedding_np, np.array([unique_id]))
            self.metadata[unique_id] = {'user_id': user_id}
            
            print(f"[Buffer] âœ… Added diverse sample (ID: {unique_id}, User: {user_id}). "
                  f"Buffer: {len(self.image_storage)}/{self.buffer_size}")
        else:
            print(f"[Buffer] âš ï¸ Similar sample skipped (similarity: {max_similarity:.4f} >= {self.similarity_threshold})")
            
    def _cull(self):
        """
        ë²„í¼ ë‚´ì—ì„œ ê°€ì¥ ì¤‘ë³µë˜ëŠ” ë°ì´í„°ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
        
        INTELLIGENT CULLING (í•µì‹¬ ê¸°ì—¬):
        - Faissë¥¼ ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µë„ê°€ ê°€ì¥ ë†’ì€ ìƒ˜í”Œ ì‹ë³„
        - ì €ì¥ëœ ì„ë² ë”©ì„ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ì•ˆì •ì„± í™•ë³´
        """
        if self.faiss_index.ntotal < 2:
            return

        print(f"[Buffer] ğŸ”„ Buffer full ({self.faiss_index.ntotal}). Finding most redundant sample...")
        
        # ì €ì¥ëœ ì„ë² ë”©ë“¤ì„ NumPy ë°°ì—´ë¡œ ë³€í™˜
        if len(self.stored_embeddings) == 0:
            print("[Buffer] âš ï¸ No stored embeddings for culling")
            return
        
        all_vectors = np.vstack(self.stored_embeddings)
        
        k = min(self.faiss_index.ntotal, 50) 
        similarities, _ = self.faiss_index.search(all_vectors, k=k)
        
        # ê° ë²¡í„°ì˜ ìœ ì‚¬ë„ ì´í•© ê³„ì‚° (ìê¸° ìì‹ ê³¼ì˜ ìœ ì‚¬ë„(1.0)ëŠ” ì œì™¸)
        diversity_scores = similarities.sum(axis=1) - 1.0
        
        cull_idx_in_storage = np.argmax(diversity_scores)
        cull_unique_id = self.image_storage[cull_idx_in_storage]['id']

        # Faiss ì¸ë±ìŠ¤ì—ì„œ ì œê±°
        try:
            self.faiss_index.remove_ids(np.array([cull_unique_id]))
        except Exception as e:
            print(f"[Buffer] âš ï¸ Faiss removal failed, rebuilding index...")
            self._rebuild_faiss_index_after_removal(cull_idx_in_storage)
        
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ì œê±°
        if cull_unique_id in self.metadata:
            del self.metadata[cull_unique_id]
        
        # ì´ë¯¸ì§€ ì €ì¥ì†Œì—ì„œë„ ì œê±°
        del self.image_storage[cull_idx_in_storage]
        del self.stored_embeddings[cull_idx_in_storage]
        
        print(f"[Buffer] ğŸ—‘ï¸ Removed redundant sample (ID: {cull_unique_id})")

    def _rebuild_faiss_index_after_removal(self, removed_idx):
        """Faiss ì¸ë±ìŠ¤ ì¬êµ¬ì¶• (ì œê±° ì‹¤íŒ¨ ì‹œ ë°±ì—… ë°©ë²•)"""
        print("[Buffer] ğŸ”§ Rebuilding Faiss index...")
        
        # ìƒˆë¡œìš´ ì¸ë±ìŠ¤ ìƒì„±
        self._initialize_faiss()
        
        # ì œê±°ë  í•­ëª©ì„ ì œì™¸í•˜ê³  ë‹¤ì‹œ ì¶”ê°€
        for i, (item, embedding) in enumerate(zip(self.image_storage, self.stored_embeddings)):
            if i != removed_idx:  # ì œê±°ë  í•­ëª© ì œì™¸
                self.faiss_index.add_with_ids(
                    embedding.reshape(1, -1), 
                    np.array([item['id']])
                )

    def sample_with_replacement(self, batch_size: int):
        """
        ë³µì› ì¶”ì¶œë¡œ ì¶©ë¶„í•œ ìƒ˜í”Œ í™•ë³´ (í•µì‹¬ ê¸°ëŠ¥)
        
        KEY CONTRIBUTION: ë²„í¼ í¬ê¸°ì— ê´€ê³„ì—†ì´ ìš”ì²­ëœ ê°œìˆ˜ë§Œí¼ ë°˜í™˜
        - í•­ìƒ ì„¤ì •ëœ batch_size ë‹¬ì„±
        - ì—°ì†í•™ìŠµì´ ì¶©ë¶„í•œ ëŒ€ì¡° ìŒìœ¼ë¡œ í•™ìŠµ ê°€ëŠ¥
        """
        if len(self.image_storage) == 0:
            print("[Buffer] âš ï¸ Empty buffer - returning empty lists")
            return [], []
        
        if len(self.image_storage) == 1:
            # ë²„í¼ì— 1ê°œë§Œ ìˆìœ¼ë©´ ê·¸ê²ƒë§Œ ë°˜ë³µ
            item = self.image_storage[0]
            images = [item['image'].clone() for _ in range(batch_size)]
            labels = [item['user_id'] for _ in range(batch_size)]
            
            print(f"[Buffer] ğŸ”„ Single sample replication: {batch_size} copies of User {item['user_id']}")
            return images, labels
        
        # ë³µì› ì¶”ì¶œë¡œ í•„ìš”í•œ ê°œìˆ˜ë§Œí¼ ìƒ˜í”Œë§
        sampled_indices = np.random.choice(
            len(self.image_storage), 
            size=batch_size, 
            replace=True  # í•µì‹¬: ë³µì› ì¶”ì¶œ í—ˆìš©
        )
        
        images = []
        labels = []
        sampled_users = []
        
        for idx in sampled_indices:
            item = self.image_storage[idx]
            images.append(item['image'].clone())  # ë³µì‚¬ë³¸ ìƒì„± (ì¤‘ìš”!)
            labels.append(item['user_id'])
            sampled_users.append(item['user_id'])
        
        # ìƒ˜í”Œë§ í’ˆì§ˆ ë¶„ì„
        unique_users = len(set(sampled_users))
        user_counts = {}
        for user_id in sampled_users:
            user_counts[user_id] = user_counts.get(user_id, 0) + 1
        
        print(f"[Buffer] ğŸ¯ Sampled {len(images)} replay samples:")
        print(f"   Unique users: {unique_users}, Distribution: {dict(sorted(user_counts.items()))}")
        
        return images, labels

    def sample(self, batch_size: int):
        """ê¸°ë³¸ ìƒ˜í”Œë§ - ë³µì› ì¶”ì¶œ ì‚¬ìš©"""
        return self.sample_with_replacement(batch_size)

    def get_diversity_stats(self):
        """ë²„í¼ ë‹¤ì–‘ì„± í†µê³„ ë°˜í™˜"""
        if len(self.image_storage) == 0:
            return {
                'total_samples': 0,
                'unique_users': 0,
                'user_distribution': {},
                'diversity_score': 0.0
            }
        
        user_counts = {}
        for item in self.image_storage:
            user_id = item['user_id']
            user_counts[user_id] = user_counts.get(user_id, 0) + 1
        
        unique_users = len(user_counts)
        diversity_score = unique_users / len(self.image_storage)
        
        return {
            'total_samples': len(self.image_storage),
            'unique_users': unique_users,
            'user_distribution': dict(sorted(user_counts.items())),
            'diversity_score': diversity_score
        }

    def save_state(self):
        """ë²„í¼ì˜ í˜„ì¬ ìƒíƒœë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        if self.faiss_index is None:
            print("[Buffer] No data to save.")
            return
            
        print(f"[Buffer] ğŸ’¾ Saving buffer state...")
        
        data_to_save = {
            'faiss_index_data': faiss.serialize_index(self.faiss_index),
            'metadata': self.metadata,
            'image_storage': self.image_storage,
            'stored_embeddings': self.stored_embeddings
        }
        
        with open(self.state_file, 'wb') as f:
            pickle.dump(data_to_save, f)
        
        # ë‹¤ì–‘ì„± í†µê³„ë„ ì €ì¥
        diversity_stats = self.get_diversity_stats()
        stats_file = self.storage_dir / 'buffer_diversity_stats.json'
        
        import json
        with open(stats_file, 'w') as f:
            json.dump(diversity_stats, f, indent=2)
        
        print(f"[Buffer] âœ… Saved {len(self.image_storage)} samples")
        print(f"[Buffer] Diversity: {diversity_stats['unique_users']} users, "
              f"{diversity_stats['diversity_score']:.2f} score")

    def _load_state(self):
        """íŒŒì¼ì—ì„œ ë²„í¼ ìƒíƒœë¥¼ ë³µì›í•©ë‹ˆë‹¤."""
        if not self.state_file.exists():
            print(f"[Buffer] ğŸ“‚ No previous state found - starting fresh")
            return

        print(f"[Buffer] ğŸ”„ Loading previous buffer state...")
        try:
            with open(self.state_file, 'rb') as f:
                saved_data = pickle.load(f)
                
                # Faiss ì¸ë±ìŠ¤ ë³µì›
                self.faiss_index = faiss.deserialize_index(saved_data['faiss_index_data'])
                self.metadata = saved_data['metadata']
                
                # ì´ë¯¸ì§€ ì €ì¥ì†Œë„ ë³µì›
                if 'image_storage' in saved_data:
                    self.image_storage = saved_data['image_storage']
                
                # ì €ì¥ëœ ì„ë² ë”©ë„ ë³µì›
                if 'stored_embeddings' in saved_data:
                    self.stored_embeddings = saved_data['stored_embeddings']
                else:
                    self.stored_embeddings = []
                    print("[Buffer] âš ï¸ No embeddings in saved state")
                
                # ë³µì›ëœ ë‹¤ì–‘ì„± í†µê³„ ì¶œë ¥
                diversity_stats = self.get_diversity_stats()
                print(f"[Buffer] âœ… Restored {len(self.image_storage)} samples")
                print(f"[Buffer] Diversity: {diversity_stats['unique_users']} users")
                
        except Exception as e:
            print(f"[Buffer] âŒ Failed to load state: {e}. Starting fresh.")
            self.faiss_index = None
            self.metadata = {}
            self.image_storage = []
            self.stored_embeddings = []