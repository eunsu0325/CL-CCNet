# framework/replay_buffer.py - Artificial Positive Pairs ì§€ì›
"""
CoCoNut Intelligent Replay Buffer with Advanced Sampling

CORE FEATURES:
- Diversity-based sample selection using Faiss similarity search
- Hard sample mining with fixed ratio
- 3-type augmentation: Geometric + Resolution + Noise
- ğŸ”¥ NEW: Artificial positive pairs for single-sample users
- Complete state management for checkpoint resume
"""

import os
import pickle
import random
from pathlib import Path

import faiss
import numpy as np
import torch
import torchvision.transforms as transforms
from PIL import Image

class CoconutReplayBuffer:
    def __init__(self, config, storage_dir: Path, feature_dimension: int = 2048):
        """
        ì§€ëŠ¥í˜• ë¦¬í”Œë ˆì´ ë²„í¼ ì´ˆê¸°í™”
        """
        self.config = config
        self.storage_dir = storage_dir
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        
        self.buffer_size = self.config.max_buffer_size
        self.similarity_threshold = self.config.similarity_threshold
        self.feature_dimension = feature_dimension

        # Faiss ì¸ë±ìŠ¤ ë° ì €ì¥ì†Œ
        self.image_storage = []
        self.faiss_index = None
        self.stored_embeddings = []
        self.metadata = {}
        self.feature_extractor = None

        # ğŸ”¥ Hard Mining ì„¤ì • (ë‚˜ì¤‘ì— CoconutSystemì—ì„œ ì„¤ì •ë¨)
        self.enable_hard_mining = False  # ê¸°ë³¸ê°’
        self.hard_ratio = 0.3  # ê¸°ë³¸ê°’
        
        # ğŸ”¥ ë°ì´í„° ì¦ê°• ì„¤ì • (ë‚˜ì¤‘ì— CoconutSystemì—ì„œ ì„¤ì •ë¨)
        self.enable_augmentation = False  # ê¸°ë³¸ê°’
        self.aug_config = None
        self._setup_augmentation_transforms()
        
        # ìƒíƒœ íŒŒì¼ ê²½ë¡œ
        self.state_file = self.storage_dir / 'buffer_state.pkl'
        self._load_state()

        print(f"[Buffer] ğŸ¥¥ CoCoNut Replay Buffer initialized")
        print(f"[Buffer] Max size: {self.buffer_size}, Threshold: {self.similarity_threshold}")
        print(f"[Buffer] Hard mining: {self.enable_hard_mining} (ratio: {self.hard_ratio})")
        print(f"[Buffer] Augmentation: {self.enable_augmentation}")
        print(f"[Buffer] Current size: {len(self.image_storage)}")

    def update_hard_mining_config(self, enable_hard_mining, hard_ratio):
        """ğŸ”¥ Hard Mining ì„¤ì • ì—…ë°ì´íŠ¸ (CoconutSystemì—ì„œ í˜¸ì¶œ)"""
        self.enable_hard_mining = enable_hard_mining
        self.hard_ratio = hard_ratio
        print(f"[Buffer] ğŸ”¥ Hard Mining updated: {self.enable_hard_mining} (ratio: {self.hard_ratio})")

    def update_augmentation_config(self, enable_augmentation, aug_config):
        """ğŸ”¥ ë°ì´í„° ì¦ê°• ì„¤ì • ì—…ë°ì´íŠ¸ (CoconutSystemì—ì„œ í˜¸ì¶œ)"""
        self.enable_augmentation = enable_augmentation
        self.aug_config = aug_config
        self._setup_augmentation_transforms()
        print(f"[Buffer] ğŸ¨ Augmentation updated: {self.enable_augmentation}")

    def _setup_augmentation_transforms(self):
        """3ê°€ì§€ ì¦ê°• ë³€í™˜ ì„¤ì •"""
        if not self.enable_augmentation or not self.aug_config:
            self.geometric_transform = None
            self.noise_transform = None
            return
            
        # 1. ê¸°í•˜í•™ì  ë³€í™˜
        if getattr(self.aug_config, 'enable_geometric', False):
            max_rotation = getattr(self.aug_config, 'max_rotation_degrees', 3)
            max_translation = getattr(self.aug_config, 'max_translation_ratio', 0.05)
            
            self.geometric_transform = transforms.RandomAffine(
                degrees=(-max_rotation, max_rotation),
                translate=(max_translation, max_translation),
                interpolation=transforms.InterpolationMode.BILINEAR
            )
        else:
            self.geometric_transform = None
            
        # 2. í•´ìƒë„ ì ì‘ - ë™ì ìœ¼ë¡œ ì²˜ë¦¬
        self.resolution_config = {
            'enable': getattr(self.aug_config, 'enable_resolution_adaptation', False),
            'probability': getattr(self.aug_config, 'resolution_probability', 0.3),
            'intermediate_sizes': getattr(self.aug_config, 'intermediate_resolutions', [[64, 64], [96, 96], [160, 160]]),
            'methods': getattr(self.aug_config, 'resize_methods', ['bilinear', 'bicubic'])
        }
        
        # 3. ë…¸ì´ì¦ˆ - ë™ì ìœ¼ë¡œ ì²˜ë¦¬
        self.noise_config = {
            'enable': getattr(self.aug_config, 'enable_noise', False),
            'probability': getattr(self.aug_config, 'noise_probability', 0.3),
            'std_range': getattr(self.aug_config, 'noise_std_range', [0.01, 0.03])
        }

    def set_feature_extractor(self, model):
        """íŠ¹ì§• ì¶”ì¶œì„ ìœ„í•œ ëª¨ë¸ ì„¤ì •"""
        self.feature_extractor = model

    def _initialize_faiss(self):
        """Faiss ì¸ë±ìŠ¤ ì´ˆê¸°í™”"""
        index = faiss.IndexFlatIP(self.feature_dimension)
        self.faiss_index = faiss.IndexIDMap(index)
        print(f"[Buffer] Faiss index initialized with dimension {self.feature_dimension}")

    def _extract_feature_for_diversity(self, image):
        """ë‹¤ì–‘ì„± ì¸¡ì •ì„ ìœ„í•œ íŠ¹ì§• ë²¡í„° ì¶”ì¶œ"""
        if self.feature_extractor is None:
            raise ValueError("Feature extractor not set")
        
        if len(image.shape) == 3:
            image = image.unsqueeze(0)
        
        with torch.no_grad():
            features = self.feature_extractor.getFeatureCode(image)
        
        return features

    def add(self, image: torch.Tensor, user_id: int):
        """ìƒˆë¡œìš´ ê²½í—˜ì„ ë²„í¼ì— ì¶”ê°€ (ë‹¤ì–‘ì„± ê¸°ë°˜)"""
        with torch.no_grad():
            embedding = self._extract_feature_for_diversity(image)
            embedding_np = embedding.cpu().numpy().astype('float32')
            faiss.normalize_L2(embedding_np)

        if self.faiss_index is None:
            self._initialize_faiss()

        # ë‹¤ì–‘ì„± í™•ì¸
        if self.faiss_index.ntotal == 0:
            max_similarity = 0.0
        else:
            distances, _ = self.faiss_index.search(embedding_np, k=1)
            max_similarity = distances[0][0]

        # ìƒˆë¡œìš´ ê²½í—˜ì´ë©´ ì €ì¥
        if max_similarity < self.similarity_threshold:
            if len(self.image_storage) >= self.buffer_size:
                self._cull()
            
            unique_id = len(self.image_storage)
            self.image_storage.append({
                'image': image.cpu().clone(),
                'user_id': user_id,
                'id': unique_id
            })
            
            self.stored_embeddings.append(embedding_np.copy())
            self.faiss_index.add_with_ids(embedding_np, np.array([unique_id]))
            self.metadata[unique_id] = {'user_id': user_id}
            
            print(f"[Buffer] âœ… Added diverse sample (ID: {unique_id}, User: {user_id}). "
                  f"Buffer: {len(self.image_storage)}/{self.buffer_size}")
        else:
            print(f"[Buffer] âš ï¸ Similar sample skipped (similarity: {max_similarity:.4f} >= {self.similarity_threshold})")

    def _cull(self):
        """ê°€ì¥ ì¤‘ë³µë˜ëŠ” ë°ì´í„° ì œê±°"""
        if self.faiss_index.ntotal < 2:
            return

        print(f"[Buffer] ğŸ”„ Buffer full. Finding most redundant sample...")
        
        if len(self.stored_embeddings) == 0:
            return
        
        all_vectors = np.vstack(self.stored_embeddings)
        k = min(self.faiss_index.ntotal, 50)
        similarities, _ = self.faiss_index.search(all_vectors, k=k)
        
        diversity_scores = similarities.sum(axis=1) - 1.0
        cull_idx_in_storage = np.argmax(diversity_scores)
        cull_unique_id = self.image_storage[cull_idx_in_storage]['id']

        try:
            self.faiss_index.remove_ids(np.array([cull_unique_id]))
        except Exception:
            self._rebuild_faiss_index_after_removal(cull_idx_in_storage)
        
        if cull_unique_id in self.metadata:
            del self.metadata[cull_unique_id]
        
        del self.image_storage[cull_idx_in_storage]
        del self.stored_embeddings[cull_idx_in_storage]
        
        print(f"[Buffer] ğŸ—‘ï¸ Removed redundant sample (ID: {cull_unique_id})")

    def _rebuild_faiss_index_after_removal(self, removed_idx):
        """Faiss ì¸ë±ìŠ¤ ì¬êµ¬ì¶•"""
        print("[Buffer] ğŸ”§ Rebuilding Faiss index...")
        self._initialize_faiss()
        
        for i, (item, embedding) in enumerate(zip(self.image_storage, self.stored_embeddings)):
            if i != removed_idx:
                self.faiss_index.add_with_ids(
                    embedding.reshape(1, -1), 
                    np.array([item['id']])
                )

    def _select_hard_samples(self, num_hard, new_embedding, current_user_id):
        """ì–´ë ¤ìš´ ìƒ˜í”Œ ì„ íƒ"""
        if len(self.stored_embeddings) == 0:
            return []
            
        buffer_embeddings = np.array(self.stored_embeddings)
        new_emb = new_embedding.cpu().numpy().flatten()
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []
        for i, buffer_emb in enumerate(buffer_embeddings):
            similarity = np.dot(new_emb, buffer_emb.flatten()) / (
                np.linalg.norm(new_emb) * np.linalg.norm(buffer_emb.flatten())
            )
            
            is_same_user = self.image_storage[i]['user_id'] == current_user_id
            
            # Hard score ê³„ì‚°
            if is_same_user:
                hard_score = 1.0 - similarity  # ê°™ì€ ìœ ì €ì¸ë° ë©€ë¦¬ ìˆìŒ
            else:
                hard_score = similarity       # ë‹¤ë¥¸ ìœ ì €ì¸ë° ê°€ê¹Œì´ ìˆìŒ
                
            similarities.append({
                'index': i,
                'similarity': similarity,
                'hard_score': hard_score,
                'is_same_user': is_same_user
            })
        
        # Hard score ê¸°ì¤€ ì •ë ¬
        similarities.sort(key=lambda x: x['hard_score'], reverse=True)
        
        # ìƒìœ„ ì–´ë ¤ìš´ ìƒ˜í”Œë“¤ ì„ íƒ
        hard_samples = similarities[:num_hard]
        hard_indices = [item['index'] for item in hard_samples]
        
        # í†µê³„
        hard_positives = sum(1 for item in hard_samples if item['is_same_user'])
        hard_negatives = len(hard_samples) - hard_positives
        
        print(f"[HardMining] ğŸ’ª Selected {num_hard} hard samples:")
        print(f"   Hard positives (same user, far): {hard_positives}")
        print(f"   Hard negatives (diff user, close): {hard_negatives}")
        
        return hard_indices

    def _apply_resolution_augmentation(self, image):
        """í•´ìƒë„ ì ì‘ ì¦ê°• (í¬ê¸°ëŠ” 128x128 ìœ ì§€)"""
        # ì¤‘ê°„ í•´ìƒë„ ì„ íƒ
        intermediate_size = random.choice(self.resolution_config['intermediate_sizes'])
        method1 = random.choice(self.resolution_config['methods'])
        method2 = random.choice(self.resolution_config['methods'])
        
        # PIL ë³€í™˜
        pil_image = transforms.ToPILImage()(image)
        
        # ì¤‘ê°„ í•´ìƒë„ë¡œ ë³€í™˜
        method1_mode = getattr(transforms.InterpolationMode, method1.upper())
        intermediate = transforms.Resize(intermediate_size, interpolation=method1_mode)(pil_image)
        
        # ì›ë˜ í¬ê¸°ë¡œ ë³µì›
        method2_mode = getattr(transforms.InterpolationMode, method2.upper())
        final_image = transforms.Resize((128, 128), interpolation=method2_mode)(intermediate)
        
        return transforms.ToTensor()(final_image)

    def _apply_differentiated_augmentation(self, image, sample_info="", intensity="normal"):
        """ğŸ”¥ NEW: ì°¨ë³„í™”ëœ ì¦ê°• ê°•ë„ ì ìš©"""
        if not self.enable_augmentation:
            return image
            
        result = image.clone()
        applied_augs = []
        
        # ê°•ë„ë³„ í™•ë¥  ì¡°ì •
        if intensity == "strong":
            geo_prob = getattr(self.aug_config, 'geometric_probability', 0.3) * 1.5  # 1.5ë°° ì¦ê°€
            res_prob = self.resolution_config.get('probability', 0.3) * 1.5
            noise_prob = self.noise_config.get('probability', 0.3) * 1.5
        else:
            geo_prob = getattr(self.aug_config, 'geometric_probability', 0.3)
            res_prob = self.resolution_config.get('probability', 0.3)
            noise_prob = self.noise_config.get('probability', 0.3)
        
        # 1. ê¸°í•˜í•™ì  ì¦ê°•
        if (self.geometric_transform and 
            getattr(self.aug_config, 'enable_geometric', False) and
            np.random.random() < geo_prob):
            
            result = self.geometric_transform(result)
            applied_augs.append("Geometric")
        
        # 2. í•´ìƒë„ ì ì‘ ì¦ê°•
        if (self.resolution_config['enable'] and 
            np.random.random() < res_prob):
            
            result = self._apply_resolution_augmentation(result)
            applied_augs.append("Resolution")
        
        # 3. ë…¸ì´ì¦ˆ ì¦ê°•
        if (self.noise_config['enable'] and 
            np.random.random() < noise_prob):
            
            # ê°•ë„ë³„ ë…¸ì´ì¦ˆ ë ˆë²¨ ì¡°ì •
            if intensity == "strong":
                noise_std = np.random.uniform(0.02, 0.05)  # ë” ê°•í•œ ë…¸ì´ì¦ˆ
            else:
                noise_std = np.random.uniform(*self.noise_config['std_range'])
            
            noise = torch.randn_like(result) * noise_std
            result = result + noise
            applied_augs.append(f"Noise(Ïƒ={noise_std:.3f})")
        
        if applied_augs:
            print(f"[Augmentation] ğŸ¨ {sample_info}: {', '.join(applied_augs)}")
        
        return result

    def sample_with_replacement(self, batch_size, new_embedding=None, current_user_id=None):
        """
        ğŸ”¥ Step 3 ìˆ˜ì •: Artificial Positive Pair ìƒì„±
        
        í•µì‹¬ ê°œì„ :
        - Single-sample ì‚¬ìš©ìë„ positive pair ìƒì„± (ê°™ì€ ì´ë¯¸ì§€, ë‹¤ë¥¸ ì¦ê°•)
        - ëª¨ë“  ìƒ˜í”Œì´ í•™ìŠµì— ì°¸ì—¬ ê°€ëŠ¥
        - SupCon Loss Warning ì œê±°
        """
        if len(self.image_storage) == 0:
            return [], []

        print(f"[Sampling] ğŸ¯ Smart sampling with artificial positive pairs (batch_size: {batch_size}, buffer_size: {len(self.image_storage)})")
        
        # 1. ì‚¬ìš©ìë³„ ìƒ˜í”Œ ê·¸ë£¹í™”
        user_groups = {}
        for i, item in enumerate(self.image_storage):
            user_id = item['user_id']
            if user_id not in user_groups:
                user_groups[user_id] = []
            user_groups[user_id].append(i)
        
        # 2. ì‚¬ìš©ì ë¶„ë¥˜
        multi_sample_users = {uid: indices for uid, indices in user_groups.items() 
                             if len(indices) >= 2}
        single_sample_users = {uid: indices for uid, indices in user_groups.items() 
                              if len(indices) == 1}
        
        print(f"[Sampling] ğŸ‘¥ User analysis:")
        print(f"   Multi-sample users: {len(multi_sample_users)} (â‰¥2 samples each)")
        print(f"   Single-sample users: {len(single_sample_users)} (1 sample each)")
        
        # 3. ìƒ˜í”Œë§ ì „ëµ ê²°ì •
        buffer_size = len(self.image_storage)
        base_allow_duplicates = buffer_size < batch_size
        
        print(f"[Sampling] ğŸ“‹ Sampling strategy:")
        print(f"   Buffer size vs Batch size: {buffer_size} vs {batch_size}")
        print(f"   Base allow duplicates: {base_allow_duplicates}")
        
        selected_indices = []
        
        # 4. ğŸ”¥ NEW: ëª¨ë“  ì‚¬ìš©ìì— ëŒ€í•´ positive pair ìš°ì„  í™•ë³´
        total_users = len(user_groups)
        max_pairs_possible = batch_size // 2  # ìµœëŒ€ ê°€ëŠ¥í•œ pair ìˆ˜
        
        # 4a. Multi-sample ì‚¬ìš©ì ì²˜ë¦¬
        natural_pairs = min(len(multi_sample_users), max_pairs_possible)
        pair_users = list(multi_sample_users.keys())[:natural_pairs]
        
        for user_id in pair_users:
            indices = multi_sample_users[user_id]
            # ê° ì‚¬ìš©ìì—ì„œ 2ê°œ ì„ íƒ
            selected = np.random.choice(indices, min(2, len(indices)), replace=False)
            selected_indices.extend(selected)
            print(f"[Sampling] âœ… User {user_id}: selected {len(selected)} samples (natural positive pair)")
        
        # 4b. ğŸ”¥ NEW: Single-sample ì‚¬ìš©ìë¥¼ artificial positive pairë¡œ ë³€í™˜
        remaining_pair_slots = max_pairs_possible - natural_pairs
        artificial_pairs = min(len(single_sample_users), remaining_pair_slots)
        
        artificial_users = list(single_sample_users.keys())[:artificial_pairs]
        for user_id in artificial_users:
            idx = single_sample_users[user_id][0]
            # ğŸ”¥ ê°™ì€ ì¸ë±ìŠ¤ë¥¼ 2ë²ˆ ì¶”ê°€ (ë‚˜ì¤‘ì— ì„œë¡œ ë‹¤ë¥¸ ì¦ê°• ì ìš©ë¨)
            selected_indices.extend([idx, idx])
            print(f"[Sampling] ğŸ¨ User {user_id}: created artificial positive pair (same image, different augmentation)")
        
        # 5. ë‚˜ë¨¸ì§€ ìŠ¬ë¡¯ ì±„ìš°ê¸°
        remaining_slots = batch_size - len(selected_indices)
        print(f"[Sampling] ğŸ“‹ Remaining slots to fill: {remaining_slots}")
        
        if remaining_slots > 0:
            # ì•„ì§ ì‚¬ìš©ë˜ì§€ ì•Šì€ ì‚¬ìš©ìë“¤
            used_users = set(pair_users + artificial_users)
            unused_users = [uid for uid in user_groups.keys() if uid not in used_users]
            
            if unused_users:
                print(f"[Sampling] ğŸ“Š Unused users available: {len(unused_users)}")
                # ë‚¨ì€ ì‚¬ìš©ìë“¤ì—ì„œ ìƒ˜í”Œ ì„ íƒ
                unused_indices = []
                for uid in unused_users:
                    unused_indices.extend(user_groups[uid])
                
                if len(unused_indices) >= remaining_slots:
                    if base_allow_duplicates:
                        additional_indices = np.random.choice(unused_indices, remaining_slots, replace=True)
                    else:
                        additional_indices = np.random.choice(unused_indices, remaining_slots, replace=False)
                    selected_indices.extend(additional_indices)
                    print(f"[Sampling] ğŸ“‹ Added {remaining_slots} samples from unused users")
                else:
                    # ë¶€ì¡±í•˜ë©´ ì „ì²´ì—ì„œ ì„ íƒ
                    all_indices = list(range(len(self.image_storage)))
                    additional_indices = np.random.choice(all_indices, remaining_slots, replace=True)
                    selected_indices.extend(additional_indices)
                    print(f"[Sampling] ğŸ”„ Added {remaining_slots} samples from all users (with replacement)")
            else:
                # ì‚¬ìš©ë˜ì§€ ì•Šì€ ì‚¬ìš©ìê°€ ì—†ìœ¼ë©´ ì „ì²´ì—ì„œ ì„ íƒ
                all_indices = list(range(len(self.image_storage)))
                additional_indices = np.random.choice(all_indices, remaining_slots, replace=True)
                selected_indices.extend(additional_indices)
                print(f"[Sampling] ğŸ”„ Added {remaining_slots} samples from all users (with replacement)")

        # 6. ğŸ”¥ NEW: ìµœì¢… ìƒ˜í”Œ êµ¬ì„± ë° ì°¨ë³„í™”ëœ ì¦ê°•
        images = []
        labels = []
        
        # ì¤‘ë³µ ì¸ë±ìŠ¤ ì¶”ì  (artificial positive pairìš©)
        index_count = {}
        for idx in selected_indices:
            index_count[idx] = index_count.get(idx, 0) + 1
        
        # ê° ì¸ë±ìŠ¤ë³„ë¡œ ëª‡ ë²ˆì§¸ ì‚¬ìš©ì¸ì§€ ì¶”ì 
        index_usage = {}
        
        for i, idx in enumerate(selected_indices):
            item = self.image_storage[idx]
            base_image = item['image'].clone()
            
            # ğŸ”¥ ì¤‘ë³µ ì‚¬ìš© ì‹œ ë” ê°•í•œ ì¦ê°• ì ìš©
            usage_count = index_usage.get(idx, 0)
            index_usage[idx] = usage_count + 1
            
            if index_count[idx] > 1:
                # Artificial positive pair: ì°¨ë³„í™”ëœ ì¦ê°•
                augmentation_intensity = "strong" if usage_count > 0 else "normal"
                sample_info = f"Sample{i+1}(User{item['user_id']}, {augmentation_intensity}_aug)"
            else:
                # Natural sample: ì¼ë°˜ ì¦ê°•
                sample_info = f"Sample{i+1}(User{item['user_id']})"
            
            # ì¦ê°• ì ìš© (ì°¨ë³„í™”ëœ ê°•ë„)
            augmented_image = self._apply_differentiated_augmentation(
                base_image, 
                sample_info=sample_info,
                intensity=augmentation_intensity if index_count[idx] > 1 else "normal"
            )
            
            images.append(augmented_image)
            labels.append(item['user_id'])

        # 7. í†µê³„ ì¶œë ¥ ë° ê²€ì¦
        unique_users = len(set(labels))
        user_counts = {}
        for label in labels:
            user_counts[label] = user_counts.get(label, 0) + 1
        
        positive_pairs = sum(1 for count in user_counts.values() if count >= 2)
        artificial_pairs_count = sum(1 for idx, count in index_count.items() if count > 1)
        total_duplicates = len(labels) - len(set(selected_indices))
        
        print(f"[Sampling] ğŸ“Š Final batch composition:")
        print(f"   Total samples: {len(images)}")
        print(f"   Unique users: {unique_users}")
        print(f"   Users with â‰¥2 samples: {positive_pairs} (positive pairs)")
        print(f"   Natural positive pairs: {natural_pairs}")
        print(f"   Artificial positive pairs: {artificial_pairs_count}")
        print(f"   Total duplicates: {total_duplicates}")
        print(f"   User distribution: {dict(sorted(user_counts.items()))}")
        
        # 8. ğŸ”¥ Positive pair ë³´ì¥ ê²€ì¦
        if positive_pairs == 0:
            print(f"ğŸš¨ WARNING: No positive pairs in batch! This may cause SupCon loss issues.")
        else:
            print(f"âœ… Positive pairs guaranteed: {positive_pairs} users have multiple samples")
            print(f"âœ… SupCon Loss Warning should be eliminated!")
        
        return images, labels

    def sample(self, batch_size, **kwargs):
        """ê¸°ë³¸ ìƒ˜í”Œë§ ì¸í„°í˜ì´ìŠ¤"""
        return self.sample_with_replacement(batch_size, **kwargs)

    def get_diversity_stats(self):
        """ë²„í¼ ë‹¤ì–‘ì„± í†µê³„"""
        if len(self.image_storage) == 0:
            return {
                'total_samples': 0,
                'unique_users': 0,
                'user_distribution': {},
                'diversity_score': 0.0
            }
        
        user_counts = {}
        for item in self.image_storage:
            user_id = item['user_id']
            user_counts[user_id] = user_counts.get(user_id, 0) + 1
        
        unique_users = len(user_counts)
        diversity_score = unique_users / len(self.image_storage)
        
        return {
            'total_samples': len(self.image_storage),
            'unique_users': unique_users,
            'user_distribution': dict(sorted(user_counts.items())),
            'diversity_score': diversity_score
        }

    def save_state(self):
        """ìƒíƒœ ì €ì¥"""
        if self.faiss_index is None:
            return
            
        data_to_save = {
            'faiss_index_data': faiss.serialize_index(self.faiss_index),
            'metadata': self.metadata,
            'image_storage': self.image_storage,
            'stored_embeddings': self.stored_embeddings
        }
        
        with open(self.state_file, 'wb') as f:
            pickle.dump(data_to_save, f)
        
        diversity_stats = self.get_diversity_stats()
        print(f"[Buffer] ğŸ’¾ Saved {len(self.image_storage)} samples")
        print(f"[Buffer] Diversity: {diversity_stats['unique_users']} users, "
              f"{diversity_stats['diversity_score']:.2f} score")

    def _load_state(self):
        """ìƒíƒœ ë¡œë“œ"""
        if not self.state_file.exists():
            return

        try:
            with open(self.state_file, 'rb') as f:
                saved_data = pickle.load(f)
                
                self.faiss_index = faiss.deserialize_index(saved_data['faiss_index_data'])
                self.metadata = saved_data['metadata']
                self.image_storage = saved_data.get('image_storage', [])
                self.stored_embeddings = saved_data.get('stored_embeddings', [])
                
                diversity_stats = self.get_diversity_stats()
                print(f"[Buffer] âœ… Restored {len(self.image_storage)} samples")
                print(f"[Buffer] Diversity: {diversity_stats['unique_users']} users")
                
        except Exception as e:
            print(f"[Buffer] âŒ Failed to load state: {e}")
            self.faiss_index = None
            self.metadata = {}
            self.image_storage = []
            self.stored_embeddings = []

print("âœ… ReplayBuffer with Artificial Positive Pairs ì™„ë£Œ!")