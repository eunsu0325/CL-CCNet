# framework/losses.py - ì™„ì „ ë‹¨ìˆœí™”ëœ ë²„ì „
"""
CoCoNut Simplified Loss Functions

DESIGN PHILOSOPHY:
- Focus on Replay Buffer innovation only
- Use proven SupCon loss for stable continual learning
- Remove all W2ML complexity for clear paper contribution

STATUS: W2ML removed, basic SupCon only
"""

# ê¸°ì¡´ CCNetì˜ ê²€ì¦ëœ SupConLossë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
from loss import SupConLoss

# ê¸°ì¡´ ë³µì¡í•œ W2ML ê´€ë ¨ í´ë˜ìŠ¤ë“¤ ëª¨ë‘ ì œê±°:
# - CompleteW2MLSupConLoss (ì‚­ì œ)
# - DifficultyWeightedSupConLoss (ì‚­ì œ)
# - create_w2ml_loss() (ì‚­ì œ)
# - benchmark_faiss_w2ml_performance() (ì‚­ì œ)


def create_coconut_loss(temperature=0.07):
    """
    CoCoNutìš© ë‹¨ìˆœí•œ ì†ì‹¤í•¨ìˆ˜ ìƒì„±
    
    Args:
        temperature: SupCon ì˜¨ë„ íŒŒë¼ë¯¸í„°
        
    Returns:
        ê¸°ë³¸ SupConLoss ì¸ìŠ¤í„´ìŠ¤
    """
    return SupConLoss(temperature=temperature)


def get_coconut_loss_config():
    """
    CoCoNut ê¶Œì¥ ì†ì‹¤í•¨ìˆ˜ ì„¤ì •
    
    Returns:
        ë‹¨ìˆœí™”ëœ ì„¤ì • ë”•ì…”ë„ˆë¦¬
    """
    return {
        "continual_loss": {
            "type": "SupConLoss",
            "temperature": 0.07
        },
        "pretrain_loss": {
            "arcface_weight": 0.8,
            "supcon_weight": 0.2,
            "temperature": 0.07
        }
    }


if __name__ == "__main__":
    # ë‹¨ìˆœí•œ í…ŒìŠ¤íŠ¸
    print("ğŸ¥¥ CoCoNut Simplified Loss Functions")
    print("âœ… Using basic SupConLoss for continual learning")
    
    # ê¸°ë³¸ ì†ì‹¤í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
    loss_fn = create_coconut_loss()
    print(f"âœ… Created SupConLoss with temperature: {loss_fn.temperature}")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ê²€ì¦
    import torch
    batch_size = 8
    features = torch.randn(batch_size, 2, 512)  # [batch, views, feature_dim]
    labels = torch.randint(0, 4, (batch_size,))
    
    loss = loss_fn(features, labels)
    print(f"âœ… Test loss computed: {loss.item():.6f}")
    print("ğŸš€ Simplified loss functions ready!")