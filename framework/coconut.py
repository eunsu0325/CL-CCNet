# framework/coconut.py - CCNet ìŠ¤íƒ€ì¼ë¡œ ìˆ˜ì •ëœ ë²„ì „

"""
=== COCONUT STAGE 2: CONTINUAL LEARNING ===

ğŸ”¥ CCNet Style Implementation:
- Use both images from dataset pairs
- SupCon loss with proper multi-view format
- User Node system maintained
- Fixed NaN issues
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import copy
import json
import pickle
import time
from pathlib import Path
from tqdm import tqdm
import datetime
from collections import defaultdict
from typing import List, Dict, Tuple, Optional
import numpy as np

from models.ccnet_model import ccnet, HeadlessVerifier
from framework.replay_buffer import ReplayBuffer
from framework.losses import create_coconut_loss
from framework.user_node import UserNodeManager, UserNode
from datasets.palm_dataset import MyDataset
from torch.utils.data import DataLoader

class CoconutSystem:
    def __init__(self, config):
        """
        ë°°ì¹˜ ê¸°ë°˜ CoCoNut ì—°ì†í•™ìŠµ ì‹œìŠ¤í…œ - CCNet ìŠ¤íƒ€ì¼
        
        DESIGN:
        - SupCon loss with proper 2-view format
        - User Node based authentication
        - Even-count buffer management
        """
        print("="*80)
        print("ğŸ¥¥ COCONUT: CONTINUAL LEARNING (CCNet Style)")
        print("="*80)
        
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Configuration
        self.headless_mode = getattr(config.palm_recognizer, 'headless_mode', False)
        self.verification_method = getattr(config.palm_recognizer, 'verification_method', 'classification')
        self.metric_type = getattr(config.palm_recognizer, 'metric_type', 'cosine')
        self.similarity_threshold = getattr(config.palm_recognizer, 'similarity_threshold', 0.5)
        
        # Batch configuration
        cfg_learner = self.config.continual_learner
        self.training_batch_size = getattr(cfg_learner, 'training_batch_size', 32)
        self.hard_negative_ratio = getattr(cfg_learner, 'hard_negative_ratio', 0.3)
        self.samples_per_label = getattr(self.config.dataset, 'samples_per_label', 10)
        
        # User Node configuration
        self.user_node_config = getattr(config, 'user_node', None)
        self.user_nodes_enabled = self.user_node_config and self.user_node_config.enable_user_nodes
        
        # Loop Closure configuration
        self.loop_closure_config = getattr(config, 'loop_closure', None)
        self.loop_closure_enabled = self.loop_closure_config and self.loop_closure_config.enabled
        
        print(f"ğŸ”§ SYSTEM CONFIGURATION:")
        print(f"   Samples per label: {self.samples_per_label}")
        print(f"   Training batch size: {self.training_batch_size}")
        print(f"   Hard negative ratio: {self.hard_negative_ratio:.1%}")
        print(f"   Mode: {'Headless' if self.headless_mode else 'Classification'}")
        print(f"   ğŸ¯ User Nodes: {'ENABLED' if self.user_nodes_enabled else 'DISABLED'}")
        print(f"   ğŸ“Š Loss: SupCon (CCNet style)")
        print("="*80)
        
        # Checkpoint directory
        self.checkpoint_dir = Path('/content/drive/MyDrive/CL-CCNet_nodemode/checkpoints')
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize components
        self._initialize_models()
        self._initialize_replay_buffer()
        self._initialize_verification_system()
        self._initialize_optimizer()
        self._initialize_user_node_system()
        
        # Learning state
        self.global_step = 0
        self.processed_users = 0
        
        # Load checkpoint if exists
        self._load_checkpoint()
        
        print(f"[System] ğŸ¥¥ CoCoNut System ready!")
        print(f"[System] Starting from step: {self.global_step}")

    def _initialize_models(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        print(f"[System] Initializing models...")
        cfg_model = self.config.palm_recognizer
        
        compression_dim = getattr(cfg_model, 'compression_dim', 128)
        
        # Predictor (frozen after sync)
        self.predictor_net = ccnet(
            num_classes=cfg_model.num_classes,
            weight=cfg_model.com_weight,
            headless_mode=self.headless_mode,
            compression_dim=compression_dim
        ).to(self.device)
        
        # Learner (continuously updated)
        self.learner_net = ccnet(
            num_classes=cfg_model.num_classes,
            weight=cfg_model.com_weight,
            headless_mode=self.headless_mode,
            compression_dim=compression_dim
        ).to(self.device)
        
        self.feature_dimension = compression_dim if self.headless_mode else 2048
        
        # Load pretrained weights
        weights_path = cfg_model.load_weights_folder
        if Path(weights_path).exists():
            print(f"[System] Loading pretrained weights from: {weights_path}")
            state_dict = torch.load(weights_path, map_location=self.device)
            
            if self.headless_mode:
                # Remove classification head
                filtered_state_dict = {k: v for k, v in state_dict.items() 
                                     if not k.startswith('arclayer_')}
                self.predictor_net.load_state_dict(filtered_state_dict, strict=False)
                self.learner_net.load_state_dict(filtered_state_dict, strict=False)
            else:
                self.predictor_net.load_state_dict(state_dict)
                self.learner_net.load_state_dict(state_dict)
                
            print(f"[System] âœ… Weights loaded successfully")
        
        self.predictor_net.eval()
        self.learner_net.train()

    def _initialize_replay_buffer(self):
        """ë¦¬í”Œë ˆì´ ë²„í¼ ì´ˆê¸°í™”"""
        print("[System] Initializing replay buffer...")
        cfg_buffer = self.config.replay_buffer
        
        self.replay_buffer = ReplayBuffer(
            config=cfg_buffer,
            storage_dir=Path(cfg_buffer.storage_path),
            feature_dimension=self.feature_dimension
        )
        
        # Set feature extractor
        self.replay_buffer.set_feature_extractor(self.learner_net)
        
        # Update hard negative ratio
        self.replay_buffer.update_hard_negative_ratio(self.hard_negative_ratio)

    def _initialize_verification_system(self):
        """ê²€ì¦ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        if self.verification_method == 'metric':
            self.verifier = HeadlessVerifier(
                metric_type=self.metric_type,
                threshold=self.similarity_threshold
            )
            print(f"[System] âœ… Metric verifier initialized")
        else:
            self.verifier = None

    def _initialize_optimizer(self):
        """ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”"""
        cfg_model = self.config.palm_recognizer
        cfg_loss = self.config.loss
        
        self.optimizer = optim.Adam(
            self.learner_net.parameters(), 
            lr=cfg_model.learning_rate
        )
        
        # ì†ì‹¤ í•¨ìˆ˜ (SupCon)
        self.criterion = create_coconut_loss(cfg_loss.__dict__)
        
        print(f"[System] âœ… Optimizer initialized (lr: {cfg_model.learning_rate})")
        print(f"[System] âœ… Loss: SupCon (CCNet style)")

    def _initialize_user_node_system(self):
        """ì‚¬ìš©ì ë…¸ë“œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        if self.user_nodes_enabled:
            print("[System] Initializing User Node system...")
            
            # UserNodeManager ìƒì„±
            node_config = self.user_node_config.__dict__.copy()
            node_config.pop('config_file', None)
            node_config['feature_dimension'] = self.feature_dimension
            
            self.node_manager = UserNodeManager(
                config=node_config,
                device=self.device
            )
            
            print(f"[System] âœ… User Node system initialized")
        else:
            self.node_manager = None
            print("[System] âš ï¸ User Node system is DISABLED")

    def _prepare_registration_image(self, sample_tensor):
        """ë“±ë¡ ì´ë¯¸ì§€ ì¤€ë¹„"""
        try:
            # í…ì„œë¥¼ numpyë¡œ ë³€í™˜
            image_np = sample_tensor.cpu().numpy()
            
            # í˜•íƒœ í™•ì¸ ë° ë³€í™˜
            if len(image_np.shape) == 3:
                # (C, H, W) -> (H, W, C)
                if image_np.shape[0] in [1, 3]:
                    image_np = image_np.transpose(1, 2, 0)
            
            # ê°’ ë²”ìœ„ ì •ê·œí™” (0-1 -> 0-255)
            if image_np.dtype in [np.float32, np.float64]:
                if image_np.max() <= 1.0:
                    image_np = (image_np * 255).astype(np.uint8)
                else:
                    image_np = image_np.astype(np.uint8)
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ì²˜ë¦¬
            if len(image_np.shape) == 3 and image_np.shape[2] == 1:
                image_np = image_np.squeeze(2)
            
            return image_np
            
        except Exception as e:
            print(f"[System] âŒ Error preparing registration image: {e}")
            # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
            dummy_image = np.full((128, 128), 128, dtype=np.uint8)
            return dummy_image

    def process_label_batch(self, sample_pairs: List[Tuple[torch.Tensor, torch.Tensor]], 
                        user_id: int,
                        raw_images: List[Tuple[np.ndarray, np.ndarray]] = None):
        """
        ë°°ì¹˜ ë‹¨ìœ„ ì²˜ë¦¬ - CCNet ìŠ¤íƒ€ì¼
        
        Args:
            sample_pairs: [(img1, img2), ...] í˜•íƒœì˜ ì •ê·œí™”ëœ ì´ë¯¸ì§€ í˜ì–´
            user_id: ì‚¬ìš©ì ID
            raw_images: [(raw1, raw2), ...] í˜•íƒœì˜ ì›ë³¸ ì´ë¯¸ì§€ í˜ì–´
        """
        print(f"\n[Process] ğŸ¯ Processing batch for User {user_id} ({len(sample_pairs)} pairs)")
        
        # 1. í›ˆë ¨ ë°°ì¹˜ êµ¬ì„±
        training_batch = self._construct_training_batch(
            sample_pairs=sample_pairs,
            user_id=user_id
        )
        
        # 2. í•™ìŠµ ìˆ˜í–‰
        adaptation_epochs = self.config.continual_learner.adaptation_epochs
        
        for epoch in range(adaptation_epochs):
            print(f"[Epoch {epoch+1}/{adaptation_epochs}]")
            loss_dict = self._train_step_ccnet_style(training_batch)
            
            # NaN ì²´í¬
            if torch.isnan(torch.tensor(loss_dict['total'])):
                print(f"   âš ï¸ NaN detected! Skipping this batch.")
                return {'stored': 0, 'total': len(sample_pairs)}
            
            print(f"   Loss: {loss_dict['total']:.4f}")
        
        # 3. ì‚¬ìš©ì ë…¸ë“œ ìƒì„±/ì—…ë°ì´íŠ¸
        if self.user_nodes_enabled and self.node_manager:
            # ëª¨ë“  ì´ë¯¸ì§€ì˜ íŠ¹ì§• ì¶”ì¶œ
            all_embeddings = []
            all_normalized_tensors = []  # ì •ê·œí™”ëœ í…ì„œ ìˆ˜ì§‘
            
            for img1, img2 in sample_pairs:
                emb1 = self._extract_feature(img1)
                emb2 = self._extract_feature(img2)
                all_embeddings.extend([emb1, emb2])
                
                # ì •ê·œí™”ëœ í…ì„œë„ ì €ì¥ (Loop Closureìš©)
                all_normalized_tensors.extend([img1.cpu(), img2.cpu()])
            
            final_embeddings = torch.stack(all_embeddings)  # [20, feature_dim]
            
            # ì›ë³¸ ì´ë¯¸ì§€ëŠ” ì‹œê°í™”ìš©
            registration_image = None
            if raw_images and len(raw_images) > 0:
                registration_image = raw_images[0][0]  # ì²« ë²ˆì§¸ ì›ë³¸ ì´ë¯¸ì§€
            
            # User Node ì—…ë°ì´íŠ¸ (ì •ê·œí™”ëœ í…ì„œ í¬í•¨)
            self.node_manager.add_user(
                user_id, 
                final_embeddings, 
                registration_image=registration_image,
                normalized_tensors=all_normalized_tensors  # Loop Closureìš©
            )
        
        # 4. ì„ ë³„ì  ë²„í¼ ì €ì¥ (ì§ìˆ˜ ìœ ì§€)
        stored_count = self._store_to_buffer_even(sample_pairs, user_id)
        
        # 5. í†µê³„ ì—…ë°ì´íŠ¸
        self.global_step += 1
        self.processed_users += 1
        
        # 6. ì£¼ê¸°ì  ë™ê¸°í™”
        if self.global_step % self.config.continual_learner.sync_frequency == 0:
            self._sync_weights()
        
        # 7. Loop Closure ì²´í¬ (ì˜µì…˜)
        if self.loop_closure_enabled and self.global_step % 10 == 0:
            self._check_loop_closure()
        
        print(f"[Process] âœ… Completed: stored={stored_count}/{len(sample_pairs)*2}")
        
        return {
            'stored': stored_count,
            'total': len(sample_pairs) * 2
        }
    
    def _check_loop_closure(self):
        """Loop Closure ì²´í¬ ë° ì‹¤í–‰"""
        if not self.node_manager:
            return
        
        print("\n[Loop Closure] Checking for candidates...")
        
        # Loop Closure í›„ë³´ ì°¾ê¸°
        candidates = self.node_manager.get_loop_closure_candidates(
            similarity_threshold=0.8
        )
        
        if not candidates:
            print("[Loop Closure] No candidates found")
            return
        
        print(f"[Loop Closure] Found {len(candidates)} candidate pairs")
        
        # ìƒìœ„ 2ê°œë§Œ ì²˜ë¦¬ (ì‹œê°„ ì ˆì•½)
        max_pairs = 2
        for user1, user2, similarity in candidates[:max_pairs]:
            print(f"[Loop Closure] Processing pair: User {user1} <-> User {user2} (sim: {similarity:.3f})")
            
            # ë‘ ì‚¬ìš©ìì˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            loop_data = self.node_manager.get_loop_closure_data([user1, user2])
            
            if user1 in loop_data and user2 in loop_data:
                # ì •ê·œí™”ëœ í…ì„œë“¤ë¡œ ì¬í•™ìŠµ
                _, tensors1 = loop_data[user1]
                _, tensors2 = loop_data[user2]
                
                # ì¬í•™ìŠµì„ ìœ„í•œ ë°°ì¹˜ êµ¬ì„±
                combined_pairs = []
                for t in tensors1[:3]:  # ê° ì‚¬ìš©ìì—ì„œ ìµœëŒ€ 3ê°œ
                    combined_pairs.append((t, t))  # ê°™ì€ ì´ë¯¸ì§€ë¡œ í˜ì–´ êµ¬ì„±
                for t in tensors2[:3]:
                    combined_pairs.append((t, t))
                
                # ì¬í•™ìŠµ ì‹¤í–‰
                if combined_pairs:
                    print(f"[Loop Closure] Retraining with {len(combined_pairs)} pairs")
                    training_batch = self._construct_training_batch(
                        sample_pairs=combined_pairs,
                        user_id=-1  # íŠ¹ë³„í•œ IDë¡œ Loop Closure í‘œì‹œ
                    )
                    
                    # 1 epochë§Œ í•™ìŠµ
                    loss_dict = self._train_step_ccnet_style(training_batch)
                    print(f"[Loop Closure] Loss: {loss_dict['total']:.4f}")
                    
    def _train_step_ccnet_style(self, batch_data: Dict) -> Dict[str, torch.Tensor]:
        """CCNet ìŠ¤íƒ€ì¼ í•™ìŠµ ìŠ¤í…"""
        sample_pairs = batch_data['sample_pairs']  # [(img1, img2), ...]
        buffer_samples = batch_data['buffer_samples']  # [(img, label), ...]
        
        if not sample_pairs and not buffer_samples:
            return {'total': 0.0, 'supcon': 0.0}
        
        self.learner_net.train()
        self.optimizer.zero_grad()
        
        # CCNet ìŠ¤íƒ€ì¼ë¡œ íŠ¹ì§• ì¶”ì¶œ
        features_list = []
        labels_list = []
        
        # 1. ìƒˆ ì‚¬ìš©ìì˜ í˜ì–´ë“¤ ì²˜ë¦¬
        for (img1, img2), label in sample_pairs:
            # ê° ì´ë¯¸ì§€ì—ì„œ íŠ¹ì§• ì¶”ì¶œ
            img1_tensor = img1.to(self.device)
            img2_tensor = img2.to(self.device)
            
            if len(img1_tensor.shape) == 3:
                img1_tensor = img1_tensor.unsqueeze(0)
            if len(img2_tensor.shape) == 3:
                img2_tensor = img2_tensor.unsqueeze(0)
            
            _, feat1 = self.learner_net(img1_tensor)
            _, feat2 = self.learner_net(img2_tensor)
            
            # [2, feature_dim] í˜•íƒœë¡œ ë¬¶ê¸°
            paired_features = torch.stack([feat1.squeeze(0), feat2.squeeze(0)], dim=0)
            features_list.append(paired_features)
            labels_list.append(label)
        
        # 2. ë²„í¼ ìƒ˜í”Œë“¤ì„ ë¼ë²¨ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ í˜ì–´ ë§Œë“¤ê¸°
        if buffer_samples:
            label_groups = defaultdict(list)
            for img, lbl in buffer_samples:
                label_groups[lbl].append(img)
            
            # ê° ë¼ë²¨ì—ì„œ ì§ìˆ˜ê°œì”© ì„ íƒí•˜ì—¬ í˜ì–´ êµ¬ì„±
            for lbl, imgs in label_groups.items():
                # ì§ìˆ˜ê°œë¡œ ë§Œë“¤ê¸°
                num_imgs = len(imgs)
                if num_imgs >= 2:
                    # ì§ìˆ˜ê°œë§Œ ì‚¬ìš©
                    for i in range(0, num_imgs - 1, 2):
                        img1_tensor = imgs[i].to(self.device)
                        img2_tensor = imgs[i+1].to(self.device)
                        
                        if len(img1_tensor.shape) == 3:
                            img1_tensor = img1_tensor.unsqueeze(0)
                        if len(img2_tensor.shape) == 3:
                            img2_tensor = img2_tensor.unsqueeze(0)
                        
                        _, feat1 = self.learner_net(img1_tensor)
                        _, feat2 = self.learner_net(img2_tensor)
                        
                        paired_features = torch.stack([feat1.squeeze(0), feat2.squeeze(0)], dim=0)
                        features_list.append(paired_features)
                        labels_list.append(lbl)
        
        if not features_list:
            return {'total': 0.0, 'supcon': 0.0}
        
        # [batch_size, 2, feature_dim] í˜•íƒœë¡œ ìŠ¤íƒ
        features_tensor = torch.stack(features_list)
        labels_tensor = torch.tensor(labels_list, dtype=torch.long, device=self.device)
        
        print(f"[Train] Batch shape: {features_tensor.shape}, Labels: {labels_tensor.shape}")
        
        # SupCon Loss ê³„ì‚°
        loss_dict = self.criterion(features_tensor, labels_tensor)
        
        # Backward with gradient clipping
        loss_dict['total'].backward()
        torch.nn.utils.clip_grad_norm_(self.learner_net.parameters(), max_norm=1.0)
        self.optimizer.step()
        
        return {k: v.item() if torch.is_tensor(v) else v for k, v in loss_dict.items()}

    def _construct_training_batch(self, sample_pairs: List[Tuple], user_id: int) -> Dict:
        """CCNet ìŠ¤íƒ€ì¼ ë°°ì¹˜ êµ¬ì„±"""
        
        # ìƒˆ ì‚¬ìš©ìì˜ í˜ì–´ë“¤
        new_pairs = [(pair, user_id) for pair in sample_pairs]
        
        # ë²„í¼ì—ì„œ ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸°
        # ëª©í‘œ: ì „ì²´ ë°°ì¹˜ê°€ ì ì ˆí•œ í¬ê¸°ê°€ ë˜ë„ë¡
        num_new_samples = len(sample_pairs) * 2  # ê° í˜ì–´ëŠ” 2ê°œ ì´ë¯¸ì§€
        buffer_samples_needed = max(0, self.training_batch_size - num_new_samples)
        
        # ì§ìˆ˜ë¡œ ë§ì¶”ê¸° (í˜ì–´ë¥¼ ë§Œë“¤ê¸° ìœ„í•´)
        if buffer_samples_needed % 2 == 1:
            buffer_samples_needed += 1
        
        print(f"[Batch] Constructing training batch:")
        print(f"   New pairs: {len(sample_pairs)} ({num_new_samples} images)")
        print(f"   Buffer samples needed: {buffer_samples_needed}")
        
        buffer_samples = []
        if buffer_samples_needed > 0:
            buffer_images, buffer_labels = self.replay_buffer.sample_for_training_even(
                num_samples=buffer_samples_needed,
                current_user_id=user_id
            )
            
            buffer_samples = list(zip(buffer_images, buffer_labels))
            print(f"   Buffer samples retrieved: {len(buffer_samples)}")
        
        print(f"[Batch] Final composition: {len(new_pairs)} pairs + {len(buffer_samples)} buffer samples")
        
        return {
            'sample_pairs': new_pairs,
            'buffer_samples': buffer_samples
        }

    def _store_to_buffer_even(self, sample_pairs: List[Tuple], user_id: int) -> int:
        """ë²„í¼ì— ì§ìˆ˜ê°œë¡œ ì €ì¥"""
        stored_count = 0
        user_embeddings = []
        user_images = []
        
        # ëª¨ë“  ì´ë¯¸ì§€ì™€ ì„ë² ë”© ìˆ˜ì§‘
        for img1, img2 in sample_pairs:
            emb1 = self._extract_feature(img1)
            emb2 = self._extract_feature(img2)
            
            user_embeddings.extend([emb1, emb2])
            user_images.extend([img1, img2])
        
        # ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚°
        diversity_scores = []
        for i, (img, emb) in enumerate(zip(user_images, user_embeddings)):
            # ë‹¤ë¥¸ ì„ë² ë”©ë“¤ê³¼ì˜ í‰ê·  ìœ ì‚¬ë„
            similarities = []
            for j, other_emb in enumerate(user_embeddings):
                if i != j:
                    sim = F.cosine_similarity(emb.unsqueeze(0), other_emb.unsqueeze(0)).item()
                    similarities.append(sim)
            avg_sim = np.mean(similarities) if similarities else 0
            diversity_scores.append((i, avg_sim, img, emb))
        
        # ë‹¤ì–‘ì„±ì´ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬ (ìœ ì‚¬ë„ê°€ ë‚®ì€ ìˆœ)
        diversity_scores.sort(key=lambda x: x[1])
        
        # ì§ìˆ˜ê°œë§Œ ì €ì¥ (ìµœëŒ€ samples_per_user_limitê¹Œì§€)
        max_to_store = min(len(diversity_scores), self.replay_buffer.samples_per_user_limit)
        if max_to_store % 2 == 1:
            max_to_store -= 1  # ì§ìˆ˜ë¡œ ë§Œë“¤ê¸°
        
        for i in range(max_to_store):
            idx, sim, img, emb = diversity_scores[i]
            if self.replay_buffer.add_sample_direct(img, user_id, emb):
                stored_count += 1
        
        return stored_count

    def _extract_feature(self, image: torch.Tensor) -> torch.Tensor:
        """ë‹¨ì¼ ì´ë¯¸ì§€ì—ì„œ íŠ¹ì§• ì¶”ì¶œ"""
        self.learner_net.eval()
        
        with torch.no_grad():
            if len(image.shape) == 3:
                image = image.unsqueeze(0)
            image = image.to(self.device)
            features = self.learner_net.getFeatureCode(image)
        
        self.learner_net.train()
        return features.squeeze(0)

    def _extract_batch_features(self, samples: List[torch.Tensor]) -> torch.Tensor:
        """ë°°ì¹˜ íŠ¹ì§• ì¶”ì¶œ"""
        self.learner_net.eval()
        
        with torch.no_grad():
            # Stack all samples into a batch
            batch = torch.stack([s.to(self.device) for s in samples])
            
            # Extract features in one forward pass
            features = self.learner_net.getFeatureCode(batch)
        
        self.learner_net.train()
        return features

    def _sync_weights(self):
        """ê°€ì¤‘ì¹˜ ë™ê¸°í™”"""
        self.predictor_net.load_state_dict(self.learner_net.state_dict())
        self.predictor_net.eval()
        
        print(f"\n[Sync] ğŸ”„ Weights synchronized at step {self.global_step}")

    def verify_user(self, probe_image: torch.Tensor, top_k: int = 10) -> Dict:
        """ì‚¬ìš©ì ì¸ì¦"""
        if not self.node_manager:
            return {
                'is_match': False,
                'error': 'No node manager available'
            }
        
        start_time = time.time()
        
        # 1. í”„ë¡œë¸Œ ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ
        self.predictor_net.eval()
        with torch.no_grad():
            if len(probe_image.shape) == 3:
                probe_image = probe_image.unsqueeze(0)
            probe_image = probe_image.to(self.device)
            probe_feature = self.predictor_net.getFeatureCode(probe_image).squeeze(0)
        
        # 2. User Node Managerë¥¼ í†µí•œ ì¸ì¦
        auth_result = self.node_manager.verify_user(probe_feature, top_k=top_k)
        
        # 3. ê²°ê³¼ì— ì¶”ê°€ ì •ë³´ í¬í•¨
        auth_result['computation_time'] = time.time() - start_time
        
        return auth_result

    def run_experiment(self):
        """ë°°ì¹˜ ê¸°ë°˜ ì‹¤í—˜ ì‹¤í–‰ - CCNet ìŠ¤íƒ€ì¼"""
        print(f"\n[System] Starting CCNet-style continual learning...")
        
        # Load dataset with return_raw=True for raw images
        cfg_dataset = self.config.dataset
        dataset = MyDataset(txt=str(cfg_dataset.train_set_file), train=False, return_raw=True)  # ğŸ”¥ FIX
        
        # Group data by label
        grouped_data = self._group_data_by_label(dataset)
        total_users = len(grouped_data)
        
        print(f"[System] Dataset loaded: {total_users} users")
        print(f"[System] Processing {self.samples_per_label} samples per user")
        print(f"[System] Using 2 images per sample (CCNet style)")
        
        # Process each user's batch
        for user_id, user_indices in tqdm(grouped_data.items(), desc="Batch Processing"):
            # Skip if already processed
            if self.processed_users > 0 and user_id in self._get_processed_user_ids():
                continue
            
            # Get sample pairs with raw images
            sample_pairs = []
            raw_images = []
            
            for idx in user_indices[:self.samples_per_label]:
                data, _, raw_data = dataset[idx]  # ì›ë³¸ë„ ë°›ìŒ
                sample_pairs.append((data[0], data[1]))
                raw_images.append((raw_data[0], raw_data[1]))
            
            if len(sample_pairs) == self.samples_per_label:
                # Process batch with raw images
                results = self.process_label_batch(sample_pairs, user_id, raw_images)
                
                # Save checkpoint periodically
                if self.global_step % self.config.continual_learner.intermediate_save_frequency == 0:
                    self._save_checkpoint()
        
        # Final save
        print(f"\n[System] Experiment completed!")
        self._save_checkpoint()
        self._save_final_model()
        
        # End-to-End í‰ê°€ ì‹¤í–‰
        if hasattr(self.config.dataset, 'test_set_file'):
            print("\n[System] Running End-to-End evaluation...")
            self.run_evaluation()

    def _group_data_by_label(self, dataset) -> Dict[int, List[int]]:
        """ë°ì´í„°ë¥¼ ë¼ë²¨ë³„ë¡œ ê·¸ë£¹í™”"""
        grouped = defaultdict(list)
        
        for idx in range(len(dataset)):
            if dataset.return_raw:
                _, label, _ = dataset[idx]
            else:
                _, label = dataset[idx]
            user_id = label.item() if torch.is_tensor(label) else label
            grouped[user_id].append(idx)
        
        return dict(grouped)

    def _get_processed_user_ids(self) -> set:
        """ì´ë¯¸ ì²˜ë¦¬ëœ ì‚¬ìš©ì ID ë°˜í™˜"""
        if self.node_manager and self.user_nodes_enabled:
            return set(self.node_manager.nodes.keys())
        return set()

    def _save_checkpoint(self):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint = {
            'global_step': self.global_step,
            'processed_users': self.processed_users,
            'learner_state_dict': self.learner_net.state_dict(),
            'predictor_state_dict': self.predictor_net.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'config': {
                'training_batch_size': self.training_batch_size,
                'hard_negative_ratio': self.hard_negative_ratio,
                'samples_per_label': self.samples_per_label,
                'headless_mode': self.headless_mode,
                'user_nodes_enabled': self.user_nodes_enabled
            }
        }
        
        checkpoint_path = self.checkpoint_dir / f'checkpoint_step_{self.global_step}.pth'
        torch.save(checkpoint, checkpoint_path)
        
        # Also save buffer state
        self.replay_buffer._save_state()
        
        # Save user nodes
        if self.node_manager and self.user_nodes_enabled:
            self.node_manager.save_nodes()
        
        print(f"[Checkpoint] ğŸ’¾ Saved at step {self.global_step}")

    def _load_checkpoint(self):
        """ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        checkpoint_files = list(self.checkpoint_dir.glob('checkpoint_step_*.pth'))
        
        if not checkpoint_files:
            print("[Checkpoint] No checkpoint found, starting fresh")
            return
        
        latest = max(checkpoint_files, key=lambda x: int(x.stem.split('_')[-1]))
        
        print(f"[Checkpoint] Loading from: {latest.name}")
        checkpoint = torch.load(latest, map_location=self.device)
        
        self.global_step = checkpoint['global_step']
        self.processed_users = checkpoint['processed_users']
        
        self.learner_net.load_state_dict(checkpoint['learner_state_dict'])
        self.predictor_net.load_state_dict(checkpoint['predictor_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        print(f"[Checkpoint] âœ… Resumed from step {self.global_step}")

    def _save_final_model(self):
        """ìµœì¢… ëª¨ë¸ ì €ì¥"""
        save_path = Path(self.config.model_saving.final_save_path)
        save_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Save learner
        learner_path = save_path / f'coconut_learner_{timestamp}.pth'
        torch.save(self.learner_net.state_dict(), learner_path)
        
        # Save predictor
        predictor_path = save_path / f'coconut_predictor_{timestamp}.pth'
        torch.save(self.predictor_net.state_dict(), predictor_path)
        
        # Save metadata
        metadata = {
            'total_steps': self.global_step,
            'total_users': self.processed_users,
            'training_batch_size': self.training_batch_size,
            'hard_negative_ratio': self.hard_negative_ratio,
            'samples_per_label': self.samples_per_label,
            'headless_mode': self.headless_mode,
            'user_nodes_enabled': self.user_nodes_enabled,
            'loss_type': 'SupCon (CCNet style)',
            'buffer_stats': self.replay_buffer.get_statistics()
        }
        
        # Add user node statistics
        if self.node_manager and self.user_nodes_enabled:
            metadata['user_node_stats'] = self.node_manager.get_statistics()
        
        metadata_path = save_path / f'coconut_metadata_{timestamp}.json'
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"[System] âœ… Final models saved to: {save_path}")
        print(f"  ğŸ“ Learner: {learner_path.name}")
        print(f"  ğŸ“ Predictor: {predictor_path.name}")
        print(f"  ğŸ“ Metadata: {metadata_path.name}")

    def run_evaluation(self):
        """End-to-End í‰ê°€ ì‹¤í–‰"""
        try:
            from evaluation.eval_utils import CoconutEvaluator
            
            test_file = getattr(self.config.dataset, 'test_set_file', None)
            if not test_file:
                print("âš ï¸ No test file specified in config")
                return None
            
            print("\n" + "="*80)
            print("ğŸ” Starting End-to-End Authentication Evaluation")
            print("="*80)
            
            # í‰ê°€ê¸° ìƒì„±
            evaluator = CoconutEvaluator(
                model=self.predictor_net,
                node_manager=self.node_manager,
                device=self.device
            )
            
            # í‰ê°€ ì‹¤í–‰
            results = evaluator.run_end_to_end_evaluation(
                test_file_path=test_file,
                batch_size=32,
                save_results=True,
                output_dir="./evaluation_results"
            )
            
            return results
            
        except ImportError as e:
            print(f"âš ï¸ Evaluation module not found: {e}")
            print("ğŸ“ Skipping end-to-end evaluation")
            return None
        except Exception as e:
            print(f"âŒ Error during evaluation: {e}")
            return None