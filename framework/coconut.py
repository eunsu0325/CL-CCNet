# framework/coconut.py - ì‚¬ìš©ì ë…¸ë“œ ì‹œìŠ¤í…œ í†µí•© ë²„ì „

"""
=== COCONUT STAGE 2: USER NODE BASED CONTINUAL LEARNING ===

ğŸ”¥ MAJOR UPDATES:
- User Node system integration
- Loop Closure mechanism with PQ restoration
- Mahalanobis + SupCon alternate training
- ON/OFF modular design
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import copy
import json
import pickle
import time
from pathlib import Path
from tqdm import tqdm
import datetime
from collections import defaultdict
from typing import List, Dict, Tuple, Optional
import numpy as np
import random

from models.ccnet_model import ccnet, HeadlessVerifier
from framework.replay_buffer import SimplifiedReplayBuffer
from framework.losses import CombinedContrastiveLoss, create_coconut_loss
from framework.user_node import UserNodeManager, UserNode
from datasets.palm_dataset import MyDataset
from torch.utils.data import DataLoader

class BatchCoconutSystem:
    def __init__(self, config):
        """
        ë°°ì¹˜ ê¸°ë°˜ CoCoNut ì—°ì†í•™ìŠµ ì‹œìŠ¤í…œ with User Nodes
        
        DESIGN:
        - User Node based authentication
        - Loop Closure self-correction with PQ restoration
        - Alternate training (SupCon â†” Mahalanobis)
        - Modular ON/OFF design
        """
        print("="*80)
        print("ğŸ¥¥ COCONUT STAGE 2: USER NODE BASED CONTINUAL LEARNING")
        print("="*80)
        
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Configuration
        self.headless_mode = getattr(config.palm_recognizer, 'headless_mode', False)
        self.verification_method = getattr(config.palm_recognizer, 'verification_method', 'classification')
        self.metric_type = getattr(config.palm_recognizer, 'metric_type', 'cosine')
        self.similarity_threshold = getattr(config.palm_recognizer, 'similarity_threshold', 0.5)
        
        # Batch configuration
        cfg_learner = self.config.continual_learner
        self.training_batch_size = getattr(cfg_learner, 'training_batch_size', 32)
        self.hard_negative_ratio = getattr(cfg_learner, 'hard_negative_ratio', 0.3)
        self.samples_per_label = getattr(self.config.dataset, 'samples_per_label', 5)
        
        # ğŸ”¥ User Node configuration
        self.user_node_config = getattr(config, 'user_node', None)
        self.user_nodes_enabled = self.user_node_config and self.user_node_config.enable_user_nodes
        
        # ğŸ”¥ Loop Closure configuration
        self.loop_closure_config = getattr(config, 'loop_closure', None)
        self.loop_closure_enabled = (self.loop_closure_config and 
                                    self.loop_closure_config.enabled and 
                                    self.user_nodes_enabled)  # ìë™ OFF
        
        print(f"ğŸ”§ SYSTEM CONFIGURATION:")
        print(f"   Samples per label: {self.samples_per_label}")
        print(f"   Training batch size: {self.training_batch_size}")
        print(f"   Hard negative ratio: {self.hard_negative_ratio:.1%}")
        print(f"   Mode: {'Headless' if self.headless_mode else 'Classification'}")
        print(f"   ğŸ¯ User Nodes: {'ENABLED' if self.user_nodes_enabled else 'DISABLED'}")
        print(f"   ğŸ”„ Loop Closure: {'ENABLED' if self.loop_closure_enabled else 'DISABLED'}")
        print("="*80)
        
        # Checkpoint directory
        self.checkpoint_dir = Path('/content/drive/MyDrive/CoCoNut_BatchMode/checkpoints')
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize components
        self._initialize_models()
        self._initialize_replay_buffer()
        self._initialize_verification_system()
        self._initialize_optimizer()
        self._initialize_user_node_system()  # ğŸ”¥ NEW
        
        # Learning state
        self.global_step = 0
        self.processed_users = 0
        
        # ğŸ”¥ Loop Closure statistics
        self.loop_closure_stats = {
            'total_collisions': 0,
            'resolved_collisions': 0,
            'failed_resolutions': 0,
            'total_resolution_time': 0.0
        }
        
        # Load checkpoint if exists
        self._load_checkpoint()
        
        print(f"[System] ğŸ¥¥ Enhanced CoCoNut System ready!")
        print(f"[System] Starting from step: {self.global_step}")

    def _initialize_models(self):
        """ëª¨ë¸ ì´ˆê¸°í™” (ê¸°ì¡´ê³¼ ë™ì¼)"""
        print(f"[System] Initializing models...")
        cfg_model = self.config.palm_recognizer
        
        compression_dim = getattr(cfg_model, 'compression_dim', 128)
        
        # Predictor (frozen after sync)
        self.predictor_net = ccnet(
            num_classes=cfg_model.num_classes,
            weight=cfg_model.com_weight,
            headless_mode=self.headless_mode,
            compression_dim=compression_dim
        ).to(self.device)
        
        # Learner (continuously updated)
        self.learner_net = ccnet(
            num_classes=cfg_model.num_classes,
            weight=cfg_model.com_weight,
            headless_mode=self.headless_mode,
            compression_dim=compression_dim
        ).to(self.device)
        
        self.feature_dimension = compression_dim if self.headless_mode else 2048
        
        # Load pretrained weights
        weights_path = cfg_model.load_weights_folder
        if Path(weights_path).exists():
            print(f"[System] Loading pretrained weights from: {weights_path}")
            state_dict = torch.load(weights_path, map_location=self.device)
            
            if self.headless_mode:
                # Remove classification head
                filtered_state_dict = {k: v for k, v in state_dict.items() 
                                     if not k.startswith('arclayer_')}
                self.predictor_net.load_state_dict(filtered_state_dict, strict=False)
                self.learner_net.load_state_dict(filtered_state_dict, strict=False)
            else:
                self.predictor_net.load_state_dict(state_dict)
                self.learner_net.load_state_dict(state_dict)
                
            print(f"[System] âœ… Weights loaded successfully")
        
        self.predictor_net.eval()
        self.learner_net.train()

    def _initialize_replay_buffer(self):
        """ë¦¬í”Œë ˆì´ ë²„í¼ ì´ˆê¸°í™” (ê¸°ì¡´ê³¼ ë™ì¼)"""
        print("[System] Initializing replay buffer...")
        cfg_buffer = self.config.replay_buffer
        
        self.replay_buffer = SimplifiedReplayBuffer(
            config=cfg_buffer,
            storage_dir=Path(cfg_buffer.storage_path),
            feature_dimension=self.feature_dimension
        )
        
        # Set feature extractor
        self.replay_buffer.set_feature_extractor(self.learner_net)
        
        # Update hard negative ratio
        self.replay_buffer.update_hard_negative_ratio(self.hard_negative_ratio)

    def _initialize_verification_system(self):
        """ê²€ì¦ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        if self.verification_method == 'metric':
            self.verifier = HeadlessVerifier(
                metric_type=self.metric_type,
                threshold=self.similarity_threshold
            )
            print(f"[System] âœ… Metric verifier initialized")
        else:
            self.verifier = None

    def _initialize_optimizer(self):
        """ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™” - ìˆ˜ì •ë¨"""
        cfg_model = self.config.palm_recognizer
        cfg_loss = self.config.loss
        
        self.optimizer = optim.Adam(
            self.learner_net.parameters(), 
            lr=cfg_model.learning_rate
        )
        
        # ğŸ”¥ Combined Loss ì‚¬ìš©
        self.criterion = create_coconut_loss(cfg_loss.__dict__)
        
        print(f"[System] âœ… Optimizer initialized (lr: {cfg_model.learning_rate})")

    def _initialize_user_node_system(self):
        """ğŸ”¥ ì‚¬ìš©ì ë…¸ë“œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        if self.user_nodes_enabled:
            print("[System] Initializing User Node system...")
            
            # UserNodeManager ìƒì„±
            node_config = self.user_node_config.__dict__.copy()
            node_config.pop('config_file', None)  # config_file ì œê±°
            
            self.node_manager = UserNodeManager(
                config=node_config,
                device=self.device
            )
            
            print(f"[System] âœ… User Node system initialized")
        else:
            self.node_manager = None
            print("[System] âš ï¸ User Node system is DISABLED")

    def process_label_batch(self, samples: List[torch.Tensor], user_id: int):
        """
        ë°°ì¹˜ ë‹¨ìœ„ ì²˜ë¦¬ - ì‚¬ìš©ì ë…¸ë“œ í†µí•© ë²„ì „ with Full Loop Closure
        
        Args:
            samples: í•œ ë¼ë²¨ì˜ ëª¨ë“  ìƒ˜í”Œë“¤
            user_id: ì‚¬ìš©ì ID
        """
        print(f"\n[Process] ğŸ¯ Processing batch for User {user_id} ({len(samples)} samples)")
        
        # ğŸ”¥ 1. Loop Closure ì²´í¬
        loop_closure_triggered = False
        collision_user = None
        collision_distance = None
        
        if self.loop_closure_enabled and self.node_manager:
            with torch.no_grad():
                # ì²« ë²ˆì§¸ ìƒ˜í”Œë¡œ ì¶©ëŒ ì²´í¬
                test_features = self._extract_batch_features(samples[:1])
                collision_info = self.node_manager.check_collision(
                    test_features[0], 
                    exclude_user=user_id
                )
                
                if collision_info:
                    collision_user, collision_distance = collision_info
                    print(f"[LoopClosure] âš ï¸ Collision detected with User {collision_user}! "
                          f"(distance: {collision_distance:.4f})")
                    loop_closure_triggered = True
                    self.loop_closure_stats['total_collisions'] += 1
        
        # 2. ë£¨í”„ í´ë¡œì € ì²˜ë¦¬
        if loop_closure_triggered:
            # ğŸ”¥ ì¶©ëŒ í•´ê²°ì„ ìœ„í•œ íŠ¹ë³„ ì²˜ë¦¬
            start_time = time.time()
            resolution_result = self._resolve_collision(
                new_user_id=user_id,
                new_samples=samples,
                collision_user=collision_user,
                original_distance=collision_distance
            )
            resolution_time = time.time() - start_time
            
            self.loop_closure_stats['total_resolution_time'] += resolution_time
            
            # ì¶©ëŒ í•´ê²° í›„ ì •ìƒ ì²˜ë¦¬ ê³„ì†
            if resolution_result['success']:
                print(f"[LoopClosure] âœ… Collision resolved! New distance: {resolution_result['new_distance']:.4f}")
                print(f"[LoopClosure] Resolution took {resolution_time:.2f} seconds")
                self.loop_closure_stats['resolved_collisions'] += 1
            else:
                print(f"[LoopClosure] âš ï¸ Collision partially resolved. Distance: {resolution_result['new_distance']:.4f}")
                self.loop_closure_stats['failed_resolutions'] += 1
        
        else:
            # ì¼ë°˜ í•™ìŠµ (ì¶©ëŒ ì—†ìŒ)
            training_batch = self._construct_training_batch(
                new_samples=samples,
                new_embeddings=None,
                new_user_id=user_id
            )
            
            # ì¼ë°˜ í•™ìŠµ ìˆ˜í–‰
            adaptation_epochs = self.config.continual_learner.adaptation_epochs
            
            for epoch in range(adaptation_epochs):
                print(f"[Epoch {epoch+1}/{adaptation_epochs}]")
                
                # Phase 1: SupConLoss
                loss_dict = self._train_step(training_batch, phase='supcon')
                print(f"   SupCon Loss: {loss_dict['supcon']:.4f}")
                
                # Phase 2: Mahalanobis Loss (if enabled)
                if self.criterion.enable_mahalanobis:
                    loss_dict = self._train_step(training_batch, phase='mahal')
                    print(f"   Mahal Loss: {loss_dict['mahal']:.4f}")
        
        # 3. ì‚¬ìš©ì ë…¸ë“œ ìƒì„±/ì—…ë°ì´íŠ¸
        if self.user_nodes_enabled and self.node_manager:
            final_embeddings = self._extract_batch_features(samples)
            
            if not loop_closure_triggered:
                # ì¶©ëŒì´ ì—†ì—ˆìœ¼ë©´ ì¼ë°˜ ì¶”ê°€
                self.node_manager.add_user(user_id, final_embeddings)
            # ì¶©ëŒì´ ìˆì—ˆìœ¼ë©´ _resolve_collisionì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë¨
        
        # 4. ì„ ë³„ì  ë²„í¼ ì €ì¥
        batch_embeddings = self._extract_batch_features(samples)
        stored_count = self._selective_buffer_storage(samples, batch_embeddings, user_id)
        
        # 5. í†µê³„ ì—…ë°ì´íŠ¸
        self.global_step += 1
        self.processed_users += 1
        
        # 6. ì£¼ê¸°ì  ë™ê¸°í™”
        if self.global_step % self.config.continual_learner.sync_frequency == 0:
            self._sync_weights()
        
        print(f"[Process] âœ… Completed: stored={stored_count}/{len(samples)}")
        
        return {
            'stored': stored_count,
            'total': len(samples),
            'loop_closure': loop_closure_triggered
        }

    def _resolve_collision(self, new_user_id: int, new_samples: List[torch.Tensor],
                          collision_user: int, original_distance: float) -> Dict:
        """
        ğŸ”¥ Loop Closure: ì¶©ëŒ í•´ê²° í”„ë¡œì„¸ìŠ¤ with PQ restoration
        
        1. PQ ë³µì›ìœ¼ë¡œ ì¶©ëŒ ì‚¬ìš©ìì˜ ì›ë³¸ ìƒ˜í”Œ íšë“
        2. íŠ¹ë³„ ë°°ì¹˜ êµ¬ì„± (ì¶©ëŒ ì‚¬ìš©ì + ìƒˆ ì‚¬ìš©ì + í•˜ë“œ ë„¤ê±°í‹°ë¸Œ)
        3. ì§‘ì¤‘ í•™ìŠµ ìˆ˜í–‰
        4. ë…¸ë“œ ì¬êµ¬ì„± ë° ê±°ë¦¬ ë³´ì •
        """
        print(f"\n[LoopClosure] ğŸ”„ Starting collision resolution process...")
        print(f"   New User: {new_user_id}")
        print(f"   Collision User: {collision_user}")
        print(f"   Original Distance: {original_distance:.4f}")
        
        # 1. ì¶©ëŒ ì‚¬ìš©ìì˜ ìƒ˜í”Œ ë³µì›
        collision_node = self.node_manager.get_node(collision_user)
        if collision_node is None:
            print(f"[LoopClosure] Error: Cannot find collision user {collision_user}")
            return {'success': False, 'new_distance': original_distance}
        
        # PQ ë³µì› ë˜ëŠ” ë²„í¼ì—ì„œ ê°€ì ¸ì˜¤ê¸°
        collision_samples = []
        
        if collision_node.compression_enabled and collision_node.pq_codes is not None:
            print(f"[LoopClosure] Restoring {collision_node.sample_count} samples from PQ compression...")
            restored_features = collision_node.restore_samples()
            
            # ë³µì›ëœ íŠ¹ì§•ì„ ê¸°ë°˜ìœ¼ë¡œ ë²„í¼ì—ì„œ ì›ë³¸ ì´ë¯¸ì§€ ì°¾ê¸° ë˜ëŠ” íŠ¹ì§• ìì²´ ì‚¬ìš©
            # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ë²„í¼ì—ì„œ í•´ë‹¹ ì‚¬ìš©ìì˜ ìƒ˜í”Œì„ ê°€ì ¸ì˜´
            collision_samples = self._get_user_samples_from_buffer(collision_user)
            
            if not collision_samples:
                # ë²„í¼ì— ì—†ìœ¼ë©´ ë³µì›ëœ íŠ¹ì§•ì„ ì§ì ‘ ì‚¬ìš© (ì‹¤ì œë¡œëŠ” ì´ë¯¸ì§€ê°€ í•„ìš”í•¨)
                print(f"[LoopClosure] Warning: Using restored features directly")
                # ì´ ê²½ìš° íŠ¹ì§• ë ˆë²¨ì—ì„œ í•™ìŠµì„ ìˆ˜í–‰í•´ì•¼ í•¨
                pass
        else:
            # ì••ì¶•ë˜ì§€ ì•Šì€ ê²½ìš° ë²„í¼ì—ì„œ ê°€ì ¸ì˜¤ê¸°
            collision_samples = self._get_user_samples_from_buffer(collision_user)
        
        if not collision_samples:
            print(f"[LoopClosure] Warning: No samples found for collision user {collision_user}")
            # ìµœì†Œí•œ ë…¸ë“œì˜ í†µê³„ ì •ë³´ëŠ” ìˆìœ¼ë¯€ë¡œ ê³„ì† ì§„í–‰
        
        # 2. íŠ¹ë³„ ë°°ì¹˜ êµ¬ì„±
        print(f"[LoopClosure] Constructing collision resolution batch...")
        
        # ì¶©ëŒ í•´ê²°ìš© í™•ì¥ ì—í¬í¬
        resolution_epochs = self.loop_closure_config.retraining_epochs
        
        # ë°°ì¹˜ êµ¬ì„±: ì¶©ëŒ ì‚¬ìš©ì + ìƒˆ ì‚¬ìš©ì + í•˜ë“œ ë„¤ê±°í‹°ë¸Œ
        all_images = []
        all_labels = []
        
        # ìƒˆ ì‚¬ìš©ì ìƒ˜í”Œ
        all_images.extend(new_samples)
        all_labels.extend([new_user_id] * len(new_samples))
        
        # ì¶©ëŒ ì‚¬ìš©ì ìƒ˜í”Œ
        if collision_samples:
            # ê· í˜•ì„ ìœ„í•´ ê°™ì€ ìˆ˜ë§Œ ì‚¬ìš©
            sample_count = min(len(collision_samples), len(new_samples))
            all_images.extend(collision_samples[:sample_count])
            all_labels.extend([collision_user] * sample_count)
        
        # í•˜ë“œ ë„¤ê±°í‹°ë¸Œ ì¶”ê°€
        remaining_slots = max(0, self.training_batch_size - len(all_images))
        if remaining_slots > 0 and len(self.replay_buffer.image_storage) > 0:
            # ì¶©ëŒ ì‚¬ìš©ìì™€ ìƒˆ ì‚¬ìš©ìë¥¼ ì œì™¸í•œ í•˜ë“œ ë„¤ê±°í‹°ë¸Œ
            hard_negatives = self._get_hard_negatives_excluding(
                exclude_users=[new_user_id, collision_user],
                num_samples=remaining_slots
            )
            
            for img, label in hard_negatives:
                all_images.append(img)
                all_labels.append(label)
        
        print(f"[LoopClosure] Batch composition:")
        print(f"   New user samples: {len(new_samples)}")
        print(f"   Collision user samples: {len([l for l in all_labels if l == collision_user])}")
        print(f"   Hard negatives: {len(all_images) - len(new_samples) - len([l for l in all_labels if l == collision_user])}")
        print(f"   Total batch size: {len(all_images)}")
        
        # 3. ì§‘ì¤‘ í•™ìŠµ ìˆ˜í–‰
        training_batch = {'images': all_images, 'labels': all_labels}
        
        print(f"\n[LoopClosure] Performing {resolution_epochs} epochs of focused training...")
        
        best_distance = original_distance
        distance_history = []
        
        for epoch in range(resolution_epochs):
            print(f"[LoopClosure Epoch {epoch+1}/{resolution_epochs}]")
            
            # Phase 1: SupConLoss - í´ë˜ìŠ¤ ë‚´ ì‘ì§‘
            loss_dict_supcon = self._train_step(training_batch, phase='supcon')
            print(f"   SupCon Loss: {loss_dict_supcon['supcon']:.4f}")
            
            # Phase 2: Mahalanobis Loss - í´ë˜ìŠ¤ ê°„ ë¶„ë¦¬
            if self.criterion.enable_mahalanobis:
                loss_dict_mahal = self._train_step(training_batch, phase='mahal')
                print(f"   Mahal Loss: {loss_dict_mahal['mahal']:.4f}")
            
            # ì¤‘ê°„ ê±°ë¦¬ í™•ì¸
            if epoch % 2 == 0 or epoch == resolution_epochs - 1:
                with torch.no_grad():
                    # ìƒˆ ì‚¬ìš©ìì˜ í˜„ì¬ íŠ¹ì§•
                    new_features = self._extract_batch_features(new_samples[:1])
                    
                    # ì¶©ëŒ ì‚¬ìš©ìì™€ì˜ ê±°ë¦¬ ì¬ê³„ì‚°
                    if collision_node and collision_node.mean is not None:
                        intermediate_distance = self._calculate_feature_distance(
                            new_features[0], collision_node.mean
                        )
                        distance_history.append(intermediate_distance)
                        print(f"   Distance at epoch {epoch+1}: {intermediate_distance:.4f}")
                        
                        if intermediate_distance > best_distance:
                            best_distance = intermediate_distance
        
        # 4. ë…¸ë“œ ì¬êµ¬ì„± ë° ê±°ë¦¬ ë³´ì •
        print(f"\n[LoopClosure] Reconstructing user nodes...")
        
        # ìƒˆ ì‚¬ìš©ì ë…¸ë“œ ì¬êµ¬ì„±
        new_embeddings = self._extract_batch_features(new_samples)
        self.node_manager.reconstruct_user_node(new_user_id, new_embeddings)
        
        # ì¶©ëŒ ì‚¬ìš©ì ë…¸ë“œ ì¬êµ¬ì„±
        if collision_samples:
            collision_embeddings = self._extract_batch_features(collision_samples)
            self.node_manager.reconstruct_user_node(collision_user, collision_embeddings)
        
        # 5. ìµœì¢… ê±°ë¦¬ í™•ì¸
        new_node = self.node_manager.get_node(new_user_id)
        collision_node = self.node_manager.get_node(collision_user)
        
        if new_node and collision_node:
            final_distance = new_node.diagonal_mahalanobis_distance(collision_node.mean)
            
            # ì¶©ëŒ ì´ë ¥ ê¸°ë¡
            new_node.add_collision_record(collision_user, final_distance)
            collision_node.add_collision_record(new_user_id, final_distance)
            
            print(f"\n[LoopClosure] Resolution complete!")
            print(f"   Original distance: {original_distance:.4f}")
            print(f"   Final distance: {final_distance:.4f}")
            print(f"   Improvement: {(final_distance - original_distance):.4f}")
            
            if distance_history:
                print(f"   Distance progression: {' â†’ '.join([f'{d:.3f}' for d in distance_history])}")
            
            success = final_distance >= self.node_manager.collision_threshold
            
            # ì¶”ê°€ ê²€ì¦: ì‹¤ì œë¡œ ë‘ ì‚¬ìš©ìë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ”ì§€ í…ŒìŠ¤íŠ¸
            if success:
                self._verify_collision_resolution(new_user_id, collision_user, new_samples, collision_samples)
            
            return {
                'success': success,
                'original_distance': original_distance,
                'new_distance': final_distance,
                'improvement': final_distance - original_distance,
                'training_epochs': resolution_epochs,
                'distance_history': distance_history
            }
        
        return {'success': False, 'new_distance': original_distance}

    def _verify_collision_resolution(self, user1: int, user2: int, 
                                    samples1: List[torch.Tensor], 
                                    samples2: List[torch.Tensor]):
        """ì¶©ëŒ í•´ê²° ê²€ì¦"""
        print(f"\n[LoopClosure] Verifying collision resolution...")
        
        with torch.no_grad():
            # ê° ì‚¬ìš©ìì˜ ìƒ˜í”Œì´ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜ë˜ëŠ”ì§€ í™•ì¸
            correct_user1 = 0
            correct_user2 = 0
            
            # User1 ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
            for sample in samples1[:3]:  # ì²˜ìŒ 3ê°œë§Œ í…ŒìŠ¤íŠ¸
                result = self.verify_user(sample.unsqueeze(0))
                if result['is_match'] and result['matched_user'] == user1:
                    correct_user1 += 1
            
            # User2 ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
            if samples2:
                for sample in samples2[:3]:
                    result = self.verify_user(sample.unsqueeze(0))
                    if result['is_match'] and result['matched_user'] == user2:
                        correct_user2 += 1
            
            print(f"   User {user1} samples: {correct_user1}/3 correctly identified")
            if samples2:
                print(f"   User {user2} samples: {correct_user2}/3 correctly identified")

    def _get_hard_negatives_excluding(self, exclude_users: List[int], 
                                     num_samples: int) -> List[Tuple]:
        """íŠ¹ì • ì‚¬ìš©ìë“¤ì„ ì œì™¸í•œ í•˜ë“œ ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸°"""
        hard_samples = []
        
        # ë²„í¼ì—ì„œ ì œì™¸í•  ì‚¬ìš©ìê°€ ì•„ë‹Œ ìƒ˜í”Œë“¤ë§Œ ì„ íƒ
        available_samples = [
            (item['image'], item['user_id']) 
            for item in self.replay_buffer.image_storage
            if item['user_id'] not in exclude_users
        ]
        
        if available_samples:
            # í•˜ë“œ ë„¤ê±°í‹°ë¸Œ ë§ˆì´ë‹ ì‹œë„
            if len(available_samples) > num_samples:
                # í˜„ì¬ ì„ë² ë”©ê³¼ ê°€ì¥ ê°€ê¹Œìš´ ìƒ˜í”Œë“¤ ì„ íƒ
                # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ëœë¤ ì„ íƒ
                selected = random.sample(available_samples, num_samples)
            else:
                selected = available_samples
            
            hard_samples.extend(selected)
        
        return hard_samples

    def _calculate_feature_distance(self, feat1: torch.Tensor, feat2: torch.Tensor) -> float:
        """ë‘ íŠ¹ì§• ê°„ ê±°ë¦¬ ê³„ì‚°"""
        # Cosine similarityë¥¼ ê±°ë¦¬ë¡œ ë³€í™˜
        similarity = F.cosine_similarity(feat1.unsqueeze(0), feat2.unsqueeze(0))
        distance = 1.0 - similarity.item()
        return distance

    def _train_step(self, batch_data: Dict, phase: str) -> Dict[str, torch.Tensor]:
        """í•œ ìŠ¤í… í•™ìŠµ - ìˆ˜ì •ë¨"""
        images = batch_data['images']
        labels = batch_data['labels']
        
        if not images:
            return {'total': 0.0, 'supcon': 0.0, 'mahal': 0.0}
        
        self.learner_net.train()
        self.optimizer.zero_grad()
        
        # Extract features
        embeddings = []
        for img in images:
            img_tensor = img.to(self.device)
            if len(img_tensor.shape) == 3:
                img_tensor = img_tensor.unsqueeze(0)
            
            if self.headless_mode:
                _, embedding = self.learner_net(img_tensor)
            else:
                _, embedding = self.learner_net(img_tensor)
            
            embeddings.append(embedding)
        
        embeddings_tensor = torch.cat(embeddings, dim=0)
        labels_tensor = torch.tensor(labels, dtype=torch.long, device=self.device)
        
        # ğŸ”¥ Combined Loss ì‚¬ìš©
        loss_dict = self.criterion(embeddings_tensor, labels_tensor, phase=phase)
        
        # Backward
        loss_dict['total'].backward()
        self.optimizer.step()
        
        return {k: v.item() if torch.is_tensor(v) else v for k, v in loss_dict.items()}

    def _construct_training_batch(self, new_samples: List[torch.Tensor], 
                                 new_embeddings: torch.Tensor, 
                                 new_user_id: int) -> Dict:
        """í•™ìŠµìš© ë°°ì¹˜ êµ¬ì„± (ê¸°ì¡´ê³¼ ë™ì¼)"""
        
        # Calculate how many samples we need from buffer
        buffer_samples_needed = max(0, self.training_batch_size - len(new_samples))
        
        print(f"[Batch] Constructing training batch:")
        print(f"   New samples: {len(new_samples)}")
        print(f"   Buffer samples needed: {buffer_samples_needed}")
        
        # Get samples from replay buffer
        if buffer_samples_needed > 0:
            if new_embeddings is None:
                new_embeddings = self._extract_batch_features(new_samples)
                
            buffer_images, buffer_labels = self.replay_buffer.sample_for_training(
                num_samples=buffer_samples_needed,
                current_embeddings=new_embeddings.cpu().split(1),  # Convert to list
                current_user_id=new_user_id
            )
        else:
            buffer_images, buffer_labels = [], []
        
        # Combine all samples
        all_images = new_samples + buffer_images
        all_labels = [new_user_id] * len(new_samples) + buffer_labels
        
        print(f"[Batch] Final composition: {len(all_images)} samples")
        
        return {
            'images': all_images,
            'labels': all_labels
        }

    def _get_user_samples_from_buffer(self, user_id: int) -> List[torch.Tensor]:
        """íŠ¹ì • ì‚¬ìš©ìì˜ ìƒ˜í”Œì„ ë²„í¼ì—ì„œ ê°€ì ¸ì˜¤ê¸°"""
        user_samples = []
        
        for item in self.replay_buffer.image_storage:
            if item['user_id'] == user_id:
                user_samples.append(item['image'])
        
        return user_samples

    def _extract_batch_features(self, samples: List[torch.Tensor]) -> torch.Tensor:
        """ë°°ì¹˜ íŠ¹ì§• ì¶”ì¶œ (GPU íš¨ìœ¨ì )"""
        self.learner_net.eval()
        
        with torch.no_grad():
            # Stack all samples into a batch
            batch = torch.stack([s.to(self.device) for s in samples])
            
            # Extract features in one forward pass
            features = self.learner_net.getFeatureCode(batch)
        
        self.learner_net.train()
        return features

    def _selective_buffer_storage(self, samples: List[torch.Tensor], 
                                 embeddings: torch.Tensor, 
                                 user_id: int) -> int:
        """ì„ ë³„ì  ë²„í¼ ì €ì¥"""
        stored_count = 0
        
        for i, (sample, embedding) in enumerate(zip(samples, embeddings)):
            if self.replay_buffer.add_if_diverse(sample, user_id, embedding):
                stored_count += 1
        
        return stored_count

    def _sync_weights(self):
        """ê°€ì¤‘ì¹˜ ë™ê¸°í™”"""
        self.predictor_net.load_state_dict(self.learner_net.state_dict())
        self.predictor_net.eval()
        
        print(f"\n[Sync] ğŸ”„ Weights synchronized at step {self.global_step}")

    def run_experiment(self):
        """ë°°ì¹˜ ê¸°ë°˜ ì‹¤í—˜ ì‹¤í–‰"""
        print(f"\n[System] Starting batch-based continual learning...")
        
        # Load dataset
        cfg_dataset = self.config.dataset
        dataset = MyDataset(txt=str(cfg_dataset.dataset_path), train=False)
        
        # Group data by label
        grouped_data = self._group_data_by_label(dataset)
        total_users = len(grouped_data)
        
        print(f"[System] Dataset loaded: {total_users} users")
        print(f"[System] Processing {self.samples_per_label} samples per user")
        
        # Process each user's batch
        for user_id, user_indices in tqdm(grouped_data.items(), desc="Batch Processing"):
            # Skip if already processed
            if self.processed_users > 0 and user_id in self._get_processed_user_ids():
                continue
            
            # Get samples for this user
            samples = []
            for idx in user_indices[:self.samples_per_label]:
                data, _ = dataset[idx]
                samples.append(data[0])  # Use first image from pair
            
            if len(samples) == self.samples_per_label:
                # Process batch
                results = self.process_label_batch(samples, user_id)
                
                # Save checkpoint periodically
                if self.global_step % self.config.continual_learner.intermediate_save_frequency == 0:
                    self._save_checkpoint()
        
        # Final save
        print(f"\n[System] Experiment completed!")
        self._print_loop_closure_statistics()
        self._save_checkpoint()
        self._save_final_model()

    def _print_loop_closure_statistics(self):
        """Loop Closure í†µê³„ ì¶œë ¥"""
        if self.loop_closure_enabled:
            print(f"\n[LoopClosure] ğŸ“Š Final Statistics:")
            print(f"   Total collisions: {self.loop_closure_stats['total_collisions']}")
            print(f"   Resolved: {self.loop_closure_stats['resolved_collisions']}")
            print(f"   Failed: {self.loop_closure_stats['failed_resolutions']}")
            if self.loop_closure_stats['total_collisions'] > 0:
                success_rate = self.loop_closure_stats['resolved_collisions'] / self.loop_closure_stats['total_collisions'] * 100
                print(f"   Success rate: {success_rate:.1f}%")
                avg_time = self.loop_closure_stats['total_resolution_time'] / self.loop_closure_stats['total_collisions']
                print(f"   Avg resolution time: {avg_time:.2f} seconds")

    def _group_data_by_label(self, dataset) -> Dict[int, List[int]]:
        """ë°ì´í„°ë¥¼ ë¼ë²¨ë³„ë¡œ ê·¸ë£¹í™”"""
        grouped = defaultdict(list)
        
        for idx in range(len(dataset)):
            _, label = dataset[idx]
            user_id = label.item() if torch.is_tensor(label) else label
            grouped[user_id].append(idx)
        
        return dict(grouped)

    def _get_processed_user_ids(self) -> set:
        """ì´ë¯¸ ì²˜ë¦¬ëœ ì‚¬ìš©ì ID ë°˜í™˜"""
        if self.node_manager and self.user_nodes_enabled:
            return set(self.node_manager.nodes.keys())
        return set()

    def _save_checkpoint(self):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥ - ìˆ˜ì •ë¨"""
        checkpoint = {
            'global_step': self.global_step,
            'processed_users': self.processed_users,
            'learner_state_dict': self.learner_net.state_dict(),
            'predictor_state_dict': self.predictor_net.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'loop_closure_stats': self.loop_closure_stats,
            'config': {
                'training_batch_size': self.training_batch_size,
                'hard_negative_ratio': self.hard_negative_ratio,
                'samples_per_label': self.samples_per_label,
                'headless_mode': self.headless_mode,
                'user_nodes_enabled': self.user_nodes_enabled,
                'loop_closure_enabled': self.loop_closure_enabled
            }
        }
        
        checkpoint_path = self.checkpoint_dir / f'checkpoint_step_{self.global_step}.pth'
        torch.save(checkpoint, checkpoint_path)
        
        # Also save buffer state
        self.replay_buffer._save_state()
        
        # ğŸ”¥ Save user nodes
        if self.node_manager and self.user_nodes_enabled:
            self.node_manager.save_nodes()
        
        print(f"[Checkpoint] ğŸ’¾ Saved at step {self.global_step}")

    def _load_checkpoint(self):
        """ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        checkpoint_files = list(self.checkpoint_dir.glob('checkpoint_step_*.pth'))
        
        if not checkpoint_files:
            print("[Checkpoint] No checkpoint found, starting fresh")
            return
        
        latest = max(checkpoint_files, key=lambda x: int(x.stem.split('_')[-1]))
        
        print(f"[Checkpoint] Loading from: {latest.name}")
        checkpoint = torch.load(latest, map_location=self.device)
        
        self.global_step = checkpoint['global_step']
        self.processed_users = checkpoint['processed_users']
        
        self.learner_net.load_state_dict(checkpoint['learner_state_dict'])
        self.predictor_net.load_state_dict(checkpoint['predictor_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        if 'loop_closure_stats' in checkpoint:
            self.loop_closure_stats = checkpoint['loop_closure_stats']
        
        print(f"[Checkpoint] âœ… Resumed from step {self.global_step}")

    def _save_final_model(self):
        """ìµœì¢… ëª¨ë¸ ì €ì¥ - ìˆ˜ì •ë¨"""
        save_path = Path(self.config.model_saving.final_save_path)
        save_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Save learner
        learner_path = save_path / f'coconut_batch_learner_{timestamp}.pth'
        torch.save(self.learner_net.state_dict(), learner_path)
        
        # Save predictor
        predictor_path = save_path / f'coconut_batch_predictor_{timestamp}.pth'
        torch.save(self.predictor_net.state_dict(), predictor_path)
        
        # Save metadata
        metadata = {
            'total_steps': self.global_step,
            'total_users': self.processed_users,
            'training_batch_size': self.training_batch_size,
            'hard_negative_ratio': self.hard_negative_ratio,
            'samples_per_label': self.samples_per_label,
            'headless_mode': self.headless_mode,
            'user_nodes_enabled': self.user_nodes_enabled,
            'loop_closure_enabled': self.loop_closure_enabled,
            'buffer_stats': self.replay_buffer.get_statistics(),
            'loop_closure_stats': self.loop_closure_stats
        }
        
        # ğŸ”¥ Add user node statistics
        if self.node_manager and self.user_nodes_enabled:
            metadata['user_node_stats'] = self.node_manager.get_statistics()
        
        metadata_path = save_path / f'coconut_batch_metadata_{timestamp}.json'
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"[System] âœ… Final models saved to: {save_path}")
        print(f"  ğŸ“ Learner: {learner_path.name}")
        print(f"  ğŸ“ Predictor: {predictor_path.name}")
        print(f"  ğŸ“ Metadata: {metadata_path.name}")

    def verify_user(self, input_palmprint: torch.Tensor) -> Dict:
        """ğŸ”¥ ì‚¬ìš©ì ê²€ì¦ (ë…¸ë“œ ê¸°ë°˜)"""
        if not self.user_nodes_enabled or not self.node_manager:
            # Fallback to original verification
            return self.verifier.verify(input_palmprint, self.predictor_net)
        
        # Extract embedding
        with torch.no_grad():
            if len(input_palmprint.shape) == 3:
                input_palmprint = input_palmprint.unsqueeze(0)
            query_embedding = self.predictor_net.getFeatureCode(input_palmprint)[0]
        
        # Find nearest users
        nearest_users = self.node_manager.find_nearest_users(query_embedding, k=5)
        
        if not nearest_users:
            return {
                'is_match': False,
                'matched_user': None,
                'confidence': 0.0,
                'reason': 'No registered users'
            }
        
        # Best match
        best_user_id, best_distance = nearest_users[0]
        
        # Threshold check
        is_match = best_distance < self.similarity_threshold
        
        # Confidence calculation
        confidence = max(0.0, 1.0 - (best_distance / self.similarity_threshold)) if is_match else 0.0
        
        return {
            'is_match': is_match,
            'matched_user': best_user_id if is_match else None,
            'distance': best_distance,
            'confidence': confidence,
            'top_k_results': nearest_users,
            'method': 'user_node_mahalanobis'
        }

# í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
CoconutSystem = BatchCoconutSystem