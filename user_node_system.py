# user_node_system.py - ì‚¬ìš©ì ë…¸ë“œ ì‹œìŠ¤í…œ + Mahalanobis ê±°ë¦¬ ì¸ì¦

"""
ğŸ¥¥ CoCoNut ì‚¬ìš©ì ë…¸ë“œ ì‹œìŠ¤í…œ êµ¬í˜„

í•µì‹¬ ê¸°ëŠ¥:
1. ì‚¬ìš©ì ë…¸ë“œ (Î¼, Î£_diag) ê´€ë¦¬
2. Mahalanobis ê±°ë¦¬ ê¸°ë°˜ ì¸ì¦
3. ì˜¨ë¼ì¸ ë…¸ë“œ ì—…ë°ì´íŠ¸
4. ë£¨í”„ í´ë¡œì € ê°ì§€ (ì¶”í›„ êµ¬í˜„)

ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±:
- ì‚¬ìš©ìë‹¹ O(256) ë©”ëª¨ë¦¬ (128D Ã— 2)
- Diagonal covarianceë§Œ ì €ì¥
"""

import torch
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional
import pickle
import json
from pathlib import Path
from collections import defaultdict
import faiss

class UserNode:
    """ê°œë³„ ì‚¬ìš©ì ë…¸ë“œ í´ë˜ìŠ¤"""
    
    def __init__(self, user_id: int, feature_dim: int = 128):
        self.user_id = user_id
        self.feature_dim = feature_dim
        
        # í†µê³„ ì •ë³´ (í•µì‹¬ ë°ì´í„°)
        self.centroid = None  # Î¼: í‰ê·  ë²¡í„° [128]
        self.diag_covariance = None  # Î£_diag: ëŒ€ê° ê³µë¶„ì‚° [128]
        
        # ì˜¨ë¼ì¸ ì—…ë°ì´íŠ¸ìš© ì„ì‹œ ì €ì¥ì†Œ
        self.embeddings = []  # ì„ë² ë”© ë²¡í„°ë“¤ ì„ì‹œ ì €ì¥
        self.is_finalized = False  # ë…¸ë“œ ì™„ì„± ì—¬ë¶€
        
        print(f"[Node] ì‚¬ìš©ì {user_id} ë…¸ë“œ ìƒì„± (feature_dim: {feature_dim})")
    
    def add_embedding(self, embedding: torch.Tensor):
        """ìƒˆë¡œìš´ ì„ë² ë”© ì¶”ê°€"""
        if self.is_finalized:
            print(f"[Node] Warning: ì™„ì„±ëœ ë…¸ë“œ {self.user_id}ì— ì„ë² ë”© ì¶”ê°€ ì‹œë„")
            return
        
        # L2 ì •ê·œí™”ëœ ì„ë² ë”©ì¸ì§€ í™•ì¸
        embedding = F.normalize(embedding.flatten(), dim=0)
        self.embeddings.append(embedding.cpu().numpy())
        
        print(f"[Node] ì‚¬ìš©ì {self.user_id}: {len(self.embeddings)}ê°œ ì„ë² ë”© ìˆ˜ì§‘")
    
    def finalize_node(self, min_samples: int = 3):
        """ë…¸ë“œ í†µê³„ ê³„ì‚° ë° ì™„ì„±"""
        if len(self.embeddings) < min_samples:
            print(f"[Node] Warning: ì‚¬ìš©ì {self.user_id} ìƒ˜í”Œ ë¶€ì¡± ({len(self.embeddings)} < {min_samples})")
            return False
        
        embeddings_array = np.array(self.embeddings)  # [N, 128]
        
        # í‰ê·  ë²¡í„° ê³„ì‚°
        self.centroid = np.mean(embeddings_array, axis=0)  # [128]
        
        # ëŒ€ê° ê³µë¶„ì‚° ê³„ì‚° (ê° ì°¨ì›ì˜ ë¶„ì‚°)
        self.diag_covariance = np.var(embeddings_array, axis=0)  # [128]
        
        # ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ ìœ„í•œ ìµœì†Œê°’ ì„¤ì •
        min_variance = 1e-6
        self.diag_covariance = np.maximum(self.diag_covariance, min_variance)
        
        # ë©”ëª¨ë¦¬ ì ˆì•½: ì„ë² ë”© ë¦¬ìŠ¤íŠ¸ ì‚­ì œ
        self.embeddings = []
        self.is_finalized = True
        
        print(f"[Node] âœ… ì‚¬ìš©ì {self.user_id} ë…¸ë“œ ì™„ì„±:")
        print(f"   Centroid norm: {np.linalg.norm(self.centroid):.4f}")
        print(f"   Variance range: [{self.diag_covariance.min():.6f}, {self.diag_covariance.max():.6f}]")
        
        return True
    
    def mahalanobis_distance(self, query_embedding: torch.Tensor) -> float:
        """ê°œì„ ëœ Diagonal Mahalanobis ê±°ë¦¬ ê³„ì‚°"""
        if not self.is_finalized:
            raise ValueError(f"ì‚¬ìš©ì {self.user_id} ë…¸ë“œê°€ ì•„ì§ ì™„ì„±ë˜ì§€ ì•ŠìŒ")
        
        # ì¿¼ë¦¬ ì„ë² ë”© ì •ê·œí™”
        query = F.normalize(query_embedding.flatten(), dim=0).cpu().numpy()  # [128]
        
        # ì°¨ì´ ë²¡í„° ê³„ì‚°
        diff = query - self.centroid  # [128]
        
        # ğŸ”¥ ê°œì„ ëœ Mahalanobis ê±°ë¦¬ ê³„ì‚°
        # 1. í‘œì¤€í™”ëœ ê±°ë¦¬ (ê° ì°¨ì›ì„ í‘œì¤€í¸ì°¨ë¡œ ë‚˜ëˆ”)
        std_devs = np.sqrt(self.diag_covariance)  # í‘œì¤€í¸ì°¨
        standardized_diff = diff / std_devs  # í‘œì¤€í™”
        
        # 2. ì°¨ì›ìœ¼ë¡œ ì •ê·œí™”ëœ ê±°ë¦¬ (ê³ ì°¨ì› ë³´ì •)
        raw_distance = np.linalg.norm(standardized_diff)
        normalized_distance = raw_distance / np.sqrt(self.feature_dim)  # 128ì°¨ì› ë³´ì •
        
        return normalized_distance
    
    def get_memory_usage(self) -> int:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜ (bytes)"""
        if not self.is_finalized:
            return len(self.embeddings) * self.feature_dim * 4  # float32
        else:
            return self.feature_dim * 2 * 4  # centroid + diag_covariance
    
    def to_dict(self) -> Dict:
        """ì§ë ¬í™”ìš© ë”•ì…”ë„ˆë¦¬ ë³€í™˜"""
        return {
            'user_id': self.user_id,
            'feature_dim': self.feature_dim,
            'centroid': self.centroid.tolist() if self.centroid is not None else None,
            'diag_covariance': self.diag_covariance.tolist() if self.diag_covariance is not None else None,
            'is_finalized': self.is_finalized,
            'num_embeddings': len(self.embeddings)
        }
    
    @classmethod
    def from_dict(cls, data: Dict):
        """ë”•ì…”ë„ˆë¦¬ì—ì„œ ë…¸ë“œ ë³µì›"""
        node = cls(data['user_id'], data['feature_dim'])
        
        if data['centroid'] is not None:
            node.centroid = np.array(data['centroid'])
        if data['diag_covariance'] is not None:
            node.diag_covariance = np.array(data['diag_covariance'])
        
        node.is_finalized = data['is_finalized']
        return node

class CoconutNodeSystem:
    """CoCoNut ì‚¬ìš©ì ë…¸ë“œ ê´€ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, feature_dim: int = 128, save_path: str = "./user_nodes.pkl"):
        self.feature_dim = feature_dim
        self.save_path = Path(save_path)
        
        # ì‚¬ìš©ì ë…¸ë“œë“¤
        self.nodes: Dict[int, UserNode] = {}  # {user_id: UserNode}
        self.temp_nodes: Dict[int, UserNode] = {}  # ì„ì‹œ ë…¸ë“œ (ì™„ì„±ë˜ì§€ ì•Šì€)
        
        # Faiss ì¸ë±ìŠ¤ (ë¹ ë¥¸ ê²€ìƒ‰ìš©)
        self.faiss_index = None
        self.node_ids = []  # Faiss ì¸ë±ìŠ¤ì™€ user_id ë§¤í•‘
        
        # ë£¨í”„ í´ë¡œì € ê´€ë ¨
        self.loop_closure_threshold = 2.0  # Mahalanobis ê±°ë¦¬ ì„ê³„ê°’
        
        print(f"[NodeSystem] ì‹œìŠ¤í…œ ì´ˆê¸°í™” (feature_dim: {feature_dim})")
        
        # ê¸°ì¡´ ë…¸ë“œ ë¡œë“œ
        self._load_nodes()
    
    def register_embedding(self, user_id: int, embedding: torch.Tensor, 
                          finalize_threshold: int = 5) -> Optional[str]:
        """
        ì„ë² ë”© ë“±ë¡ ë° í•„ìš”ì‹œ ë…¸ë“œ ì™„ì„±
        
        Args:
            user_id: ì‚¬ìš©ì ID
            embedding: ì„ë² ë”© ë²¡í„° [128]
            finalize_threshold: ë…¸ë“œ ì™„ì„± ì„ê³„ê°’
            
        Returns:
            ìƒíƒœ ë©”ì‹œì§€
        """
        # 1. ê¸°ì¡´ ì™„ì„±ëœ ë…¸ë“œê°€ ìˆëŠ”ì§€ í™•ì¸
        if user_id in self.nodes:
            print(f"[NodeSystem] ì‚¬ìš©ì {user_id}ëŠ” ì´ë¯¸ ë“±ë¡ë¨")
            return "already_registered"
        
        # 2. ì„ì‹œ ë…¸ë“œì— ì¶”ê°€
        if user_id not in self.temp_nodes:
            self.temp_nodes[user_id] = UserNode(user_id, self.feature_dim)
        
        self.temp_nodes[user_id].add_embedding(embedding)
        
        # 3. ì™„ì„± ì¡°ê±´ í™•ì¸
        if len(self.temp_nodes[user_id].embeddings) >= finalize_threshold:
            return self._finalize_user_node(user_id)
        
        return f"collecting_{len(self.temp_nodes[user_id].embeddings)}"
    
    def _finalize_user_node(self, user_id: int) -> str:
        """ì‚¬ìš©ì ë…¸ë“œ ì™„ì„± ë° ë“±ë¡"""
        temp_node = self.temp_nodes[user_id]
        
        # ë…¸ë“œ í†µê³„ ê³„ì‚°
        if temp_node.finalize_node():
            # ë£¨í”„ í´ë¡œì € ê²€ì‚¬ (ì¶”í›„ êµ¬í˜„)
            loop_closure_detected = self._check_loop_closure(temp_node)
            
            if loop_closure_detected:
                return "loop_closure_detected"
            
            # ì •ìƒ ë“±ë¡
            self.nodes[user_id] = temp_node
            del self.temp_nodes[user_id]
            
            # Faiss ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
            self._update_faiss_index()
            
            # ì €ì¥
            self._save_nodes()
            
            print(f"[NodeSystem] âœ… ì‚¬ìš©ì {user_id} ë…¸ë“œ ë“±ë¡ ì™„ë£Œ")
            return "registered"
        else:
            print(f"[NodeSystem] âŒ ì‚¬ìš©ì {user_id} ë…¸ë“œ ì™„ì„± ì‹¤íŒ¨")
            return "finalization_failed"
    
    def _check_loop_closure(self, new_node: UserNode) -> bool:
        """ë£¨í”„ í´ë¡œì € ê°ì§€ (ì„ì‹œ êµ¬í˜„)"""
        if len(self.nodes) == 0:
            return False
        
        # ìƒˆ ë…¸ë“œì™€ ê¸°ì¡´ ë…¸ë“œë“¤ ê°„ ê±°ë¦¬ ê³„ì‚°
        new_centroid = torch.tensor(new_node.centroid)
        
        for existing_id, existing_node in self.nodes.items():
            distance = existing_node.mahalanobis_distance(new_centroid)
            
            if distance < self.loop_closure_threshold:
                print(f"[LoopClosure] ğŸ”„ ê°ì§€: ì‚¬ìš©ì {new_node.user_id} vs {existing_id} (ê±°ë¦¬: {distance:.4f})")
                # TODO: ë£¨í”„ í´ë¡œì € ì²˜ë¦¬ ë¡œì§
                return True
        
        return False
    
    def authenticate(self, query_embedding: torch.Tensor, 
                    auth_threshold: float = 0.8, top_k: int = 5) -> Dict:
        """
        ê°œì„ ëœ Mahalanobis ê±°ë¦¬ ê¸°ë°˜ ì¸ì¦
        
        Args:
            query_embedding: ì¿¼ë¦¬ ì„ë² ë”© [128]
            auth_threshold: ì •ê·œí™”ëœ ì¸ì¦ ì„ê³„ê°’ (0.8 ì¶”ì²œ)
            top_k: ìƒìœ„ kê°œ í›„ë³´ ê²€ì‚¬
            
        Returns:
            ì¸ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if len(self.nodes) == 0:
            return {
                'authenticated': False,
                'user_id': None,
                'distance': float('inf'),
                'reason': 'no_registered_users'
            }
        
        # 1. Faissë¡œ ë¹ ë¥¸ í›„ë³´ ê²€ìƒ‰ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜)
        top_candidates = self._faiss_search(query_embedding, top_k)
        
        # 2. ê° í›„ë³´ì— ëŒ€í•´ Mahalanobis ê±°ë¦¬ ê³„ì‚°
        best_user_id = None
        best_distance = float('inf')
        
        for candidate_id in top_candidates:
            if candidate_id in self.nodes:
                distance = self.nodes[candidate_id].mahalanobis_distance(query_embedding)
                
                if distance < best_distance:
                    best_distance = distance
                    best_user_id = candidate_id
        
        # 3. ì„ê³„ê°’ ê¸°ë°˜ ì¸ì¦ ê²°ì •
        authenticated = best_distance < auth_threshold
        
        result = {
            'authenticated': authenticated,
            'user_id': best_user_id if authenticated else None,
            'distance': best_distance,
            'threshold': auth_threshold,
            'candidates_checked': len(top_candidates)
        }
        
        if authenticated:
            print(f"[Auth] âœ… ì¸ì¦ ì„±ê³µ: ì‚¬ìš©ì {best_user_id} (ê±°ë¦¬: {best_distance:.4f})")
        else:
            print(f"[Auth] âŒ ì¸ì¦ ì‹¤íŒ¨: ìµœì†Œ ê±°ë¦¬ {best_distance:.4f} > ì„ê³„ê°’ {auth_threshold}")
        
        return result
    
    def _update_faiss_index(self):
        """Faiss ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸"""
        if len(self.nodes) == 0:
            return
        
        # ëª¨ë“  ë…¸ë“œì˜ centroid ìˆ˜ì§‘
        centroids = []
        node_ids = []
        
        for user_id, node in self.nodes.items():
            centroids.append(node.centroid)
            node_ids.append(user_id)
        
        centroids_array = np.array(centroids).astype('float32')  # [N, 128]
        
        # Faiss ì¸ë±ìŠ¤ ìƒì„± (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
        self.faiss_index = faiss.IndexFlatIP(self.feature_dim)
        
        # L2 ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìœ„í•´)
        faiss.normalize_L2(centroids_array)
        self.faiss_index.add(centroids_array)
        
        self.node_ids = node_ids
        print(f"[Faiss] ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸: {len(node_ids)} ë…¸ë“œ")
    
    def _faiss_search(self, query_embedding: torch.Tensor, k: int) -> List[int]:
        """Faissë¡œ ë¹ ë¥¸ í›„ë³´ ê²€ìƒ‰"""
        if self.faiss_index is None:
            return list(self.nodes.keys())
        
        # ì¿¼ë¦¬ ì •ê·œí™”
        query = F.normalize(query_embedding.flatten(), dim=0).cpu().numpy().astype('float32')
        query = query.reshape(1, -1)
        faiss.normalize_L2(query)
        
        # ê²€ìƒ‰
        k = min(k, len(self.node_ids))
        similarities, indices = self.faiss_index.search(query, k)
        
        # user_idë¡œ ë³€í™˜
        candidate_ids = [self.node_ids[idx] for idx in indices[0] if idx < len(self.node_ids)]
        
        return candidate_ids
    
    def get_system_stats(self) -> Dict:
        """ì‹œìŠ¤í…œ í†µê³„ ë°˜í™˜"""
        total_memory = sum(node.get_memory_usage() for node in self.nodes.values())
        temp_memory = sum(node.get_memory_usage() for node in self.temp_nodes.values())
        
        return {
            'registered_users': len(self.nodes),
            'temp_users': len(self.temp_nodes),
            'total_memory_bytes': total_memory,
            'temp_memory_bytes': temp_memory,
            'memory_per_user_bytes': total_memory // len(self.nodes) if self.nodes else 0,
            'faiss_index_size': len(self.node_ids)
        }
    
    def _save_nodes(self):
        """ë…¸ë“œ ì €ì¥"""
        save_data = {
            'nodes': {uid: node.to_dict() for uid, node in self.nodes.items()},
            'temp_nodes': {uid: node.to_dict() for uid, node in self.temp_nodes.items()},
            'feature_dim': self.feature_dim
        }
        
        with open(self.save_path, 'wb') as f:
            pickle.dump(save_data, f)
        
        print(f"[NodeSystem] ğŸ’¾ ë…¸ë“œ ì €ì¥: {len(self.nodes)} ì™„ì„±, {len(self.temp_nodes)} ì„ì‹œ")
    
    def _load_nodes(self):
        """ë…¸ë“œ ë¡œë“œ"""
        if not self.save_path.exists():
            print(f"[NodeSystem] ì €ì¥ëœ ë…¸ë“œ ì—†ìŒ")
            return
        
        try:
            with open(self.save_path, 'rb') as f:
                save_data = pickle.load(f)
            
            # ì™„ì„±ëœ ë…¸ë“œ ë³µì›
            for uid, node_data in save_data.get('nodes', {}).items():
                self.nodes[int(uid)] = UserNode.from_dict(node_data)
            
            # ì„ì‹œ ë…¸ë“œ ë³µì›
            for uid, node_data in save_data.get('temp_nodes', {}).items():
                self.temp_nodes[int(uid)] = UserNode.from_dict(node_data)
            
            # Faiss ì¸ë±ìŠ¤ ì¬êµ¬ì„±
            if self.nodes:
                self._update_faiss_index()
            
            print(f"[NodeSystem] ğŸ“‚ ë…¸ë“œ ë¡œë“œ: {len(self.nodes)} ì™„ì„±, {len(self.temp_nodes)} ì„ì‹œ")
            
        except Exception as e:
            print(f"[NodeSystem] âŒ ë…¸ë“œ ë¡œë“œ ì‹¤íŒ¨: {e}")

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_node_system():
    """ë…¸ë“œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ì‚¬ìš©ì ë…¸ë“œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("="*50)
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    node_system = CoconutNodeSystem(feature_dim=128)
    
    # ê°€ì§œ ì„ë² ë”© ìƒì„±
    def generate_user_embeddings(user_id: int, num_samples: int = 6):
        """íŠ¹ì • ì‚¬ìš©ìì˜ ìœ ì‚¬í•œ ì„ë² ë”©ë“¤ ìƒì„±"""
        base_vector = torch.randn(128) * 0.1
        embeddings = []
        
        for i in range(num_samples):
            noise = torch.randn(128) * 0.05  # ì‘ì€ ë…¸ì´ì¦ˆ
            embedding = F.normalize(base_vector + noise, dim=0)
            embeddings.append(embedding)
        
        return embeddings
    
    # ì‚¬ìš©ì ë“±ë¡ ì‹œë®¬ë ˆì´ì…˜
    for user_id in [1, 2, 3]:
        print(f"\nğŸ‘¤ ì‚¬ìš©ì {user_id} ë“±ë¡ ì¤‘...")
        embeddings = generate_user_embeddings(user_id)
        
        for i, embedding in enumerate(embeddings):
            result = node_system.register_embedding(user_id, embedding)
            print(f"  ì„ë² ë”© {i+1}: {result}")
    
    # ì‹œìŠ¤í…œ í†µê³„
    stats = node_system.get_system_stats()
    print(f"\nğŸ“Š ì‹œìŠ¤í…œ í†µê³„:")
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    # ì¸ì¦ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ” ì¸ì¦ í…ŒìŠ¤íŠ¸")
    
    # ë“±ë¡ëœ ì‚¬ìš©ì (ì‚¬ìš©ì 1ê³¼ ìœ ì‚¬í•œ ì„ë² ë”©)
    test_embedding_1 = generate_user_embeddings(1, 1)[0]
    result = node_system.authenticate(test_embedding_1, auth_threshold=0.8)
    print(f"ì‚¬ìš©ì 1 ìœ ì‚¬ ì„ë² ë”©: {result}")
    
    # ë¯¸ë“±ë¡ ì‚¬ìš©ì (ì™„ì „íˆ ë‹¤ë¥¸ ì„ë² ë”©)
    test_embedding_unknown = F.normalize(torch.randn(128) * 2.0, dim=0)
    result = node_system.authenticate(test_embedding_unknown, auth_threshold=0.8)
    print(f"ë¯¸ë“±ë¡ ì‚¬ìš©ì: {result}")
    
    # ğŸ”¥ ì¶”ê°€: ê±°ë¦¬ ë¶„í¬ ë¶„ì„
    print(f"\nğŸ“Š ê±°ë¦¬ ë¶„í¬ ë¶„ì„")
    
    # ê°™ì€ ì‚¬ìš©ìë“¤ì˜ ê±°ë¦¬
    genuine_distances = []
    for user_id in [1, 2, 3]:
        for _ in range(10):
            test_emb = generate_user_embeddings(user_id, 1)[0]
            distance = node_system.nodes[user_id].mahalanobis_distance(test_emb)
            genuine_distances.append(distance)
    
    # ë‹¤ë¥¸ ì‚¬ìš©ìë“¤ì˜ ê±°ë¦¬
    imposter_distances = []
    for _ in range(30):
        unknown_emb = F.normalize(torch.randn(128) * 2.0, dim=0)
        for user_id in [1, 2, 3]:
            distance = node_system.nodes[user_id].mahalanobis_distance(unknown_emb)
            imposter_distances.append(distance)
    
    print(f"Genuine ê±°ë¦¬: í‰ê·  {np.mean(genuine_distances):.4f}, ë²”ìœ„ [{np.min(genuine_distances):.4f}, {np.max(genuine_distances):.4f}]")
    print(f"Imposter ê±°ë¦¬: í‰ê·  {np.mean(imposter_distances):.4f}, ë²”ìœ„ [{np.min(imposter_distances):.4f}, {np.max(imposter_distances):.4f}]")
    
    # ê¶Œì¥ ì„ê³„ê°’ ê³„ì‚°
    recommended_threshold = (np.max(genuine_distances) + np.min(imposter_distances)) / 2
    print(f"ê¶Œì¥ ì„ê³„ê°’: {recommended_threshold:.4f}")
    
    # ê¶Œì¥ ì„ê³„ê°’ìœ¼ë¡œ ì¬í…ŒìŠ¤íŠ¸
    print(f"\nğŸ¯ ê¶Œì¥ ì„ê³„ê°’({recommended_threshold:.2f})ìœ¼ë¡œ ì¬í…ŒìŠ¤íŠ¸")
    result1 = node_system.authenticate(test_embedding_1, auth_threshold=recommended_threshold)
    result2 = node_system.authenticate(test_embedding_unknown, auth_threshold=recommended_threshold)
    print(f"ì‚¬ìš©ì 1 ìœ ì‚¬: {result1['authenticated']} (ê±°ë¦¬: {result1['distance']:.4f})")
    print(f"ë¯¸ë“±ë¡ ì‚¬ìš©ì: {result2['authenticated']} (ê±°ë¦¬: {result2['distance']:.4f})")

if __name__ == "__main__":
    test_node_system()