# evaluation/eval_utils.py - í™•ì¥ëœ í‰ê°€ ìœ í‹¸ë¦¬í‹°
"""
COCONUT Evaluation Utilities (Extended)

FEATURES:
- Feature extraction and similarity computation
- Rank-1 accuracy calculation  
- EER (Equal Error Rate) calculation
- ROC curve analysis
- ğŸ”¥ NEW: Detailed biometric metrics (FAR/FRR/AUC)
- ğŸ”¥ NEW: Visualization and report generation
- ğŸ”¥ NEW: COCONUT Headless mode specialized evaluation
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from sklearn import metrics
from scipy.optimize import brentq
from scipy.interpolate import interp1d
import os
from pathlib import Path
import json
from datetime import datetime

# ê¸°ì¡´ í•¨ìˆ˜ë“¤ ìœ ì§€
def extract_features(model, dataloader, device):
    """ì£¼ì–´ì§„ ë°ì´í„°ë¡œë”ì—ì„œ ëª¨ë“  íŠ¹ì§• ë²¡í„°ì™€ ë¼ë²¨ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    model.to(device)
    model.eval()
    
    features_list = []
    labels_list = []

    with torch.no_grad():
        for datas, target in tqdm(dataloader, desc="Extracting features"):
            data = datas[0].to(device)
            codes = model.getFeatureCode(data)
            
            features_list.append(codes.cpu().numpy())
            labels_list.append(target.cpu().numpy())
            
    features = np.concatenate(features_list, axis=0)
    labels = np.concatenate(labels_list, axis=0)
    
    print(f"Extracted {len(features)} features.")
    return features, labels

def calculate_scores(probe_features, gallery_features):
    """Probe ì„¸íŠ¸ì™€ Gallery ì„¸íŠ¸ ê°„ì˜ ëª¨ë“  ë§¤ì¹­ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    cosine_similarity = np.dot(probe_features, gallery_features.T)
    cosine_similarity = np.clip(cosine_similarity, -1.0, 1.0)
    distances = np.arccos(cosine_similarity) / np.pi
    
    return distances

def calculate_eer(genuine_scores, imposter_scores):
    """ì •ê·œ ë§¤ì¹­ ì ìˆ˜ì™€ ë¹„ì •ê·œ ë§¤ì¹­ ì ìˆ˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ EERì„ ê³„ì‚°í•©ë‹ˆë‹¤. (ìŒìˆ˜ ì ìˆ˜ ì§€ì›)"""
    
    # ğŸ”¥ ìŒìˆ˜ ì ìˆ˜ ì²˜ë¦¬: ì „ì²´ ì ìˆ˜ë¥¼ ì–‘ìˆ˜ë¡œ shift
    all_scores = np.concatenate([genuine_scores, imposter_scores])
    min_score = np.min(all_scores)
    
    if min_score < 0:
        print(f"[EER] ìŒìˆ˜ ì ìˆ˜ ê°ì§€: {min_score:.6f}, ì–‘ìˆ˜ë¡œ shift ì ìš©")
        genuine_scores = genuine_scores - min_score
        imposter_scores = imposter_scores - min_score
        print(f"[EER] Shift í›„ ë²”ìœ„: [{np.min(all_scores - min_score):.6f}, {np.max(all_scores - min_score):.6f}]")
    
    # ğŸ”¥ ì ìˆ˜ í†µê³„ ì¶œë ¥
    print(f"[EER Stats] Genuine: count={len(genuine_scores)}, min={np.min(genuine_scores):.4f}, max={np.max(genuine_scores):.4f}, mean={np.mean(genuine_scores):.4f}")
    print(f"[EER Stats] Imposter: count={len(imposter_scores)}, min={np.min(imposter_scores):.4f}, max={np.max(imposter_scores):.4f}, mean={np.mean(imposter_scores):.4f}")
    
    # ë¼ë²¨ ìƒì„±
    labels = np.concatenate([np.ones_like(genuine_scores), np.zeros_like(imposter_scores)])
    scores = np.concatenate([genuine_scores, imposter_scores])

    try:
        # ğŸ”¥ ì›ë˜ ë°©ì‹: -scores ì‚¬ìš© (ê±°ë¦¬ê°€ ì•„ë‹Œ ìœ ì‚¬ë„ë¡œ ë³€í™˜)
        fpr, tpr, thresholds = metrics.roc_curve(labels, -scores, pos_label=1)
        
        # EER ê³„ì‚° ì‹œë„
        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)
        thresh = interp1d(fpr, thresholds)(-eer)
        
        print(f"[EER] ì„±ê³µì ìœ¼ë¡œ ê³„ì‚°ë¨: {eer*100:.4f}% at threshold {thresh:.6f}")
        
    except Exception as e:
        print(f"[EER] ë³´ê°„ ê³„ì‚° ì‹¤íŒ¨: {e}")
        print("[EER] ëŒ€ì•ˆ ë°©ë²• ì‚¬ìš©...")
        
        # ğŸ”¥ ëŒ€ì•ˆ: ì§ì ‘ EER ì§€ì  ì°¾ê¸°
        fpr, tpr, thresholds = metrics.roc_curve(labels, -scores, pos_label=1)
        fnr = 1 - tpr
        
        # FPRê³¼ FNRì´ ê°€ì¥ ê°€ê¹Œìš´ ì§€ì  ì°¾ê¸°
        diff = np.abs(fpr - fnr)
        eer_idx = np.argmin(diff)
        eer = (fpr[eer_idx] + fnr[eer_idx]) / 2
        thresh = thresholds[eer_idx]
        
        print(f"[EER] ëŒ€ì•ˆ ê³„ì‚° ê²°ê³¼: {eer*100:.4f}% at threshold {thresh:.6f}")

    return eer * 100, thresh

def calculate_rank1(distances, probe_labels, gallery_labels):
    """ê±°ë¦¬ í–‰ë ¬ì„ ë°”íƒ•ìœ¼ë¡œ Rank-1 ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    correct_predictions = 0
    num_probes = len(probe_labels)

    indices_of_closest = np.argmin(distances, axis=1)
    
    for i in range(num_probes):
        predicted_label = gallery_labels[indices_of_closest[i]]
        if probe_labels[i] == predicted_label:
            correct_predictions += 1
            
    rank1_accuracy = (correct_predictions / num_probes) * 100
    return rank1_accuracy

# ğŸ”¥ NEW: í™•ì¥ëœ í‰ê°€ í•¨ìˆ˜ë“¤

def calculate_detailed_biometric_metrics(genuine_scores, imposter_scores, system_info=None):
    """
    ìƒì„¸í•œ ìƒì²´ì¸ì‹ ë©”íŠ¸ë¦­ ê³„ì‚°
    
    Args:
        genuine_scores: ì •í’ˆ ë§¤ì¹­ ì ìˆ˜ ë°°ì—´
        imposter_scores: ìœ„ì¡° ë§¤ì¹­ ì ìˆ˜ ë°°ì—´  
        system_info: ì‹œìŠ¤í…œ ì •ë³´ (COCONUT ê´€ë ¨)
        
    Returns:
        dict: ìƒì„¸í•œ ë©”íŠ¸ë¦­ ê²°ê³¼
    """
    if len(genuine_scores) == 0 or len(imposter_scores) == 0:
        print("âš ï¸ Warning: Empty score arrays")
        return None
    
    # ë¼ë²¨ ìƒì„±
    labels = np.concatenate([np.ones_like(genuine_scores), np.zeros_like(imposter_scores)])
    scores = np.concatenate([genuine_scores, imposter_scores])
    
    # ROC ì»¤ë¸Œ ê³„ì‚°
    fpr, tpr, thresholds = metrics.roc_curve(labels, -scores, pos_label=1)
    fnr = 1 - tpr  # False Negative Rate
    
    # EER ê³„ì‚°
    try:
        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)
        eer_threshold = interp1d(fpr, thresholds)(eer)
    except:
        # ë³´ê°„ ì‹¤íŒ¨ ì‹œ ê°€ì¥ ê°€ê¹Œìš´ ì§€ì  ì°¾ê¸°
        eer_idx = np.argmin(np.abs(fpr - fnr))
        eer = (fpr[eer_idx] + fnr[eer_idx]) / 2
        eer_threshold = thresholds[eer_idx]
    
    # AUC ê³„ì‚°
    auc_score = metrics.auc(fpr, tpr)
    
    # ì ìˆ˜ í†µê³„
    genuine_stats = {
        'mean': np.mean(genuine_scores),
        'std': np.std(genuine_scores),
        'min': np.min(genuine_scores),
        'max': np.max(genuine_scores),
        'count': len(genuine_scores)
    }
    
    imposter_stats = {
        'mean': np.mean(imposter_scores),
        'std': np.std(imposter_scores),
        'min': np.min(imposter_scores),
        'max': np.max(imposter_scores),
        'count': len(imposter_scores)
    }
    
    # d-prime ê³„ì‚° (ë¶„ë¦¬ë„ ì¸¡ì •)
    d_prime = abs(genuine_stats['mean'] - imposter_stats['mean']) / \
              np.sqrt(0.5 * (genuine_stats['std']**2 + imposter_stats['std']**2))
    
    # ë‹¤ì–‘í•œ ì„ê³„ê°’ì—ì„œì˜ ì„±ëŠ¥
    threshold_analysis = []
    test_thresholds = np.linspace(scores.min(), scores.max(), 100)
    
    for thresh in test_thresholds:
        predictions = scores > thresh
        
        tp = np.sum((labels == 1) & predictions)
        tn = np.sum((labels == 0) & ~predictions)
        fp = np.sum((labels == 0) & predictions)
        fn = np.sum((labels == 1) & ~predictions)
        
        far = fp / (fp + tn) if (fp + tn) > 0 else 0
        frr = fn / (fn + tp) if (fn + tp) > 0 else 0
        accuracy = (tp + tn) / len(labels)
        
        threshold_analysis.append({
            'threshold': thresh,
            'far': far,
            'frr': frr,
            'accuracy': accuracy
        })
    
    results = {
        'eer': eer * 100,
        'eer_threshold': eer_threshold,
        'auc': auc_score,
        'd_prime': d_prime,
        'genuine_stats': genuine_stats,
        'imposter_stats': imposter_stats,
        'roc_curve': {
            'fpr': fpr,
            'tpr': tpr,
            'fnr': fnr,
            'thresholds': thresholds
        },
        'threshold_analysis': threshold_analysis,
        'raw_scores': {
            'genuine': genuine_scores,
            'imposter': imposter_scores
        }
    }
    
    # COCONUT ì‹œìŠ¤í…œ ì •ë³´ ì¶”ê°€
    if system_info:
        results['system_info'] = system_info
    
    return results

def save_biometric_evaluation_plots(results, output_dir="./results/evaluation", 
                                   title_prefix="COCONUT"):
    """
    ìƒì²´ì¸ì‹ í‰ê°€ ê·¸ë˜í”„ë“¤ì„ ì €ì¥
    
    Args:
        results: calculate_detailed_biometric_metricsì˜ ê²°ê³¼
        output_dir: ì €ì¥ ë””ë ‰í† ë¦¬
        title_prefix: ê·¸ë˜í”„ ì œëª© ì ‘ë‘ì‚¬
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ìŠ¤íƒ€ì¼ ì„¤ì •
    plt.style.use('default')
    sns.set_palette("husl")
    
    # 1. ROC ì»¤ë¸Œ
    plt.figure(figsize=(10, 8))
    plt.plot(results['roc_curve']['fpr'] * 100, 
             results['roc_curve']['tpr'] * 100, 
             'b-', linewidth=2, label=f'ROC Curve (AUC = {results["auc"]:.4f})')
    plt.plot([0, 100], [0, 100], 'k--', alpha=0.5, label='Random Classifier')
    
    # EER í¬ì¸íŠ¸ í‘œì‹œ
    eer_point_idx = np.argmin(np.abs(results['roc_curve']['fpr'] - results['roc_curve']['fnr']))
    plt.plot(results['roc_curve']['fpr'][eer_point_idx] * 100,
             results['roc_curve']['tpr'][eer_point_idx] * 100,
             'ro', markersize=8, label=f'EER = {results["eer"]:.3f}%')
    
    plt.xlabel('False Acceptance Rate (%)', fontsize=12)
    plt.ylabel('Genuine Acceptance Rate (%)', fontsize=12)
    plt.title(f'{title_prefix} ROC Curve', fontsize=14, fontweight='bold')
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.xlim([0, 10])  # ê´€ì‹¬ ì˜ì—­ì— ì§‘ì¤‘
    plt.ylim([90, 100])
    plt.tight_layout()
    plt.savefig(output_path / 'roc_curve.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. DET ì»¤ë¸Œ (Detection Error Tradeoff)
    plt.figure(figsize=(10, 8))
    plt.plot(results['roc_curve']['fpr'] * 100,
             results['roc_curve']['fnr'] * 100,
             'r-', linewidth=2, label='DET Curve')
    plt.plot([0, 100], [0, 100], 'k--', alpha=0.5, label='EER Line')
    
    # EER í¬ì¸íŠ¸ í‘œì‹œ
    plt.plot(results['roc_curve']['fpr'][eer_point_idx] * 100,
             results['roc_curve']['fnr'][eer_point_idx] * 100,
             'go', markersize=8, label=f'EER = {results["eer"]:.3f}%')
    
    plt.xlabel('False Acceptance Rate (%)', fontsize=12)
    plt.ylabel('False Rejection Rate (%)', fontsize=12)
    plt.title(f'{title_prefix} DET Curve', fontsize=14, fontweight='bold')
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.xlim([0, 10])
    plt.ylim([0, 10])
    plt.tight_layout()
    plt.savefig(output_path / 'det_curve.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # 3. ì ìˆ˜ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
    plt.figure(figsize=(12, 8))
    
    # ê²¹ì¹˜ëŠ” íˆìŠ¤í† ê·¸ë¨
    plt.hist(results['raw_scores']['genuine'], bins=50, alpha=0.7, 
             label=f'Genuine ({len(results["raw_scores"]["genuine"])} samples)', 
             color='blue', density=True)
    plt.hist(results['raw_scores']['imposter'], bins=50, alpha=0.7,
             label=f'Imposter ({len(results["raw_scores"]["imposter"])} samples)',
             color='red', density=True)
    
    # EER ì„ê³„ê°’ í‘œì‹œ
    plt.axvline(results['eer_threshold'], color='green', linestyle='--', linewidth=2,
                label=f'EER Threshold = {results["eer_threshold"]:.4f}')
    
    # í†µê³„ ì •ë³´ í‘œì‹œ
    plt.axvline(results['genuine_stats']['mean'], color='blue', linestyle=':', alpha=0.8,
                label=f'Genuine Mean = {results["genuine_stats"]["mean"]:.4f}')
    plt.axvline(results['imposter_stats']['mean'], color='red', linestyle=':', alpha=0.8,
                label=f'Imposter Mean = {results["imposter_stats"]["mean"]:.4f}')
    
    plt.xlabel('Similarity Score', fontsize=12)
    plt.ylabel('Density', fontsize=12)
    plt.title(f'{title_prefix} Score Distribution (d\' = {results["d_prime"]:.3f})', 
              fontsize=14, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_path / 'score_distribution.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # 4. FAR/FRR vs Threshold
    plt.figure(figsize=(12, 8))
    
    thresholds = [ta['threshold'] for ta in results['threshold_analysis']]
    fars = [ta['far'] * 100 for ta in results['threshold_analysis']]
    frrs = [ta['frr'] * 100 for ta in results['threshold_analysis']]
    
    plt.plot(thresholds, fars, 'r-', linewidth=2, label='FAR (%)')
    plt.plot(thresholds, frrs, 'b-', linewidth=2, label='FRR (%)')
    
    # EER í¬ì¸íŠ¸ í‘œì‹œ
    plt.axvline(results['eer_threshold'], color='green', linestyle='--', linewidth=2,
                label=f'EER Threshold = {results["eer_threshold"]:.4f}')
    plt.axhline(results['eer'], color='green', linestyle='--', linewidth=2,
                label=f'EER = {results["eer"]:.3f}%')
    
    plt.xlabel('Threshold', fontsize=12)
    plt.ylabel('Error Rate (%)', fontsize=12)
    plt.title(f'{title_prefix} FAR/FRR vs Threshold', fontsize=14, fontweight='bold')
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.yscale('log')
    plt.tight_layout()
    plt.savefig(output_path / 'far_frr_curve.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… í‰ê°€ ê·¸ë˜í”„ë“¤ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_path}")

def generate_evaluation_report(results, output_dir="./results/evaluation", 
                              system_name="COCONUT"):
    """
    í‰ê°€ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„± (JSON + í…ìŠ¤íŠ¸)
    
    Args:
        results: calculate_detailed_biometric_metricsì˜ ê²°ê³¼
        output_dir: ì €ì¥ ë””ë ‰í† ë¦¬
        system_name: ì‹œìŠ¤í…œ ì´ë¦„
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # JSON ë¦¬í¬íŠ¸ (ìƒì„¸ ë°ì´í„°)
    report_data = {
        'system_name': system_name,
        'evaluation_date': datetime.now().isoformat(),
        'metrics': {
            'eer_percent': float(results['eer']),
            'eer_threshold': float(results['eer_threshold']),
            'auc': float(results['auc']),
            'd_prime': float(results['d_prime'])
        },
        'genuine_statistics': {k: float(v) for k, v in results['genuine_stats'].items()},
        'imposter_statistics': {k: float(v) for k, v in results['imposter_stats'].items()},
        'system_info': results.get('system_info', {})
    }
    
    # JSON ì €ì¥
    with open(output_path / 'evaluation_report.json', 'w') as f:
        json.dump(report_data, f, indent=2)
    
    # í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ (ìš”ì•½)
    report_text = f"""
{'='*80}
{system_name} ìƒì²´ì¸ì‹ ì‹œìŠ¤í…œ í‰ê°€ ë¦¬í¬íŠ¸
{'='*80}
í‰ê°€ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

ğŸ“Š í•µì‹¬ ì„±ëŠ¥ ì§€í‘œ:
  â€¢ Equal Error Rate (EER): {results['eer']:.4f}%
  â€¢ Area Under Curve (AUC): {results['auc']:.6f}
  â€¢ d-prime (ë¶„ë¦¬ë„): {results['d_prime']:.4f}
  â€¢ EER ì„ê³„ê°’: {results['eer_threshold']:.6f}

ğŸ“ˆ ì •í’ˆ ë§¤ì¹­ í†µê³„:
  â€¢ ìƒ˜í”Œ ìˆ˜: {results['genuine_stats']['count']:,}
  â€¢ í‰ê· : {results['genuine_stats']['mean']:.6f}
  â€¢ í‘œì¤€í¸ì°¨: {results['genuine_stats']['std']:.6f}
  â€¢ ë²”ìœ„: [{results['genuine_stats']['min']:.6f}, {results['genuine_stats']['max']:.6f}]

ğŸ“‰ ìœ„ì¡° ë§¤ì¹­ í†µê³„:
  â€¢ ìƒ˜í”Œ ìˆ˜: {results['imposter_stats']['count']:,}
  â€¢ í‰ê· : {results['imposter_stats']['mean']:.6f}
  â€¢ í‘œì¤€í¸ì°¨: {results['imposter_stats']['std']:.6f}
  â€¢ ë²”ìœ„: [{results['imposter_stats']['min']:.6f}, {results['imposter_stats']['max']:.6f}]

ğŸ¯ ì„±ëŠ¥ í‰ê°€:
"""
    
    # ì„±ëŠ¥ ë“±ê¸‰ í‰ê°€
    if results['eer'] < 0.1:
        grade = "Excellent (< 0.1%)"
    elif results['eer'] < 1.0:
        grade = "Very Good (< 1.0%)"
    elif results['eer'] < 5.0:
        grade = "Good (< 5.0%)"
    elif results['eer'] < 10.0:
        grade = "Fair (< 10.0%)"
    else:
        grade = "Poor (â‰¥ 10.0%)"
    
    report_text += f"  â€¢ EER ë“±ê¸‰: {grade}\n"
    
    if results['auc'] > 0.99:
        auc_grade = "Excellent (> 0.99)"
    elif results['auc'] > 0.95:
        auc_grade = "Very Good (> 0.95)"
    elif results['auc'] > 0.90:
        auc_grade = "Good (> 0.90)"
    else:
        auc_grade = "Needs Improvement (â‰¤ 0.90)"
    
    report_text += f"  â€¢ AUC ë“±ê¸‰: {auc_grade}\n"
    
    # ì‹œìŠ¤í…œ ì •ë³´ ì¶”ê°€
    if 'system_info' in results and results['system_info']:
        report_text += f"\nğŸ”§ ì‹œìŠ¤í…œ ì •ë³´:\n"
        for key, value in results['system_info'].items():
            report_text += f"  â€¢ {key}: {value}\n"
    
    report_text += f"\n{'='*80}\n"
    report_text += "ğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:\n"
    report_text += "  â€¢ roc_curve.png - ROC ì»¤ë¸Œ\n"
    report_text += "  â€¢ det_curve.png - DET ì»¤ë¸Œ\n"
    report_text += "  â€¢ score_distribution.png - ì ìˆ˜ ë¶„í¬\n"
    report_text += "  â€¢ far_frr_curve.png - FAR/FRR ì»¤ë¸Œ\n"
    report_text += "  â€¢ evaluation_report.json - ìƒì„¸ ë°ì´í„°\n"
    report_text += "  â€¢ evaluation_summary.txt - ì´ ë¦¬í¬íŠ¸\n"
    report_text += f"{'='*80}\n"
    
    # í…ìŠ¤íŠ¸ ì €ì¥
    with open(output_path / 'evaluation_summary.txt', 'w', encoding='utf-8') as f:
        f.write(report_text)
    
    print(f"âœ… í‰ê°€ ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {output_path}")
    print(f"ğŸ“Š EER: {results['eer']:.4f}%, AUC: {results['auc']:.6f}")
    
    return report_data

def perform_coconut_evaluation(model, train_loader, test_loader, device, 
                              save_plots=True, save_report=True,
                              output_dir="./results/coconut_evaluation"):
    """
    COCONUT ì‹œìŠ¤í…œ ì „ìš© ì¢…í•© í‰ê°€
    
    Args:
        model: COCONUT ëª¨ë¸ (headless mode)
        train_loader: ê°¤ëŸ¬ë¦¬ ë°ì´í„°ë¡œë”
        test_loader: í”„ë¡œë¸Œ ë°ì´í„°ë¡œë”  
        device: ì—°ì‚° ë””ë°”ì´ìŠ¤
        save_plots: ê·¸ë˜í”„ ì €ì¥ ì—¬ë¶€
        save_report: ë¦¬í¬íŠ¸ ì €ì¥ ì—¬ë¶€
        output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        
    Returns:
        dict: ìƒì„¸í•œ í‰ê°€ ê²°ê³¼
    """
    print("\n" + "="*80)
    print("ğŸ¥¥ COCONUT ì‹œìŠ¤í…œ ì¢…í•© í‰ê°€ ì‹œì‘")
    print("="*80)
    
    # 1. ê¸°ë³¸ í‰ê°€ (ê¸°ì¡´ í•¨ìˆ˜ í™œìš©)
    basic_results = perform_evaluation(model, train_loader, test_loader, device)
    
    # 2. íŠ¹ì§• ì¶”ì¶œ
    print("\nğŸ“Š íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
    gallery_features, gallery_labels = extract_features(model, train_loader, device)
    probe_features, probe_labels = extract_features(model, test_loader, device)
    
    # 3. ìœ ì‚¬ë„ ê³„ì‚° (cosine similarity ì‚¬ìš©)
    print("ğŸ”„ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...")
    similarities = np.dot(probe_features, gallery_features.T)
    
    # 4. Genuine/Imposter ì ìˆ˜ ë¶„ë¦¬
    genuine_scores = []
    imposter_scores = []
    
    for i, probe_label in enumerate(probe_labels):
        for j, gallery_label in enumerate(gallery_labels):
            score = similarities[i, j]
            
            if probe_label == gallery_label:
                genuine_scores.append(score)
            else:
                imposter_scores.append(score)
    
    # 5. ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘
    system_info = {}
    if hasattr(model, 'get_model_info'):
        system_info = model.get_model_info()
    
    system_info.update({
        'probe_samples': len(probe_features),
        'gallery_samples': len(gallery_features),
        'genuine_comparisons': len(genuine_scores),
        'imposter_comparisons': len(imposter_scores),
        'feature_dimension': probe_features.shape[1],
        'evaluation_type': 'COCONUT_Headless'
    })
    
    # 6. ìƒì„¸ ë©”íŠ¸ë¦­ ê³„ì‚°
    print("ğŸ“ˆ ìƒì„¸ ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘...")
    detailed_results = calculate_detailed_biometric_metrics(
        np.array(genuine_scores), 
        np.array(imposter_scores),
        system_info
    )
    
    if detailed_results is None:
        print("âŒ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨")
        return basic_results
    
    # 7. ê·¸ë˜í”„ ìƒì„±
    if save_plots:
        print("ğŸ“Š í‰ê°€ ê·¸ë˜í”„ ìƒì„± ì¤‘...")
        save_biometric_evaluation_plots(
            detailed_results, 
            output_dir=output_dir,
            title_prefix="COCONUT"
        )
    
    # 8. ë¦¬í¬íŠ¸ ìƒì„±
    if save_report:
        print("ğŸ“„ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        generate_evaluation_report(
            detailed_results,
            output_dir=output_dir,
            system_name="COCONUT"
        )
    
    # 9. ê²°ê³¼ í†µí•©
    final_results = {
        **basic_results,
        'detailed_metrics': detailed_results,
        'output_directory': output_dir
    }
    
    print("\n" + "="*80)
    print("ğŸ‰ COCONUT í‰ê°€ ì™„ë£Œ!")
    print(f"ğŸ“Š EER: {detailed_results['eer']:.4f}%")
    print(f"ğŸ“ˆ AUC: {detailed_results['auc']:.6f}")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥: {output_dir}")
    print("="*80)
    
    return final_results

# ê¸°ì¡´ perform_evaluation í•¨ìˆ˜ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
def perform_evaluation(model, train_loader, test_loader, device):
    """ëª¨ë¸ì˜ ì „ì²´ ì„±ëŠ¥ í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ (ê¸°ì¡´ ë²„ì „)"""
    print("\n--- Starting Full Evaluation ---")
    
    # 1. íŠ¹ì§• ì¶”ì¶œ
    gallery_features, gallery_labels = extract_features(model, train_loader, device)
    probe_features, probe_labels = extract_features(model, test_loader, device)
    
    # 2. ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
    print("Calculating matching scores...")
    distances = calculate_scores(probe_features, gallery_features)
    
    # 3. Rank-1 ì •í™•ë„ ê³„ì‚°
    rank1_acc = calculate_rank1(distances, probe_labels, gallery_labels)
    print(f"Rank-1 Accuracy: {rank1_acc:.3f}%")
    
    # 4. EER ê³„ì‚°
    genuine_scores = []
    imposter_scores = []
    for i in range(len(probe_labels)):
        for j in range(len(gallery_labels)):
            score = distances[i, j]
            if probe_labels[i] == gallery_labels[j]:
                genuine_scores.append(score)
            else:
                imposter_scores.append(score)

    eer, threshold = calculate_eer(np.array(genuine_scores), np.array(imposter_scores))
    print(f"Equal Error Rate (EER): {eer:.4f}% at Threshold: {threshold:.4f}")
    
    results = {
        'rank1_accuracy': rank1_acc,
        'eer': eer,
        'threshold': threshold
    }
    return results